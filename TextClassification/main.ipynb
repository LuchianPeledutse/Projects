{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a282fc",
   "metadata": {},
   "source": [
    "импортируем нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ea9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer,models,trainers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score,precision_score,accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset,random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f72e5",
   "metadata": {},
   "source": [
    "считываем новости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec380c",
   "metadata": {},
   "source": [
    "новости считанные с lenta news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7c8a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>МВД создало бюро по борьбе с отмыванием денег</td>\n",
       "      <td>В МВД России создано оперативно-розыскное бюро...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Правительство не будет занимать деньги у ЦБ</td>\n",
       "      <td>Правительство не будет прибегать к заимствован...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ВВС США заказали 60 \"Боингов\" для вторжения в ...</td>\n",
       "      <td>Компания Boeing получит пятилетний контракт на...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Правительственная комиссия не дала ExxonMobil ...</td>\n",
       "      <td>Результаты конкурса на право освоения месторож...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Неимущим для получения пособия придется расста...</td>\n",
       "      <td>В доходах малоимущих россиян будут учитываться...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23944</th>\n",
       "      <td>Борис Гребенщиков награжден орденом \"За заслуг...</td>\n",
       "      <td>Лидер легендарной группы \"Аквариум\" Борис Греб...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23945</th>\n",
       "      <td>Мадонна и Траволта названы худшими актерами года</td>\n",
       "      <td>24 марта, накануне вручения премии Американско...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23946</th>\n",
       "      <td>Габриэль Гарсиа Маркес простил Голливуд ради д...</td>\n",
       "      <td>Габриэль Гарсиа Маркес</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23947</th>\n",
       "      <td>Аль Пачино отстоял в суде свое право на детей</td>\n",
       "      <td>Знаменитый американский актер Аль Пачино добил...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23948</th>\n",
       "      <td>Восстановленный ректор Московской консерватори...</td>\n",
       "      <td>Мосгорсуд отменил решение Пресненского межмуни...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0          МВД создало бюро по борьбе с отмыванием денег   \n",
       "1            Правительство не будет занимать деньги у ЦБ   \n",
       "2      ВВС США заказали 60 \"Боингов\" для вторжения в ...   \n",
       "3      Правительственная комиссия не дала ExxonMobil ...   \n",
       "4      Неимущим для получения пособия придется расста...   \n",
       "...                                                  ...   \n",
       "23944  Борис Гребенщиков награжден орденом \"За заслуг...   \n",
       "23945   Мадонна и Траволта названы худшими актерами года   \n",
       "23946  Габриэль Гарсиа Маркес простил Голливуд ради д...   \n",
       "23947      Аль Пачино отстоял в суде свое право на детей   \n",
       "23948  Восстановленный ректор Московской консерватори...   \n",
       "\n",
       "                                                    text      topic  \n",
       "0      В МВД России создано оперативно-розыскное бюро...  Экономика  \n",
       "1      Правительство не будет прибегать к заимствован...  Экономика  \n",
       "2      Компания Boeing получит пятилетний контракт на...  Экономика  \n",
       "3      Результаты конкурса на право освоения месторож...  Экономика  \n",
       "4      В доходах малоимущих россиян будут учитываться...  Экономика  \n",
       "...                                                  ...        ...  \n",
       "23944  Лидер легендарной группы \"Аквариум\" Борис Греб...   Культура  \n",
       "23945  24 марта, накануне вручения премии Американско...   Культура  \n",
       "23946                             Габриэль Гарсиа Маркес   Культура  \n",
       "23947  Знаменитый американский актер Аль Пачино добил...   Культура  \n",
       "23948  Мосгорсуд отменил решение Пресненского межмуни...   Культура  \n",
       "\n",
       "[23949 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta_news = pd.read_csv('lenta_news.csv')\n",
    "lenta_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad97a33",
   "metadata": {},
   "source": [
    "новости считанные с rbk news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37b02a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>США обеспокоены участием военных КНДР в военно...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пхеньян, по ее словам «не помогает» в урегулир...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>США «по-прежнему обеспокоены прямым участием» ...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>«Переброска на территорию России военных [КНДР...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    Минобороны п...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>В отличие от стратегий автоследования в ОПИФе ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Идею привлечения авторов инвестиционных блогов...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>Инвестиционный стратег «Гарда Капитал» Алексан...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0         1\n",
       "0     США обеспокоены участием военных КНДР в военно...  Политика\n",
       "1     Пхеньян, по ее словам «не помогает» в урегулир...  Политика\n",
       "2     США «по-прежнему обеспокоены прямым участием» ...  Политика\n",
       "3     «Переброска на территорию России военных [КНДР...  Политика\n",
       "4     \\n\\n\\n\\n\\n\\n\\n                    Минобороны п...  Политика\n",
       "...                                                 ...       ...\n",
       "1170  \\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...   Финансы\n",
       "1171  В отличие от стратегий автоследования в ОПИФе ...   Финансы\n",
       "1172  \\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...   Финансы\n",
       "1173  Идею привлечения авторов инвестиционных блогов...   Финансы\n",
       "1174  Инвестиционный стратег «Гарда Капитал» Алексан...   Финансы\n",
       "\n",
       "[1175 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbk_news = pd.read_csv('rbk_news.csv',header = None)\n",
    "rbk_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505ef98",
   "metadata": {},
   "source": [
    "выводим название всех таргетов (типов новостей) для каждой новости с целью их последующего объеденения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ccef43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Политика', 'Экономика', 'Бизнес', 'Технологии и медиа', 'Финансы'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbk_news.iloc[:,1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1dcd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Экономика', 'Интернет и СМИ', 'Спорт', 'Культура'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta_news.iloc[:,2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95dd083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbk_news.loc[rbk_news[1] == 'Технологии и медиа',1] = 'Интернет и СМИ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25ba420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Политика', 'Экономика', 'Бизнес', 'Интернет и СМИ', 'Финансы'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbk_news[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f128cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Экономика', 'Интернет и СМИ', 'Спорт', 'Культура'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta_news.iloc[:,2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e47d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = list(set(rbk_news[1].unique().tolist() + lenta_news.iloc[:,2].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dd5b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Экономика',\n",
       " 'Бизнес',\n",
       " 'Финансы',\n",
       " 'Культура',\n",
       " 'Интернет и СМИ',\n",
       " 'Спорт',\n",
       " 'Политика']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_classes))\n",
    "all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f1cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lent_text_col = pd.DataFrame({'Text':[lenta_news.iloc[row,0]+ ' ' +lenta_news.iloc[row,1] for row in range(len(lenta_news))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_news.insert(loc = 0,column = 'Text',value = lent_text_col['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "784ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_news.drop(['title','text'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbaac4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>МВД создало бюро по борьбе с отмыванием денег ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Правительство не будет занимать деньги у ЦБ Пр...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ВВС США заказали 60 \"Боингов\" для вторжения в ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Правительственная комиссия не дала ExxonMobil ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Неимущим для получения пособия придется расста...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23944</th>\n",
       "      <td>Борис Гребенщиков награжден орденом \"За заслуг...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23945</th>\n",
       "      <td>Мадонна и Траволта названы худшими актерами го...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23946</th>\n",
       "      <td>Габриэль Гарсиа Маркес простил Голливуд ради д...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23947</th>\n",
       "      <td>Аль Пачино отстоял в суде свое право на детей ...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23948</th>\n",
       "      <td>Восстановленный ректор Московской консерватори...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23949 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text      topic\n",
       "0      МВД создало бюро по борьбе с отмыванием денег ...  Экономика\n",
       "1      Правительство не будет занимать деньги у ЦБ Пр...  Экономика\n",
       "2      ВВС США заказали 60 \"Боингов\" для вторжения в ...  Экономика\n",
       "3      Правительственная комиссия не дала ExxonMobil ...  Экономика\n",
       "4      Неимущим для получения пособия придется расста...  Экономика\n",
       "...                                                  ...        ...\n",
       "23944  Борис Гребенщиков награжден орденом \"За заслуг...   Культура\n",
       "23945  Мадонна и Траволта названы худшими актерами го...   Культура\n",
       "23946  Габриэль Гарсиа Маркес простил Голливуд ради д...   Культура\n",
       "23947  Аль Пачино отстоял в суде свое право на детей ...   Культура\n",
       "23948  Восстановленный ректор Московской консерватори...   Культура\n",
       "\n",
       "[23949 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7b9d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbk_news.columns = ['Text','topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75f2bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>США обеспокоены участием военных КНДР в военно...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пхеньян, по ее словам «не помогает» в урегулир...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>США «по-прежнему обеспокоены прямым участием» ...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>«Переброска на территорию России военных [КНДР...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    Минобороны п...</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>В отличие от стратегий автоследования в ОПИФе ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Идею привлечения авторов инвестиционных блогов...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>Инвестиционный стратег «Гарда Капитал» Алексан...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text     topic\n",
       "0     США обеспокоены участием военных КНДР в военно...  Политика\n",
       "1     Пхеньян, по ее словам «не помогает» в урегулир...  Политика\n",
       "2     США «по-прежнему обеспокоены прямым участием» ...  Политика\n",
       "3     «Переброска на территорию России военных [КНДР...  Политика\n",
       "4     \\n\\n\\n\\n\\n\\n\\n                    Минобороны п...  Политика\n",
       "...                                                 ...       ...\n",
       "1170  \\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...   Финансы\n",
       "1171  В отличие от стратегий автоследования в ОПИФе ...   Финансы\n",
       "1172  \\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...   Финансы\n",
       "1173  Идею привлечения авторов инвестиционных блогов...   Финансы\n",
       "1174  Инвестиционный стратег «Гарда Капитал» Алексан...   Финансы\n",
       "\n",
       "[1175 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbk_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e85050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'topic'], dtype='object')\n",
      "Index(['Text', 'topic'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rbk_news.columns)\n",
    "print(lenta_news.columns)\n",
    "rbk_news.columns == lenta_news.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fa9fd",
   "metadata": {},
   "source": [
    "объеденяем все новости в один датафрайм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b05fbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_data = pd.concat([lenta_news,rbk_news],axis = 0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3800694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>МВД создало бюро по борьбе с отмыванием денег ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Правительство не будет занимать деньги у ЦБ Пр...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ВВС США заказали 60 \"Боингов\" для вторжения в ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Правительственная комиссия не дала ExxonMobil ...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Неимущим для получения пособия придется расста...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25119</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25120</th>\n",
       "      <td>В отличие от стратегий автоследования в ОПИФе ...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25121</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25122</th>\n",
       "      <td>Идею привлечения авторов инвестиционных блогов...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25123</th>\n",
       "      <td>Инвестиционный стратег «Гарда Капитал» Алексан...</td>\n",
       "      <td>Финансы</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25124 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text      topic\n",
       "0      МВД создало бюро по борьбе с отмыванием денег ...  Экономика\n",
       "1      Правительство не будет занимать деньги у ЦБ Пр...  Экономика\n",
       "2      ВВС США заказали 60 \"Боингов\" для вторжения в ...  Экономика\n",
       "3      Правительственная комиссия не дала ExxonMobil ...  Экономика\n",
       "4      Неимущим для получения пособия придется расста...  Экономика\n",
       "...                                                  ...        ...\n",
       "25119  \\n\\n\\n\\n\\n\\n\\n                    ЦБ подумает ...    Финансы\n",
       "25120  В отличие от стратегий автоследования в ОПИФе ...    Финансы\n",
       "25121  \\n\\n\\n\\n\\n\\n\\n                    Индекс Мосби...    Финансы\n",
       "25122  Идею привлечения авторов инвестиционных блогов...    Финансы\n",
       "25123  Инвестиционный стратег «Гарда Капитал» Алексан...    Финансы\n",
       "\n",
       "[25124 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d29049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Экономика', 'Интернет и СМИ', 'Спорт', 'Культура', 'Политика',\n",
       "       'Бизнес', 'Финансы'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = all_news_data['topic'].unique()\n",
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e145e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_data.to_csv('all_news_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "934cc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Экономика\n",
      "Касьянов заинтересовал Словакию новым газопроводом Словацкая сторона проявила \"заметный интерес\" к идее строительства газопровода по своей территории для транспортировки российского газа в Европу, об этом заявлено на встрече премьер-министра РФ Михаила Касьянова и министра экономики Словакии Любомира Гараха в среду в московском Доме правительства. Как сообщают из Департамента правительственной информации, Словакия уже заявила, что придает большое значение этому проекту, который свяжет магистральные газопроводы на территории Словакии и Польши. Сооружение новой нитки газопровода через Словакию позволит направить российский газ с Ямала в Европу в обход Украины, которая продолжает прибегать к несанкционированным заборам российского газа. Потери России от действий Украины оцениваются в несколько миллиардов кубических  метров в год. Беседа Касьянова и Гараха состоялась в рамках обсуждения итогов прошедшего накануне заседания российско-словацкой межправительственной комиссии по научно-техническому, экономическому и культурному сотрудничеству. В рамках заседания подписаны соглашения о поставках в Словакию нефтепродуктов из России, о взаимном обучении студентов и протокол о сотрудничестве между малыми и средними предприятиями. Кроме того, принято решение об участии российского бизнеса в приватизационных процессах в Словакии, сообщает РИА \"Новости\".\n",
      "\n",
      "\n",
      "\n",
      "Интернет и СМИ\n",
      "Образовательному сайту запретили рассказывать о венерических болезнях Правительство Великобритании признало в понедельник, что на правительственном образовательном сайте Eduweb находятся ссылки на сайты, содержащие информацию сексуального характера, а также на ресурсы, посвященные наркотикам, сообщает в понедельник новостной сайт Yahoo! News. Образовательный сайт Eduweb представляет собой интернет-каталог, содержащий большое количество ссылок на различные образовательные ресурсы. Eduweb является частью широкомасштабной программы правительства Великобритании по предоставлению Интернета во все школы страны. По признанию правительства, некоторые ссылки, опубликованные на сайте, \"оказались не слишком подходящими для детей\". В частности, на одном из сайтов - \"виртуальная поликлиника\", на который можно было попасть через Eduweb, подробно обсуждались такие признанные неподходящими для детей темы, как заболевания, передающиеся половым путем, а также различные аспекты наркотической зависимости. \"Мы признаем, что некоторые материалы, ссылки на которые помещены на сайте, не слишком подходят для детей, - заявил представитель департамента образования по связям с общественностью. - Правительство примет меры и изменит содержание этого сайта\".\n",
      "\n",
      "\n",
      "\n",
      "Спорт\n",
      "Матч чемпионата Италии по футболу прерван из-за бесчинств римских болельщиков Матч очередного тура чемпионата Италии по футболу между римскими командами \"Лацио\" и \"Рома\" был прерван во втором тайме из-за бесчинств, которые устроили на олимпийском стадионе римские болельщики. На 47-й минуте встрече один из болельщиков распространил слух, что за пределами стадиона в столкновении с полицией убит футбольный фанат. Несмотря на то, что представители полиции по стадионному радио опровергли эту информацию, болельщики на стадионе устроили беспорядки и матч был отменен, сообщает агентство ANSA. При счете 0:0 во втором тайме, когда болельщики начали устраивать беспорядки на трибунах, команды собрались в центре поля и ожидали решения арбитра матча Розетти, который дал сигнал к продолжению игры. Диктор по стадиону озвучил официальное заявление полиции о том, что никакого убийства болельщика ни на стадионе, ни за его пределами не было, но на поле и в полицейских, обеспечивавших охрану матча, полетели петарды и другие подручные предметы. Получив по телефону указание президента итальянской футбольной лиги Адриано Галлиани, судья встречи отменил матч, поскольку ситуация могла в любой момент выйти из под контроля, ведь на матче присутствовало около 70 тысяч зрителей. \"Я принял решение отменить матч, поскольку дальше продолжать встречу было невозможна. Матч будет переигран\", - сказал Галлиани. Во время беспорядков на стадионе пострадали три болельщика, которые были госпитализированы. До и после игры в Риме произошло несколько столкновений футбольных болельщиков с полицией. С обеих сторон есть пострадавшие, но обошлось без жертв. \"Полиция и карабинеры гарантировали мне, что никаких беспорядков не будет, но свое обещание не сдержали, - сказал префект Рима Акиле Серра. - До факту беспорядков во время футбола будет проведено расследование\". Остальные встречи тура были сыграны без инцидентов, но никаких изменений в положение лидеров не внесли. \"Милан\" по-прежнему занимает первое место, опережая \"Рому\" на 10, а \"Ювентус\" на 11 очков.\n",
      "\n",
      "\n",
      "\n",
      "Культура\n",
      "Самая большая в мире литературная премия досталась дебютному роману В Дублине назван лауреат литературной премии Impac Dublin Literary Awards - это самая большая в мире премия, которую можно получить за отдельное произведение. В соревновании с пятью другими претендентами победил дебютный роман \"Без особого вреда\" (No Great Mischief) университетского преподавателя литературы 65-летнего канадца Элистера Маклауда (Alistair MacLeod). Победа принесла начинающему романисту  75 тысяч фунтов стерлингов (120 тысяч долларов). \"Без особого труда\" - первый роман Маклауда: до сих пор он писал только рассказы. А прежде чем заняться литературой, он был рыбаком, лесорубом, шахтером. По словам автора, он писал \"Без особого вреда\" десять лет. Cудя по описанию ВВС, это классическая семейная сага, история некоего шотландца  Кэлума Макдональда (Calum MacDonald), переселившегося в Канаду в 1779 году со своим многочисленным семейством. Речь в романе идет о конфликте \"отцов и детей\" и традиционных проблемах иммигрантов. Рассказчиком выступает потомок Кэлума - Александр Макдональд. По мнению жюри,  \"Без особого вреда\" - \"грандиозная, возвышенная история о наследии, утрате и возрождении\". Кроме романа Маклауда, в финал  Impac Dublin Literary Awards вышли еще пять книг, которые жюри отобрало из тех 98, что были номинированы двумястами крупнейшими библиотеками ста городов мира. В финал вышли сочинения ирландского, шотландского, мексиканского, ямайского и русского авторов. Этот русский роман - \"Чапаев и Пустота\" Виктора Пелевина.\n",
      "\n",
      "\n",
      "\n",
      "Политика\n",
      "В СМИ стала распространяться позже опровергнутая информация, что глава Еврокомиссии Урсула фон дер Ляйен в разговоре с CNN якобы обвинила российских хакеров в произошедшем на юге Европы. В действительности в единственном заявлении фон дер Ляйен на эту тему в соцсети X говорилось лишь, что она находится в постоянном контакте с властями пострадавших стран. Глава Евросовета, экс-премьер Португалии Антониу Кошта заявил, что «нет никаких признаков того, что отключение электричества было вызвано кибератакой».\n",
      "\n",
      "\n",
      "\n",
      "Бизнес\n",
      "В августе 2024 года РБК стало известно о планируемом переходе оператора под контроль ВТБ, тогда предполагалось несколько сценариев: смена акционера, продажа активов компании рынку и консолидация парка между операторами. Вскоре сделку подтвердил глава банка Андрей Костин, уточнив, что ВТБ купил ПГК «исходя из оценки более 200 млрд руб. с учетом долга». Задерживаться в непрофильном активе банк не планировал. В декабре Костин объявил о планах продать грузовую компанию до конца 2024 года.\n",
      "\n",
      "\n",
      "\n",
      "Финансы\n",
      "Судебное разбирательство между Райффайзенбанком и «Распериа» длится с августа 2024 года, когда «Распериа» подала иск в арбитражный суд к Райффайзенбанку, Strabag и его основателю Петеру Хазельштайнеру. Компания обвинила их в захвате корпоративного контроля в Strabag, отстранении представителей Rasperia от собраний акционеров, размытии ее доли ниже блокирующего пакета и невыплате дивидендов.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for one_label in labels:\n",
    "    the_one = all_news_data.loc[all_news_data['topic'] == one_label,'Text'].iloc[rd.randint(0,300)]\n",
    "    print(one_label)\n",
    "    print(the_one)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34aa435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news = pd.read_csv('all_news_data.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25308e9",
   "metadata": {},
   "source": [
    "разделяем данные на тестовую и обучающую выборку таким образом, чтобы распределения классов в них было одинаковым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b56c4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_news,Test_news = train_test_split(all_news,stratify=all_news['topic'],train_size=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7a2df470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23365\n",
      "1759\n"
     ]
    }
   ],
   "source": [
    "print(len(Train_news))\n",
    "print(len(Test_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_news.to_csv('Train_news.csv')\n",
    "# Test_news.to_csv('Test_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e18491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_news,Test_news = pd.read_csv('Train_news.csv',index_col=0),pd.read_csv('Test_news.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_news = Test_news.reset_index().drop(['index'],axis = 1)\n",
    "# Train_news = Train_news.reset_index().drop(['index'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49366d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23364\n",
      "1759\n"
     ]
    }
   ],
   "source": [
    "print(len(Train_news))\n",
    "print(len(Test_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d15f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Украина готова вернуть \"взятый по ошибке\" кред...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Рио-де-Жанейро стал кандидатом на роль столицы...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Скончался драматург и сценарист Александр Воло...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Испанец пересек Атлантический океан на водном ...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Украина на Новый год останавливает все, что дв...</td>\n",
       "      <td>Интернет и СМИ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           topic\n",
       "0  Украина готова вернуть \"взятый по ошибке\" кред...       Экономика\n",
       "1  Рио-де-Жанейро стал кандидатом на роль столицы...           Спорт\n",
       "2  Скончался драматург и сценарист Александр Воло...        Культура\n",
       "3  Испанец пересек Атлантический океан на водном ...           Спорт\n",
       "4  Украина на Новый год останавливает все, что дв...  Интернет и СМИ"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Train_news))\n",
    "Train_news.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6cd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1759\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Падение фондового рынка возглавили \"ЮКОС\" и \"Г...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Контрабанда нефти из Ирака достигла рекордного...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Андрей Шевченко продлил контракт с \"Миланом\" д...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Американцы запрещают детям играть в компьютерн...</td>\n",
       "      <td>Интернет и СМИ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Napster идет маршем на Вашингтон Napster призы...</td>\n",
       "      <td>Интернет и СМИ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           topic\n",
       "0  Падение фондового рынка возглавили \"ЮКОС\" и \"Г...       Экономика\n",
       "1  Контрабанда нефти из Ирака достигла рекордного...       Экономика\n",
       "2  Андрей Шевченко продлил контракт с \"Миланом\" д...           Спорт\n",
       "3  Американцы запрещают детям играть в компьютерн...  Интернет и СМИ\n",
       "4  Napster идет маршем на Вашингтон Napster призы...  Интернет и СМИ"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Test_news))\n",
    "Test_news.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eab3db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Test_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae05328c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Test_news.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7929b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_news.dropna(inplace=True)\n",
    "# Train_news.reset_index().drop('index',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5314e",
   "metadata": {},
   "source": [
    "тут мы просто сохраняем для последующего использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_news.to_csv('Train_news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fb550",
   "metadata": {},
   "source": [
    "выводи русские стоп слова для их последующего использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c91f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_stopwords = stopwords.words('russian')\n",
    "print(len(russian_stopwords))\n",
    "russian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149d924",
   "metadata": {},
   "source": [
    "напишем функцию, которая будет нормализовывать текст следующим образом:    \n",
    "1)удаление всех символов кроме русских и английсских цифр и букв    \n",
    "2)удаление всех русских стоп-слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ff0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(the_text):\n",
    "    lowered = the_text.lower()\n",
    "    found = re.findall(r'[А-я0-9A-z]+',lowered)\n",
    "    filtered_found = [word for word in found if word != '' and word not in russian_stopwords]\n",
    "    return filtered_found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09eb441",
   "metadata": {},
   "source": [
    "выводим тут пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bcb0a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Испанец пересек Атлантический океан на\\xa0водном мотоцикле Испанец Альваро де Маричалар (Alvaro de Marichalar) пересек Атлантический океан на водном мотоцикле, сообщает в воскресенье BBC News. Маричалар начал свое путешествие четыре месяца в Риме. Проплыв по реке Тибр до Средиземного моря, он делал остановки в Марокко, на Гибралтаре и на Канарских островах. Достигнув Атлантического океана, путешественник посетил Кубу и в Доминиканскую республику. Конечной точкой маршрута Маричалара стал Майами, США. Маричалар ехал 12 часов в сутки. На протяжении всего маршрута его сопровождало судно с командой из шести человек на борту, которое перевозило еду и топливо для водного мотоцикла. По словам испанца, он потратил на подготовку к путешествию несколько лет, и поправился на 18 килограмм, чтобы компенсировать потерю веса во время прохождения маршрута. Отвечая на вопрос журналистов, что было самым тяжелым во время пересечения Атлантического океана, Маричалар ответил: \"Холод. По дороге из Рима в Неаполь я очень замерз\".'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = Train_news.iloc[3,0]\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458618bb",
   "metadata": {},
   "source": [
    "создаем генератор, который будет генерировать нормализованне текст (токены) из колонки текстового описания новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c9b4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleared_texts(text_col):\n",
    "    for ith_row in range(len(text_col)):\n",
    "        for token in normalize_text(text_col.iloc[ith_row]):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e9d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dataset = cleared_texts(Train_news['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca6cdb",
   "metadata": {},
   "source": [
    "с помощью библиотеки hugging face мы создаем тренировочны словарь (словарь из всех токенов которые содержатся во всех новостях тренировочного набора дданных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62814316",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlevel_trainer = trainers.WordLevelTrainer(vocab_size = 500_000,show_progress = True,special_tokens = ['[UNK]','[PAD]'],min_frequency = 5)\n",
    "wordlevel_model = models.WordLevel()\n",
    "Train_tokenizer = Tokenizer(model = wordlevel_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c65f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_tokenizer.train_from_iterator(tokens_dataset,trainer=wordlevel_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7297b",
   "metadata": {},
   "source": [
    "фиксируем полученный словарь и создаём функцию для помследующей его загрузки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ec6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57568"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Train_tokenizer.pkl','wb') as file:\n",
    "#     pickle.dump(Train_tokenizer,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b8a58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_tokenizer.pkl','rb') as file:\n",
    "    Train_tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efb284f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57568"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4da39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_dataset = cleared_texts(Test_news['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e44a20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlevel_trainer = trainers.WordLevelTrainer(vocab_size = 500_000,show_progress = True,special_tokens = ['[UNK]','[PAD]'])\n",
    "wordlevel_model = models.WordLevel()\n",
    "Test_tokenizer = Tokenizer(model = wordlevel_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "083d7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_tokenizer.train_from_iterator(test_tokens_dataset,trainer=wordlevel_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8331f900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51219"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Test_tokenizer.pkl','wb') as file:\n",
    "#     pickle.dump(Test_tokenizer,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6acde8",
   "metadata": {},
   "source": [
    "собираем всен нужные функции в одно место"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0f8d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading main functions\n",
    "def normalize_text(the_text):\n",
    "    lowered = the_text.lower()\n",
    "    found = re.findall(r'[А-я0-9A-z]+',lowered)\n",
    "    filtered_found = [word for word in found if word != '' and word not in russian_stopwords]\n",
    "    return filtered_found\n",
    "\n",
    "\n",
    "def cleared_texts(text_col):\n",
    "    for ith_row in range(len(text_col)):\n",
    "        for token in normalize_text(text_col.iloc[ith_row]):\n",
    "            yield token\n",
    "\n",
    "\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "print(len(russian_stopwords))\n",
    "russian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ccac48",
   "metadata": {},
   "source": [
    "загружаем наши данные разделенные на тестовую и обучающую "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376fef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "Train_news,Test_news = pd.read_csv('Train_news.csv',index_col=0),pd.read_csv('Test_news.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee24464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved tokenizers\n",
    "with open('Train_tokenizer.pkl','rb') as file:\n",
    "    Train_tokenizer = pickle.load(file)\n",
    "\n",
    "with open('Test_tokenizer.pkl','rb') as file:\n",
    "    Test_tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f4ca7",
   "metadata": {},
   "source": [
    "проверяем что все нормально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84541c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Украина готова вернуть \"взятый по ошибке\" кред...\n",
       "1    Рио-де-Жанейро стал кандидатом на роль столицы...\n",
       "2    Скончался драматург и сценарист Александр Воло...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_news.iloc[:3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a716064a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Падение фондового рынка возглавили \"ЮКОС\" и \"Г...\n",
       "1    Контрабанда нефти из Ирака достигла рекордного...\n",
       "2    Андрей Шевченко продлил контракт с \"Миланом\" д...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_news.iloc[:3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9daf02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51219\n",
      "57568\n"
     ]
    }
   ],
   "source": [
    "print(Test_tokenizer.get_vocab_size())\n",
    "print(Train_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14edec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the embeddings (3 types)\n",
    "#n_grams\n",
    "embd_training_text_list = list(cleared_texts(Train_news['Text']))\n",
    "embd_test_text_list = list(cleared_texts(Test_news['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a3fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['украина', 'готова', 'вернуть', 'взятый', 'ошибке']\n",
      "['падение', 'фондового', 'рынка', 'возглавили', 'юкос']\n"
     ]
    }
   ],
   "source": [
    "print(embd_training_text_list[:5])\n",
    "print(embd_test_text_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508547b5",
   "metadata": {},
   "source": [
    "теперь нужно создать такой датасет, чтобы по нему было возможно обучение первой модели векторных представлений ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f91ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(list_of_tokens,window_size):\n",
    "    '''creates N-grams with specified window'''\n",
    "    for ind in range(window_size,len(list_of_tokens)-window_size):\n",
    "        n_grams = []\n",
    "        #adding the words in window to the list\n",
    "        for window_gap in range(1,window_size + 1):\n",
    "            n_grams.append(list_of_tokens[ind - window_gap])\n",
    "        for window_gap in range(1,window_size + 1):\n",
    "            n_grams.append(list_of_tokens[ind + window_gap])\n",
    "        #yielding the result so that we can save some memory)\n",
    "        yield n_grams,list_of_tokens[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbcb76",
   "metadata": {},
   "source": [
    "создаем обучающий датасет для обучения модели ngram (окно будем брать по два слова из прошлого и будущего)    \n",
    "подробнее смотреть в аналитической записке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd7439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_grams = create_n_grams(embd_training_text_list,window_size=2)\n",
    "test_n_grams = create_n_grams(embd_test_text_list,window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5664e",
   "metadata": {},
   "source": [
    "проверяем, что датасет создался правильным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f238bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['готова', 'украина', 'взятый', 'ошибке'], 'вернуть')\n",
      "(['вернуть', 'готова', 'ошибке', 'кредит'], 'взятый')\n",
      "(['взятый', 'вернуть', 'кредит', 'мвф'], 'ошибке')\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "print(next(train_n_grams))\n",
    "print(next(train_n_grams))\n",
    "print(next(train_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed0949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['фондового', 'падение', 'возглавили', 'юкос'], 'рынка')\n",
      "(['рынка', 'фондового', 'юкос', 'газпром'], 'возглавили')\n",
      "(['возглавили', 'рынка', 'газпром', 'настоящее'], 'юкос')\n"
     ]
    }
   ],
   "source": [
    "print(next(test_n_grams))\n",
    "print(next(test_n_grams))\n",
    "print(next(test_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0ea0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['падение',\n",
       " 'фондового',\n",
       " 'рынка',\n",
       " 'возглавили',\n",
       " 'юкос',\n",
       " 'газпром',\n",
       " 'настоящее',\n",
       " 'время',\n",
       " '16',\n",
       " '00']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_test_text_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b5f2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['украина',\n",
       " 'готова',\n",
       " 'вернуть',\n",
       " 'взятый',\n",
       " 'ошибке',\n",
       " 'кредит',\n",
       " 'мвф',\n",
       " 'украина',\n",
       " 'готова',\n",
       " 'вернуть']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_training_text_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa09ef",
   "metadata": {},
   "source": [
    "на овнове датасета полученного сверху создается датасет предназначенный для обученя модели с помощью библиотеки Pytorch    \n",
    "класс основывается на класс Pytorch Dataset    \n",
    "он возвращает объекты обучающего множества ngram но в виде индексов из словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715dbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    def __init__(self,tokenizer,the_data):\n",
    "        self.main_data = the_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        X = torch.tensor([self.tokenizer.token_to_id(word) if self.tokenizer.token_to_id(word) != None else self.tokenizer.token_to_id('[UNK]') for word in self.main_data[indx][0]],dtype=torch.long)\n",
    "        true_word = self.main_data[indx][1]\n",
    "        y = self.tokenizer.token_to_id(true_word) if self.tokenizer.token_to_id(true_word) != None else self.tokenizer.token_to_id('[UNK]')\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f88e2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Ngramsdataset = NGramDataset(Train_tokenizer,list(train_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2417c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this dataset\n",
    "# with open('/home/luchian/all_data/uni_data/Train_Ngramsdataset','wb') as file:\n",
    "#     pickle.dump(Train_Ngramsdataset,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35105a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading this dataset\n",
    "with open('/home/luchian/all_data/uni_data/Train_Ngramsdataset','rb') as file:\n",
    "    Train_Ngramsdataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c47fae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3153657\n",
      "(tensor([8481,   69,  670,  370]), 140)\n"
     ]
    }
   ],
   "source": [
    "print(len(Train_Ngramsdataset))\n",
    "print(Train_Ngramsdataset[339823])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f3d16",
   "metadata": {},
   "source": [
    "создаем модель для обучения векторных представлений ngram (подробнее про архитектуру модели смотреть в аналитичнской записка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed,window_size = 2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed)\n",
    "        #from both sides\n",
    "        self.linear1 = nn.Linear(2*window_size*dim_embed,128)\n",
    "        self.linear2 = nn.Linear(128,vocab_size)\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,ind_list):\n",
    "        y = self.embed(ind_list)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        y = self.rel(self.linear1(y))\n",
    "        y = self.linear2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f858c",
   "metadata": {},
   "source": [
    "создаем функцию для обучения мрдели ngram    \n",
    "она представляется из себя проход по всем данным за одну эпоху    \n",
    "выисление результаты модели, подсчет ошибки, backprop и шаг оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b97473f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_ngram_model(the_model,epoch,loader,optimizer,loss_func,dev = 'cuda'):\n",
    "    for one_ep in range(epoch):\n",
    "        the_model.train()\n",
    "        the_losses = []\n",
    "        for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "            X,y = X.to(device = 'cuda'),y.to(device = 'cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = the_model(X)\n",
    "            the_loss = loss_func(y_pred,y)\n",
    "            the_loss.backward()\n",
    "            the_losses.append(the_loss.item())\n",
    "            optimizer.step()\n",
    "        if one_ep%1== 0:\n",
    "            print(f'Current Loss: {np.array(the_losses).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdee844",
   "metadata": {},
   "source": [
    "создаются все параметры нужные для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ad5a6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramModel(\n",
       "  (embed): Embedding(57568, 100)\n",
       "  (linear1): Linear(in_features=400, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=57568, bias=True)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the model\n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "ngram_model = NGramModel(vocab_size,dim_embed = 100,window_size = 2)\n",
    "ngram_model.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765a484",
   "metadata": {},
   "source": [
    "далее представлен коду обучения моделей (некоторые ячейки содержат в себе ошибку KeyboarInterrup так как обучения модели было временно преостановено в какой-то момент)    \n",
    "модель ngram обучалась для векторных представлений размерности 150 и 300    \n",
    "некоторые модели обучались в google colab в виду нехватки локальных ресурсов (смотреть ноутбук goolge_train.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a1c525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = Train_Ngramsdataset,shuffle = True,batch_size = 24)\n",
    "lr = 0.001\n",
    "epoch = 10\n",
    "optimizer = torch.optim.SGD(lr = lr,params= ngram_model.parameters(),momentum=0.02)\n",
    "the_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cef8cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:03<00:00, 269.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.32483832654222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:00<00:00, 269.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.641969712639877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:00<00:00, 269.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.432383479773657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:02<00:00, 269.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.273298127135142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:01<00:00, 269.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.135015301765817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  17%|█▋        | 105677/630732 [06:34<32:39, 267.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#training ngrams models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_ngram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthe_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m, in \u001b[0;36mtrain_ngram_model\u001b[0;34m(the_model, epoch, loader, optimizer, loss_func, dev)\u001b[0m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m the_model(X)\n\u001b[1;32m     10\u001b[0m the_loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred,y)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mthe_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m the_losses\u001b[38;5;241m.\u001b[39mappend(the_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training ngrams models\n",
    "train_ngram_model(ngram_model,epoch,train_loader,optimizer,the_loss,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c7feac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d516a41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b264108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:03<00:00, 269.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.171061615305863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:53<00:00, 270.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 7.468744769802325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:58<00:00, 269.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 7.080750655115399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:56<00:00, 269.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.818268371984311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:59<00:00, 269.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.624463000038144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:58<00:00, 269.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.471705604922975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:57<00:00, 269.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.348056463583278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:59<00:00, 269.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.244131233123362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [38:57<00:00, 269.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.155822211073704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 630732/630732 [39:00<00:00, 269.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 6.0792385670027205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ngram_model(ngram_model,epoch,train_loader,optimizer,the_loss,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f059b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 244.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.519884179043307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.442827039329851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.413536088380986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.390498882853134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.370131912798847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 245.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.351071769359849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 245.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.333531328547933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 245.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.316638866926077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.300499587808625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 244.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.285289190922727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 244.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.270228960653361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.25600776868283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.242303302258789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.228750664615409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.215810238418262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.202770970433312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.190729383192025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.178503718572878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.167148923234867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.1553040912675465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "train_ngram_model(ngram_model,epoch,train_loader,optimizer,the_loss,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fca3ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.1\n",
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efd4efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 245.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.344169051660087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 244.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.325969000098109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.304829706328518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.285560171995143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.2673491077421355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.250226574608237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.233648620168864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.217173271853868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.201587012839929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:56<00:00, 245.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.186790356493399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.1721138142324135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.1579464965471455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.144093390611656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:54<00:00, 245.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.130876923368022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.117994414570968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:55<00:00, 245.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.105082858274955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:53<00:00, 246.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.093274562882113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:52<00:00, 246.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.080866250494819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:53<00:00, 246.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.069270572151561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [08:52<00:00, 246.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 5.057996065708434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "train_ngram_model(ngram_model,epoch,train_loader,optimizer,the_loss,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "1d8542dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total time is 201+2*178+389\n",
    "# torch.save(ngram_model.state_dict(),'/home/luchian/all_data/uni_data/ngram_embeddings.pth')\n",
    "# torch.save(optimizer.state_dict(),'/home/luchian/all_data/uni_data/optimizer_ngram_params.pth')\n",
    "# ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_embeddings.pth',weights_only = True))\n",
    "# optimizer.load_state_dict(torch.load('/home/luchian/all_data/uni_data/optimizer_ngram_params.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2acfe",
   "metadata": {},
   "source": [
    "аналогично создается и модель векторных представлений cbow (подробнее архитектуру можно посмотреть в аналитической записке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78cc2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model \n",
    "#we can use the same dataset here, but we just have to sum things up\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed,window_size = 2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed)\n",
    "        self.linear = nn.Linear(1,vocab_size)\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,ind_list):\n",
    "        y = self.embed(ind_list).sum(dim = (1,2)).reshape(-1,1)\n",
    "        y = self.rel(self.linear(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d9639",
   "metadata": {},
   "source": [
    "по аналогии с модель ngram происходит и обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41419abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWModel(\n",
       "  (embed): Embedding(57568, 100)\n",
       "  (linear): Linear(in_features=1, out_features=57568, bias=True)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the model\n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "cbow_model = CBOWModel(vocab_size,dim_embed = 100,window_size = 2)\n",
    "cbow_model.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbf78889",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cbow_loader = DataLoader(dataset = Train_Ngramsdataset,shuffle = True,batch_size = 64)\n",
    "lr = 0.001\n",
    "epoch = 20\n",
    "optimizer_cbow = torch.optim.SGD(lr = lr,params= cbow_model.parameters())\n",
    "the_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d82b4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow_model(the_model,epoch,loader,optimizer,loss_func,dev = 'cuda'):\n",
    "    for one_ep in range(epoch):\n",
    "        the_model.train()\n",
    "        the_losses = []\n",
    "        for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "            X,y = X.to(device = 'cuda'),y.to(device = 'cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = the_model(X)\n",
    "            the_loss = loss_func(y_pred,y)\n",
    "            the_loss.backward()\n",
    "            the_losses.append(the_loss.item())\n",
    "            optimizer.step()\n",
    "        if one_ep%1== 0:\n",
    "            print(f'Current Loss: {the_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "844fba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.356122016906738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.109236717224121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.097528457641602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.620772361755371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.99347972869873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.907873153686523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.344832420349121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.648063659667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.355978012084961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.652222633361816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.432632446289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.463277816772461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.017885208129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.411460876464844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 10.048050880432129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.609384536743164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.471277236938477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.50818920135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.563058853149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 49276/49276 [03:48<00:00, 215.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.967806816101074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_cbow_model(cbow_model,epoch,train_cbow_loader,optimizer_cbow,the_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(cbow_model.state_dict(),'/home/luchian/all_data/uni_data/cbow_embeddings.pth')\n",
    "# torch.save(optimizer_cbow.state_dict(),'/home/luchian/all_data/uni_data/cbow_optimizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c951212",
   "metadata": {},
   "source": [
    "тут мы кодируем тэги наших новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d54b7e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "7\n",
      "['Экономика' 'Спорт' 'Культура' 'Интернет и СМИ' 'Финансы' 'Бизнес'\n",
      " 'Политика']\n",
      "['Экономика' 'Спорт' 'Интернет и СМИ' 'Культура' 'Политика' 'Финансы'\n",
      " 'Бизнес']\n"
     ]
    }
   ],
   "source": [
    "#encoding labels (they have to be the same in the dataset below for test and for train)\n",
    "unique_train_labels = Train_news['topic'].unique()\n",
    "unique_test_labels = Test_news['topic'].unique()\n",
    "print(set(unique_test_labels) == set(unique_train_labels),end = '\\n\\n')\n",
    "print(type(unique_test_labels))\n",
    "print(len(unique_test_labels))\n",
    "print(unique_train_labels)\n",
    "print(unique_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e906463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Бизнес' 'Интернет и СМИ' 'Культура' 'Политика' 'Спорт' 'Финансы'\n",
      " 'Экономика']\n",
      "6\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder().fit(unique_train_labels)\n",
    "print(le.classes_)\n",
    "print(le.transform(['Экономика']).item())\n",
    "print(le.transform(['Финансы']).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f31482ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Train_tokenizer.token_to_id('[UNK]'))\n",
    "print(Train_tokenizer.token_to_id('[PAD]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808c6ba",
   "metadata": {},
   "source": [
    "создаем класс падинг который дополняем предложения специальным токеном pad (точнее его индексом)    \n",
    "это требуется для того, чтобы можно было создавать тензоры одинаковой размерности для последующей обработки моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "814d2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding():\n",
    "    def __init__(self,pad = 945,dtype = torch.long):\n",
    "        self.pad = pad\n",
    "        self.dtype = dtype\n",
    "    def __call__(self,tens):\n",
    "        ones = torch.ones(self.pad,dtype = self.dtype)\n",
    "        for ind in range(tens.shape[0]):\n",
    "            ones[ind] = tens[ind]\n",
    "        return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ef41a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OnesPadding = Padding(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e596293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tens = torch.tensor([1,2,3],dtype = torch.long)\n",
    "some_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "484520da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 1, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnesPadding(some_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540141b",
   "metadata": {},
   "source": [
    "создаем датасет для новостей (этот датасет будет уже использоваться моделями предсказания) (смотреть аналитическую записку) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4b49ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataset for word indices and padding\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self,the_data,le,tokenizer,transforms = None):\n",
    "        self.main_data = the_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = le\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        X = [self.tokenizer.token_to_id(one_token) if self.tokenizer.token_to_id(one_token) != None else self.tokenizer.token_to_id('[UNK]') for one_token in normalize_text(self.main_data.iloc[indx,0])]\n",
    "        y = self.label_encoder.transform([self.main_data.iloc[indx,1]]).item()\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "941c5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = NewsDataset(Train_news,le,Train_tokenizer,transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "410be2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TrainDataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "12386fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the maximum length of the sentence, so that we use the necessary padding here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "71274cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_max_len_of_sents = 0\n",
    "for one_item in TrainDataset:\n",
    "    the_max_len_of_sents = max(the_max_len_of_sents,len(one_item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "57807159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_max_len_of_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "84c56c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataset = NewsDataset(Test_news,le,Test_tokenizer,transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5d5e150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_test = 0\n",
    "for one_item in TestDataset:\n",
    "    max_len_test = max(max_len_test,one_item[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7fcd938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6deb654",
   "metadata": {},
   "source": [
    "немного подправили датасет сверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3c5b1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE MAIN Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self,the_data,le,tokenizer,transforms = None):\n",
    "        self.main_data = the_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = le\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        X = [self.tokenizer.token_to_id(one_token) if self.tokenizer.token_to_id(one_token) != None else self.tokenizer.token_to_id('[UNK]') for one_token in normalize_text(self.main_data.iloc[indx,0])]\n",
    "        y = self.label_encoder.transform([self.main_data.iloc[indx,1]]).item()\n",
    "        X,y = torch.tensor(X,dtype = torch.long),torch.tensor(y,dtype = torch.long)\n",
    "        return (X,y) if self.transforms == None else (self.transforms(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c10a43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = NewsDataset(Train_news,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "4d4d5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataset = NewsDataset(Test_news,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f053614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking weather everything matches\n",
    "for one_sample in TrainDataset:\n",
    "    if one_sample[0].shape[0] != 945:\n",
    "        print('SHORTER than it should be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "48093d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x76c994a77b00>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_loader = DataLoader(dataset = TrainDataset,shuffle = True,batch_size = 32)\n",
    "Train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "46dc1cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x76c97da6cbc0>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_loader = DataLoader(dataset = TestDataset,shuffle = True,batch_size = 1)\n",
    "Test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8a7ca0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_iter = next(iter(Test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9823c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in Test_loader:\n",
    "    X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "29d8482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in Train_loader:\n",
    "    X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a6a0d",
   "metadata": {},
   "source": [
    "итак создаем первую модель предсказания CNN (подробнее смотреть в аналитической записке )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "f9b02eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramCNNMod(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed = 100,ker_sizes = [2,5,11]):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed,padding_idx=1)\n",
    "        self.conv1 = nn.Conv2d(1,111,kernel_size=(ker_sizes[0],dim_embed))\n",
    "        self.conv2 = nn.Conv2d(1,111,kernel_size=(ker_sizes[1],dim_embed))\n",
    "        self.conv3 = nn.Conv2d(1,111,kernel_size=(ker_sizes[2],dim_embed))\n",
    "        self.lin1 = nn.Linear(333,7)\n",
    "\n",
    "        self.mp1 = nn.MaxPool1d(kernel_size=945-ker_sizes[0]+1)\n",
    "        self.mp2 = nn.MaxPool1d(kernel_size=945-ker_sizes[1]+1)\n",
    "        self.mp3 = nn.MaxPool1d(kernel_size=945-ker_sizes[2]+1)\n",
    "        self.drop = nn.Dropout1d(p = 0.5)\n",
    "\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,inds_list):\n",
    "        y = self.embed(inds_list)\n",
    "        y = y.unsqueeze(dim = 1)\n",
    "\n",
    "        y1 = self.rel(self.conv1(y))\n",
    "        y2 = self.rel(self.conv2(y))\n",
    "        y3 = self.rel(self.conv3(y))\n",
    "\n",
    "        y1 = y1.squeeze(dim = -1)\n",
    "        y2 = y2.squeeze(dim = -1)\n",
    "        y3 = y3.squeeze(dim = -1)\n",
    "        \n",
    "        y1 = self.mp1(y1)\n",
    "        y2 = self.mp2(y2)\n",
    "        y3 = self.mp3(y3)\n",
    "\n",
    "        y = torch.concat([y1,y2,y3],dim = -1)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        y = self.drop(y)\n",
    "        y = self.rel(self.lin1(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "ccb8b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(57568, 100, padding_idx=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NgramCNNMod(\n",
       "  (embed): Embedding(57568, 100, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 100), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(11, 100), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=935, stride=935, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramcnnmod = NgramCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=100,ker_sizes=[2,5,11])\n",
    "ngramcnnmod.embed.weight = ngram_model.embed.weight\n",
    "print(ngramcnnmod.embed)\n",
    "ngramcnnmod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554afd0",
   "metadata": {},
   "source": [
    "тут проводятся некотрые проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "28bfe0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 100])"
      ]
     },
     "execution_count": 1104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramcnnmod.embed(torch.tensor([[1,2],[3,4]]).to(device = 'cuda')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "9e92293a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 945])"
      ]
     },
     "execution_count": 1105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample = next(iter(Train_loader))[0].to(device = 'cuda')\n",
    "one_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "5c330650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 945])"
      ]
     },
     "execution_count": 1106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_tens = torch.from_numpy(np.random.randint(1,100,size = (1,945))).to(device=  'cuda')\n",
    "one_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "656fb2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7])"
      ]
     },
     "execution_count": 1107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mod = ngramcnnmod(one_sample)\n",
    "output_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in output_mod:\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678d554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18560"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "bccc0f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0002)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.1365, 0.1474, 0.1432, 0.1365, 0.1411, 0.1365, 0.1590]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "afea1004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22430 934\n"
     ]
    }
   ],
   "source": [
    "MainTrain,MainVal = random_split(TrainDataset,lengths=[0.96,0.04])\n",
    "print(len(MainTrain),len(MainVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "365ef3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_classes = []\n",
    "for X,y in MainTrain:\n",
    "    the_classes.append(int(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "63904345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(the_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df62b91",
   "metadata": {},
   "source": [
    "в процессе обработки данных было замечено, что даатсет несбалансирлванный    \n",
    "в этой связи было принятоо решение посичтать веся для функции потерб"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "75729784",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_weights = compute_class_weight(class_weight= 'balanced',classes=np.unique(the_classes),y=np.array(the_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "id": "6815f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2875652 , 0.01068498, 0.01108492, 0.38403869, 0.01106226,\n",
       "       0.28481338, 0.01075059])"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_weights = the_weights/the_weights.sum()\n",
    "scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "57e31b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2876, 0.0107, 0.0111, 0.3840, 0.0111, 0.2848, 0.0108])"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train_weights = torch.tensor(scaled_weights,dtype = torch.float32)\n",
    "torch_train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "1401caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_dataset = NewsDataset(Train_news[:10],le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b1604",
   "metadata": {},
   "source": [
    "создаем функцию которая по модели будет считать ее точность на представленном датасете    \n",
    "точность будет основной метриков в данной работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "5ae66ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_accuracy(model,dataset,dev = 'cuda'):\n",
    "    model.eval()\n",
    "    model.to(device = dev)\n",
    "\n",
    "    N = len(dataset)\n",
    "    correct = 0\n",
    "\n",
    "    the_loader = DataLoader(dataset = dataset,shuffle=True,batch_size=1)\n",
    "    for X,y in the_loader:\n",
    "        X,y = X.to(device= dev),y.to(device= dev)\n",
    "        y_pred = model(X).softmax(dim = 1).argmax(dim = 1).item()\n",
    "        y = y.item()\n",
    "        if y == y_pred:\n",
    "            correct += 1\n",
    "    return round(correct/N,5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "6328cb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01178"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(ngramcnnmod,MainVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "2878847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0457, -0.3225,  0.0623,  ...,  0.1534, -0.1295,  0.5025],\n",
       "        [ 0.4798, -0.5573,  0.7445,  ..., -0.3655,  0.4920,  1.2093],\n",
       "        [ 0.6104, -0.5248,  1.0010,  ...,  0.4128,  0.6423, -0.1973],\n",
       "        ...,\n",
       "        [ 0.5436,  0.6824,  0.4370,  ...,  0.4218, -1.4919,  1.1227],\n",
       "        [ 0.7240, -0.2097,  0.7371,  ...,  0.0740, -0.9405,  1.1640],\n",
       "        [-1.2459, -0.7283, -1.6251,  ..., -1.4985, -0.8383, -1.2114]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 1172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramcnnmod.embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0db9a",
   "metadata": {},
   "source": [
    "инициализируем модель и проводим основной цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "8c0b2e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(57568, 100, padding_idx=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NgramCNNMod(\n",
       "  (embed): Embedding(57568, 100, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 100), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=942, stride=942, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 1171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramcnnmod = NgramCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=100,ker_sizes=[2,4,5])\n",
    "ngramcnnmod.embed.weight = ngram_model.embed.weight\n",
    "print(ngramcnnmod.embed)\n",
    "ngramcnnmod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "ca79ef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2876, 0.0107, 0.0111, 0.3840, 0.0111, 0.2848, 0.0108])"
      ]
     },
     "execution_count": 1173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "4f29d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader = DataLoader(dataset = MainTrain,shuffle=True,batch_size=45)\n",
    "epoch = 100\n",
    "lr = 0.00008\n",
    "optimizer = torch.optim.Adam(lr = lr,params = ngramcnnmod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "25424bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_classification_model(the_model,epoch,loader,optimizer,loss_func,val_dataset,dev = 'cuda'):\n",
    "    average_ep_losses = []\n",
    "    best_epoch = None\n",
    "    val_accs = []\n",
    "    try:\n",
    "        for one_ep in range(epoch):\n",
    "            the_model.train()\n",
    "            the_losses = []\n",
    "            for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "                X,y = X.to(device = dev),y.to(device = dev)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = the_model(X)\n",
    "                the_loss = loss_func(y_pred,y)\n",
    "                the_loss.backward()\n",
    "                the_losses.append(the_loss.item())\n",
    "                optimizer.step()\n",
    "            if one_ep%1== 0:\n",
    "                avg_loss = np.array(the_losses).mean()\n",
    "                average_ep_losses.append(avg_loss)\n",
    "                val_score = get_accuracy(the_model,val_dataset,dev =dev)\n",
    "                val_accs.append(val_score)\n",
    "\n",
    "                if best_epoch == None:\n",
    "                    best_epoch = one_ep + 1\n",
    "                else:\n",
    "                    best_epoch = one_ep + 1 if val_accs[-1] > val_accs[-2] else best_epoch\n",
    "\n",
    "                print(f'Epoch # {one_ep+1} | Loss: {avg_loss} | Val acc: {val_score} | Best acc on epoch #{best_epoch}',end = '\\n\\n')\n",
    "        return best_epoch,average_ep_losses,val_accs\n",
    "    except KeyboardInterrupt:\n",
    "        return best_epoch,average_ep_losses,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "5811315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.8663510155821132 | Val acc: 0.49358 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.7411002318462532 | Val acc: 0.55996 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.647804072481358 | Val acc: 0.58137 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.5654594163617535 | Val acc: 0.60385 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.513895118881562 | Val acc: 0.60707 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.4992192710090975 | Val acc: 0.6167 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.4609081176574341 | Val acc: 0.62206 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.419200757820764 | Val acc: 0.62848 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.4002165766899475 | Val acc: 0.63919 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.3796466817717274 | Val acc: 0.63919 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.3594645166803219 | Val acc: 0.64668 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.3349026629107748 | Val acc: 0.64989 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.3780407669071204 | Val acc: 0.64882 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.3139220670851055 | Val acc: 0.65953 | Best acc on epoch #14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.2986505340956496 | Val acc: 0.66488 | Best acc on epoch #15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.3049662966646987 | Val acc: 0.65846 | Best acc on epoch #15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.2999512526339185 | Val acc: 0.66702 | Best acc on epoch #17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.2989628381265668 | Val acc: 0.66595 | Best acc on epoch #17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.2874569529760815 | Val acc: 0.66488 | Best acc on epoch #17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.2657920808973675 | Val acc: 0.66916 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.2795108680376308 | Val acc: 0.67238 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.2755679581948893 | Val acc: 0.67238 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.2822679652121358 | Val acc: 0.67024 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.274586511518291 | Val acc: 0.67452 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.2664132090154774 | Val acc: 0.6788 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.2758818759707984 | Val acc: 0.67987 | Best acc on epoch #26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.2590960868016512 | Val acc: 0.68094 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:39<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.272218721782516 | Val acc: 0.68201 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.270583929363377 | Val acc: 0.68094 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.269055374698314 | Val acc: 0.68415 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.2442996859072684 | Val acc: 0.68415 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.2505052211528311 | Val acc: 0.67666 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.2586373649164289 | Val acc: 0.6863 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.2684249720855323 | Val acc: 0.68094 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.2668320738480898 | Val acc: 0.68737 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.245291234854467 | Val acc: 0.68415 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.2496771103513982 | Val acc: 0.68415 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.2559009049244538 | Val acc: 0.68308 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.2627620532182988 | Val acc: 0.68522 | Best acc on epoch #39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.2715711540473487 | Val acc: 0.68308 | Best acc on epoch #39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.276884485581117 | Val acc: 0.6863 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:39<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.2380906877990716 | Val acc: 0.68308 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.2372292006063557 | Val acc: 0.68522 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 44 | Loss: 1.2671031668513475 | Val acc: 0.68415 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 45 | Loss: 1.252553538353983 | Val acc: 0.68094 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 46 | Loss: 1.2563118931047903 | Val acc: 0.68951 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 47 | Loss: 1.249796578544892 | Val acc: 0.6863 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 48 | Loss: 1.2480562289874395 | Val acc: 0.68522 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 49 | Loss: 1.2633115661168146 | Val acc: 0.68094 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50 | Loss: 1.2503758010142791 | Val acc: 0.6863 | Best acc on epoch #50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 51 | Loss: 1.2487192162292038 | Val acc: 0.68308 | Best acc on epoch #50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 52 | Loss: 1.2630148654829763 | Val acc: 0.68522 | Best acc on epoch #52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:39<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 53 | Loss: 1.2532634152797515 | Val acc: 0.68308 | Best acc on epoch #52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 54 | Loss: 1.278036791659548 | Val acc: 0.68737 | Best acc on epoch #54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 55 | Loss: 1.2427701256079282 | Val acc: 0.68844 | Best acc on epoch #55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 56 | Loss: 1.2784411938849813 | Val acc: 0.68522 | Best acc on epoch #55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 57 | Loss: 1.2537257435446034 | Val acc: 0.68844 | Best acc on epoch #57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 58 | Loss: 1.2664136248743367 | Val acc: 0.68951 | Best acc on epoch #58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:41<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 59 | Loss: 1.2619863353176919 | Val acc: 0.68415 | Best acc on epoch #58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 60 | Loss: 1.233722197746705 | Val acc: 0.68522 | Best acc on epoch #60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 61 | Loss: 1.2611143731163117 | Val acc: 0.68415 | Best acc on epoch #60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 62 | Loss: 1.2497663333683549 | Val acc: 0.6863 | Best acc on epoch #62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 63 | Loss: 1.2432720811668045 | Val acc: 0.6863 | Best acc on epoch #62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 64 | Loss: 1.26290546945914 | Val acc: 0.68737 | Best acc on epoch #64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 65 | Loss: 1.2369022607086655 | Val acc: 0.68201 | Best acc on epoch #64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 66 | Loss: 1.2383731955397344 | Val acc: 0.68737 | Best acc on epoch #66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 67 | Loss: 1.2533120912516522 | Val acc: 0.69165 | Best acc on epoch #67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 68 | Loss: 1.2923994168848216 | Val acc: 0.69058 | Best acc on epoch #67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:39<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 69 | Loss: 1.2679876569875017 | Val acc: 0.68844 | Best acc on epoch #67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 499/499 [00:40<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 70 | Loss: 1.2511635988293763 | Val acc: 0.68844 | Best acc on epoch #67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  55%|█████▌    | 275/499 [00:22<00:18, 12.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#time is 53 minnutes\n",
    "cnn_results = train_classification_model(ngramcnnmod,epoch,train_loader,optimizer,loss_func,MainVal,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "id": "7e48c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_losses = cnn_results[1]\n",
    "the_scores = cnn_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "id": "6933e747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x76c97dee69c0>"
      ]
     },
     "execution_count": 1272,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_figure = plt.figure(figsize = (10,5),facecolor = 'skyblue')\n",
    "some_ax = some_figure.add_subplot()\n",
    "some_ax.grid(linestyle = '--',c = 'gray',alpha = 0.4)\n",
    "some_ax.plot(the_scores,c = 'green',label = 'Accuracy on Validation')\n",
    "some_ax.plot(the_losses,color = 'purple',label  = 'Average error rate')\n",
    "some_ax.set_ylabel('Accuracy - Error')\n",
    "some_ax.set_xlabel('Epoch')\n",
    "some_ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "4d9dad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_figure.savefig('/home/luchian/all_data/uni_data/val_48.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "7232b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(57568, 100, padding_idx=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NgramCNNMod(\n",
       "  (embed): Embedding(57568, 100, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 100), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=942, stride=942, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 1177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_100_embeddings.pth',weights_only = True))\n",
    "ngramcnnmod = NgramCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=100,ker_sizes=[2,4,5])\n",
    "ngramcnnmod.embed.weight = ngram_model.embed.weight\n",
    "print(ngramcnnmod.embed)\n",
    "ngramcnnmod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "b6604062",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=45)\n",
    "epoch = 25\n",
    "lr = 0.00005\n",
    "optimizer = torch.optim.Adam(lr = lr,params = ngramcnnmod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "401895bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.899601865044007 | Val acc: 0.37408 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:43<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.817601461135424 | Val acc: 0.4315 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.7601632396762188 | Val acc: 0.4548 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.7186798946215556 | Val acc: 0.45139 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.6766660290268751 | Val acc: 0.46731 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.666257315644851 | Val acc: 0.45765 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.6337176936177107 | Val acc: 0.47243 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.5974375266295213 | Val acc: 0.47698 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.579207618190692 | Val acc: 0.47641 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.5791923078206869 | Val acc: 0.47527 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.5658939520900066 | Val acc: 0.47811 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.5525419179063578 | Val acc: 0.48039 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.4902894253914172 | Val acc: 0.47925 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.5104087794056305 | Val acc: 0.47925 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:43<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.4911682434953175 | Val acc: 0.47982 | Best acc on epoch #15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.4871607064054562 | Val acc: 0.48209 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.4953259321359487 | Val acc: 0.48039 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.4827568783209875 | Val acc: 0.48152 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.4648281867687518 | Val acc: 0.4838 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.4748991367622064 | Val acc: 0.48266 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.4761326182347076 | Val acc: 0.4838 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:42<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.463422204554081 | Val acc: 0.48607 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.4445023076465497 | Val acc: 0.48493 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.449032515172775 | Val acc: 0.4855 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 520/520 [00:41<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.4261510462141953 | Val acc: 0.48493 | Best acc on epoch #24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_cnn_ngram_100 = train_classification_model(ngramcnnmod,epoch,train_loader,optimizer,loss_func,TestDataset,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "id": "b5513282",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_losses = final_cnn_ngram_100[1]\n",
    "the_scores = final_cnn_ngram_100[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "id": "7f8d7fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x76c97dd2b6b0>"
      ]
     },
     "execution_count": 1278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_figure = plt.figure(figsize = (10,5),facecolor = 'skyblue')\n",
    "some_ax = some_figure.add_subplot()\n",
    "some_ax.grid(linestyle = '--',c = 'gray',alpha = 0.4)\n",
    "some_ax.plot(the_scores,c = 'green',label = 'Accuracy on Test')\n",
    "some_ax.plot(the_losses,color = 'purple',label  = 'Average error rate')\n",
    "some_ax.set_ylabel('Accuracy - Error')\n",
    "some_ax.set_xlabel('Epoch')\n",
    "some_ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "df924e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_figure.savefig('/home/luchian/all_data/uni_data/test_48.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807f135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(ngramcnnmod.state_dict(),'/home/luchian/all_data/uni_data/NgramCNNmod_acc_48.pth')\n",
    "# ngramcnnmod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/NgramCNNmod_acc_48.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43181719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48493"
      ]
     },
     "execution_count": 1184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_accuracy(ngramcnnmod,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64002831",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################RECAP\n",
    "#CBOW\n",
    "#NGRAM\n",
    "#GloVe\n",
    "#300 embedding dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f978d51",
   "metadata": {},
   "source": [
    "далее представленый ячейки в которых некотрые данные снова загружаются для удобства загрузки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b240e",
   "metadata": {},
   "source": [
    "также там представлены модели cbow и glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5eee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASETS CLASSES\n",
    "\n",
    "#Embedding Datasest\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self,tokenizer,the_data):\n",
    "        self.main_data = the_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        X = torch.tensor([self.tokenizer.token_to_id(word) if self.tokenizer.token_to_id(word) != None else self.tokenizer.token_to_id('[UNK]') for word in self.main_data[indx][0]],dtype=torch.long)\n",
    "        true_word = self.main_data[indx][1]\n",
    "        y = self.tokenizer.token_to_id(true_word) if self.tokenizer.token_to_id(true_word) != None else self.tokenizer.token_to_id('[UNK]')\n",
    "        return X,y\n",
    "    \n",
    "#News Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self,the_data,le,tokenizer,transforms = None):\n",
    "        self.main_data = the_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = le\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_data)\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "        X = [self.tokenizer.token_to_id(one_token) if self.tokenizer.token_to_id(one_token) != None else self.tokenizer.token_to_id('[UNK]') for one_token in normalize_text(self.main_data.iloc[indx,0])]\n",
    "        y = self.label_encoder.transform([self.main_data.iloc[indx,1]]).item()\n",
    "        X,y = torch.tensor(X,dtype = torch.long),torch.tensor(y,dtype = torch.long)\n",
    "        return (X,y) if self.transforms == None else (self.transforms(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71552b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIFFERENT MODELS\n",
    "\n",
    "\n",
    "#EmbedCNNMod\n",
    "class EmbedCNNMod(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed = 300,ker_sizes = [2,4,5]):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed,padding_idx=1)\n",
    "        self.conv1 = nn.Conv2d(1,111,kernel_size=(ker_sizes[0],dim_embed))\n",
    "        self.conv2 = nn.Conv2d(1,111,kernel_size=(ker_sizes[1],dim_embed))\n",
    "        self.conv3 = nn.Conv2d(1,111,kernel_size=(ker_sizes[2],dim_embed))\n",
    "        self.lin1 = nn.Linear(333,7)\n",
    "\n",
    "        self.mp1 = nn.MaxPool1d(kernel_size=945-ker_sizes[0]+1)\n",
    "        self.mp2 = nn.MaxPool1d(kernel_size=945-ker_sizes[1]+1)\n",
    "        self.mp3 = nn.MaxPool1d(kernel_size=945-ker_sizes[2]+1)\n",
    "        self.drop = nn.Dropout1d(p = 0.5)\n",
    "\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,inds_list):\n",
    "        y = self.embed(inds_list)\n",
    "        if len(y.size()) == 3:\n",
    "            y = y.unsqueeze(dim = 1)\n",
    "        else:\n",
    "            y = y.unsqueeze(dim = 0)\n",
    "            y = y.unsqueeze(dim = 0)\n",
    "\n",
    "        y1 = self.rel(self.conv1(y))\n",
    "        y2 = self.rel(self.conv2(y))\n",
    "        y3 = self.rel(self.conv3(y))\n",
    "\n",
    "        y1 = y1.squeeze(dim = -1)\n",
    "        y2 = y2.squeeze(dim = -1)\n",
    "        y3 = y3.squeeze(dim = -1)\n",
    "        \n",
    "        y1 = self.mp1(y1)\n",
    "        y2 = self.mp2(y2)\n",
    "        y3 = self.mp3(y3)\n",
    "\n",
    "        y = torch.concat([y1,y2,y3],dim = -1)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        y = self.drop(y)\n",
    "        y = self.rel(self.lin1(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "#ngram model \n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed,window_size = 2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed)\n",
    "        #from both sides\n",
    "        self.linear1 = nn.Linear(2*window_size*dim_embed,128)\n",
    "        self.linear2 = nn.Linear(128,vocab_size)\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,ind_list):\n",
    "        y = self.embed(ind_list)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        y = self.rel(self.linear1(y))\n",
    "        y = self.rel(self.linear2(y))\n",
    "        return y\n",
    "    \n",
    "#CBOW model \n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed,window_size = 2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed)\n",
    "        self.linear1 = nn.Linear(dim_embed,128)\n",
    "        self.linear2 = nn.Linear(128,vocab_size)\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,ind_list):\n",
    "        y = self.embed(ind_list).sum(dim = 1)\n",
    "        y = self.rel(self.linear1(y))\n",
    "        y = self.rel(self.linear2(y))\n",
    "        return y\n",
    "    \n",
    "#GloVe model\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim = 150,dev = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_dim,padding_idx = 1)\n",
    "        self.bias = torch.randn(1).to(device = dev)\n",
    "\n",
    "    def forward(self,inds_tensor):\n",
    "        ind1 = inds_tensor[:1]\n",
    "        ind2 = inds_tensor[1:]\n",
    "        vec1 = self.embed(ind1).reshape(-1)\n",
    "        vec2 = self.embed(ind2).reshape(-1)\n",
    "        return vec1.dot(vec2) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddea1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAIN FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function that prints losses and val scores\n",
    "def print_loss_val(the_losses,the_vals,val_name = 'Validation'):\n",
    "    some_figure = plt.figure(figsize = (10,5),facecolor = 'skyblue')\n",
    "    some_ax = some_figure.add_subplot()\n",
    "    some_ax.grid(linestyle = '--',c = 'gray',alpha = 0.4)\n",
    "    some_ax.plot(the_vals,c = 'green',label = 'Accuracy on ' + val_name)\n",
    "    some_ax.plot(the_losses,color = 'purple',label  = 'Average error rate')\n",
    "    some_ax.set_ylabel('Accuracy - Error')\n",
    "    some_ax.set_xlabel('Epoch')\n",
    "    some_ax.legend()\n",
    "    return some_figure\n",
    "\n",
    "#function for training the classification model\n",
    "def train_classification_model(the_model,epoch,loader,optimizer,loss_func,val_dataset,dev = 'cuda'):\n",
    "    average_ep_losses = []\n",
    "    best_epoch = None\n",
    "    val_accs = []\n",
    "    try:\n",
    "        for one_ep in range(epoch):\n",
    "            the_model.train()\n",
    "            the_losses = []\n",
    "            for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "                X,y = X.to(device = dev),y.to(device = dev)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = the_model(X)\n",
    "                the_loss = loss_func(y_pred,y)\n",
    "                the_loss.backward()\n",
    "                the_losses.append(the_loss.item())\n",
    "                optimizer.step()\n",
    "            if one_ep%1== 0:\n",
    "                avg_loss = np.array(the_losses).mean()\n",
    "                average_ep_losses.append(avg_loss)\n",
    "                val_score = get_accuracy(the_model,val_dataset,dev =dev)\n",
    "                val_accs.append(val_score)\n",
    "\n",
    "                if best_epoch == None:\n",
    "                    best_epoch = one_ep + 1\n",
    "                else:\n",
    "                    best_epoch = one_ep + 1 if val_accs[-1] > val_accs[-2] else best_epoch\n",
    "\n",
    "                print(f'Epoch # {one_ep+1} | Loss: {avg_loss} | Val acc: {val_score} | Best acc on epoch #{best_epoch}',end = '\\n\\n')\n",
    "        return best_epoch,average_ep_losses,val_accs\n",
    "    except KeyboardInterrupt:\n",
    "        return best_epoch,average_ep_losses,val_accs\n",
    "\n",
    "def train_ngram_model(the_model,epoch,loader,optimizer,loss_func,dev = 'cuda',save_path = '1'):\n",
    "    for one_ep in range(epoch):\n",
    "        torch.save(the_model.state_dict(),save_path)\n",
    "        the_model.train()\n",
    "        the_losses = []\n",
    "        for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "            X,y = X.to(device = 'cuda'),y.to(device = 'cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = the_model(X)\n",
    "            the_loss = loss_func(y_pred,y)\n",
    "            the_loss.backward()\n",
    "            the_losses.append(the_loss.item())\n",
    "            optimizer.step()\n",
    "        if one_ep%1== 0:\n",
    "            print(f'Current Loss: {np.array(the_losses).mean()}')\n",
    "\n",
    "def train_cbow_model(the_model,epoch,loader,optimizer,loss_func,dev = 'cuda'):\n",
    "    for one_ep in range(epoch):\n",
    "        the_model.train()\n",
    "        the_losses = []\n",
    "        for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "            X,y = X.to(device = 'cuda'),y.to(device = 'cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = the_model(X)\n",
    "            the_loss = loss_func(y_pred,y)\n",
    "            the_loss.backward()\n",
    "            the_losses.append(the_loss.item())\n",
    "            optimizer.step()\n",
    "        if one_ep%1== 0:\n",
    "            print(f'Current Loss: {np.array(the_losses).mean()}')\n",
    "\n",
    "class Padding():\n",
    "    def __init__(self,pad = 945,dtype = torch.long,dev = 'cpu'):\n",
    "        self.pad = pad\n",
    "        self.dtype = dtype\n",
    "        self.dev = dev\n",
    "    def __call__(self,tens):\n",
    "        ones = torch.ones(self.pad,dtype = self.dtype).to(device = self.dev)\n",
    "        for ind in range(tens.shape[0]):\n",
    "            ones[ind] = tens[ind]\n",
    "        return ones\n",
    "    \n",
    "#accuracy score\n",
    "@torch.no_grad()\n",
    "def get_accuracy(model,dataset,dev = 'cuda'):\n",
    "    model.eval()\n",
    "    model.to(device = dev)\n",
    "\n",
    "    N = len(dataset)\n",
    "    correct = 0\n",
    "\n",
    "    the_loader = DataLoader(dataset = dataset,shuffle=True,batch_size=1)\n",
    "    for X,y in the_loader:\n",
    "        X,y = X.to(device= dev),y.to(device= dev)\n",
    "        y_pred = model(X).softmax(dim = 1).argmax(dim = 1).item()\n",
    "        y = y.item()\n",
    "        if y == y_pred:\n",
    "            correct += 1\n",
    "    return round(correct/N,5)\n",
    "\n",
    "\n",
    "#text normalization\n",
    "def normalize_text(the_text):\n",
    "    lowered = the_text.lower()\n",
    "    found = re.findall(r'[А-я0-9A-z]+',lowered)\n",
    "    filtered_found = [word for word in found if word != '' and word not in russian_stopwords]\n",
    "    return filtered_found\n",
    "\n",
    "\n",
    "def cleared_texts(text_col):\n",
    "    for ith_row in range(len(text_col)):\n",
    "        for token in normalize_text(text_col.iloc[ith_row]):\n",
    "            yield token\n",
    "\n",
    "\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "print(len(russian_stopwords))\n",
    "russian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd1a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Бизнес' 'Интернет и СМИ' 'Культура' 'Политика' 'Спорт' 'Финансы'\n",
      " 'Экономика']\n",
      "22430 934\n",
      "tensor([0.2881, 0.0106, 0.0111, 0.3776, 0.0111, 0.2908, 0.0107])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GloveModel(\n",
       "  (embed): Embedding(57568, 150, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOADING\n",
    "\n",
    "#some variables that need to be saved\n",
    "the_max_len_of_sents = 945\n",
    "\n",
    "#loading saved tokenizers\n",
    "\n",
    "with open('Train_tokenizer.pkl','rb') as file:\n",
    "    Train_tokenizer = pickle.load(file)\n",
    "\n",
    "with open('Test_tokenizer.pkl','rb') as file:\n",
    "    Test_tokenizer = pickle.load(file)\n",
    "\n",
    "#loading datasets\n",
    "Train_news,Test_news = pd.read_csv('Train_news.csv',index_col=0),pd.read_csv('Test_news.csv',index_col=0)\n",
    "\n",
    "#load dataset for ngram and cbow models\n",
    "with open('/home/luchian/all_data/uni_data/Train_Ngramsdataset','rb') as file:\n",
    "    Train_Ngramsdataset = pickle.load(file)\n",
    "\n",
    "#loading labels\n",
    "unique_train_labels = Train_news['topic'].unique()\n",
    "le = preprocessing.LabelEncoder().fit(unique_train_labels)\n",
    "print(le.classes_)\n",
    "\n",
    "#loading torhch datasets\n",
    "TrainDataset = NewsDataset(Train_news,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))\n",
    "TestDataset = NewsDataset(Test_news,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))\n",
    "\n",
    "#computing classes\n",
    "the_classes = []\n",
    "for X,y in TrainDataset:\n",
    "    the_classes.append(int(y))\n",
    "\n",
    "#Train and Validation\n",
    "MainTrain,MainVal = random_split(TrainDataset,lengths=[0.96,0.04])\n",
    "print(len(MainTrain),len(MainVal))\n",
    "\n",
    "the_weights = compute_class_weight(class_weight= 'balanced',classes=np.unique(the_classes),y=np.array(the_classes))\n",
    "scaled_weights = the_weights/the_weights.sum()\n",
    "torch_train_weights = torch.tensor(scaled_weights,dtype = torch.float32)\n",
    "print(torch_train_weights)\n",
    "\n",
    "\n",
    "#embeddings models\n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "ngram_model = NGramModel(vocab_size,dim_embed = 300,window_size = 2)\n",
    "\n",
    "\n",
    "#TRAINING Glove Model \n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "glove_mod = GloveModel(vocab_size,embed_dim = 150)\n",
    "glove_mod.to(device = 'cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af740ad3",
   "metadata": {},
   "source": [
    "создаем модель ngram но уже с размерность 300 (полное обучения смотреть в ноутбуке goolge_grain.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b98ce1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramModel(\n",
       "  (embed): Embedding(57568, 300)\n",
       "  (linear1): Linear(in_features=1200, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=57568, bias=True)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING NGRAM model \n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "ngram_model = NGramModel(vocab_size,dim_embed = 300,window_size = 2)\n",
    "ngram_model.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13868c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_ngram_300 = DataLoader(dataset = Train_Ngramsdataset,shuffle = True,batch_size = 24)\n",
    "lr = 0.002\n",
    "epoch = 50\n",
    "optimizer_ngram_300 = torch.optim.SGD(lr = lr,params= ngram_model.parameters(),momentum=0.3)\n",
    "the_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bca777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [52:20<00:00, 41.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 9.59820879176584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [1:46:07<00:00, 20.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.753965046482305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 131403/131403 [1:57:13<00:00, 18.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.484169470494848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  55%|█████▍    | 71905/131403 [1:23:35<1:09:09, 14.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_ngram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader_ngram_300\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer_ngram_300\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthe_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m, in \u001b[0;36mtrain_ngram_model\u001b[0;34m(the_model, epoch, loader, optimizer, loss_func, dev)\u001b[0m\n\u001b[1;32m     47\u001b[0m     the_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     48\u001b[0m     the_losses\u001b[38;5;241m.\u001b[39mappend(the_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 49\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m one_ep\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(the_losses)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/optim/optimizer.py:472\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    471\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 472\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/autograd/profiler.py:733\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the model was trained in google colab\n",
    "train_ngram_model(ngram_model,epoch,train_loader_ngram_300,optimizer_ngram_300,the_loss,dev = 'cuda',save_path='/home/luchian/all_data/uni_data/ngram_300_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a33c2721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4625, -2.8590, -0.2006,  ...,  0.0446,  0.1728,  0.1914],\n",
       "        [ 0.6954, -0.7205,  1.4831,  ...,  0.7945, -0.1423,  1.7137],\n",
       "        [-0.4005,  2.2723, -0.0741,  ..., -0.7002,  0.2441, -0.3373],\n",
       "        ...,\n",
       "        [-0.5664, -0.0742,  1.0343,  ..., -0.6893,  2.0791,  1.2044],\n",
       "        [-0.4950,  0.5707,  1.4642,  ..., -1.8228, -0.6479, -2.0907],\n",
       "        [ 1.9204,  0.7842,  1.4448,  ..., -1.3439,  0.3315,  0.1267]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e0376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(ngram_model.state_dict(),'/home/luchian/all_data/uni_data/ngram_300_embeddings.pth')\n",
    "# torch.save(optimizer_ngram_300.state_dict(),'/home/luchian/all_data/uni_data/optimizer_ngram_300.pth')\n",
    "# optimizer_ngram_300.load_state_dict(torch.load('/home/luchian/all_data/uni_data/optimizer_ngram_300.pth',weights_only=True))\n",
    "# ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_300_embeddings.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dea07f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedCNNMod(\n",
       "  (embed): Embedding(57568, 300, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(5, 300), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=942, stride=942, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_300_cnn_mod = EmbedCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=300,ker_sizes=[2,4,5])\n",
    "ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_300_embeddings_epoch#9.pth',weights_only=True))\n",
    "ngram_300_cnn_mod.embed.weight = ngram_model.embed.weight\n",
    "ngram_300_cnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48192bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2881, 0.0106, 0.0111, 0.3776, 0.0111, 0.2908, 0.0107])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1f8e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21028 2336\n"
     ]
    }
   ],
   "source": [
    "MainTrain,MainVal = random_split(TrainDataset,lengths=[0.9,0.1])\n",
    "print(len(MainTrain),len(MainVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23da1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader_val_ngram_cnn = DataLoader(dataset = MainTrain,shuffle=True,batch_size=35)\n",
    "epoch = 50\n",
    "lr = 0.00005\n",
    "optimizer_val_ngram_cnn = torch.optim.Adam(lr = lr,params = ngram_300_cnn_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 641/641 [01:24<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.8861202682236242 | Val acc: 0.39507 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 641/641 [01:24<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.77881420207284 | Val acc: 0.42934 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 641/641 [01:24<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.6857086637648704 | Val acc: 0.44004 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 641/641 [01:25<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.6190637604866682 | Val acc: 0.45396 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  30%|██▉       | 191/641 [01:35<03:45,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "res_300 = train_classification_model(ngram_300_cnn_mod,epoch,train_loader_val_ngram_cnn,optimizer_val_ngram_cnn,loss_func,MainVal,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9f6ff52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model was trained in google colab\n",
    "ngram_300_cnn_mod.load_state_dict(torch.load('ngram_300_cnn_mod_epoch#20.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064de446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE CBOW MODEL  the model was trained in google colab\n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "cbow_model = CBOWModel(vocab_size,dim_embed = 300,window_size = 2)\n",
    "cbow_model.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5d341d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model was trained in google colab\n",
    "ngram_300_cnn_mod.load_state_dict(torch.load('cbow_300_cnn_mod_epoch#13.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fcb76",
   "metadata": {},
   "source": [
    "более менее нормальный результат для сверточной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5892005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71291"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(ngram_300_cnn_mod,dataset=TestDataset,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7021"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for CBOW model\n",
    "get_accuracy(ngram_300_cnn_mod,dataset=TestDataset,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47f85708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_docs(the_series,context_window):\n",
    "    docs  = []\n",
    "    for one_text in the_series:\n",
    "        normal = normalize_text(one_text)\n",
    "        N = len(normal)\n",
    "        if N >= 2*context_window + 1:\n",
    "            for ind in range(context_window,N-context_window):\n",
    "                docs.append(' '.join(normal[ind-context_window:ind+context_window]))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16f9c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2690373"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_for_freqs = get_context_docs(Train_news['Text'],context_window = 10)\n",
    "len(docs_for_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "526ffd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(token_pattern=&#x27;[^\\\\s]+&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>CountVectorizer(token_pattern=&#x27;[^\\\\s]+&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(token_pattern='[^\\\\s]+')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "288c3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count = vectorizer.fit_transform(docs_for_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8d16109",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_matrix = X_count.T@X_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bc3daaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Column sparse matrix of dtype 'int64'\n",
       "\twith 61030179 stored elements and shape (203323, 203323)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c1b428d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['благодарностей', 'благодарности', 'благодарность', ..., 'ящура',\n",
       "       'ящуре', 'ящуром'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "features[33235:333393]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0917746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {features[k]:k for k in range(len(features))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bd18b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2690373/2690373 [00:09<00:00, 285366.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_cont_words = dict()\n",
    "ind = 0\n",
    "for cont in tqdm(docs_for_freqs):\n",
    "    for word in cont.strip().split(' '):\n",
    "        dict_cont_words[word] = ind\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bda47c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_words_in_order = list(dict_cont_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f204df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train GloVe model\n",
    "def glove_dataset(words,context_window = 10):\n",
    "        for ind1 in range(len(words)-2*context_window):\n",
    "            for ind2 in range(ind1,ind1+2*context_window):\n",
    "                word1 = words[ind1]\n",
    "                word2 = words[ind2]\n",
    "                tok_ind1 = Train_tokenizer.token_to_id(word1)\n",
    "                tok_ind2 = Train_tokenizer.token_to_id(word2)\n",
    "                tok_values = torch.tensor([tok_ind1 if tok_ind1 != None else Train_tokenizer.token_to_id('[UNK]'),tok_ind2 if tok_ind2 != None else Train_tokenizer.token_to_id('[UNK]')])\n",
    "                freq_value = freq_matrix[feature_dict[word1],feature_dict[word2]]\n",
    "                yield tok_values,freq_value\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68b40429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object glove_dataset at 0x70b622d34fb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = glove_dataset(cont_words_in_order)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b816c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution took 60.29656666 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "one  = 1\n",
    "for one_sample in dataset:\n",
    "    1\n",
    "\n",
    "t1 = perf_counter()\n",
    "print(f\"Execution took {t1 - t0:.8f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0c3e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim = 150,dev = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_dim,padding_idx = 1)\n",
    "        self.bias = torch.randn(1).to(device = dev)\n",
    "\n",
    "    def forward(self,inds_tensor):\n",
    "        ind1 = inds_tensor[:1]\n",
    "        ind2 = inds_tensor[1:]\n",
    "        vec1 = self.embed(ind1).reshape(-1)\n",
    "        vec2 = self.embed(ind2).reshape(-1)\n",
    "        return vec1.dot(vec2) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "460afd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_func(x,alpha = 3/4,x_max = 100):\n",
    "    return (x/x_max)**alpha if x < x_max else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff376bbf",
   "metadata": {},
   "source": [
    "аналогично обучаем модель векторных представлений glove (подробнгее смотреть в аналитической записке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ad74d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE GLOVE MODEL\n",
    "def train_glove_model(the_model,epoch,optimizer,batch_size = 2000):\n",
    "    epoch_losses = []\n",
    "    for ep in tqdm(range(epoch)):\n",
    "        glove_data = glove_dataset(cont_words_in_order)\n",
    "\n",
    "        count = 0\n",
    "        batch_losses = []\n",
    "        the_loss = 0\n",
    "\n",
    "        for X,y in glove_data:\n",
    "            count += 1\n",
    "            X,y = X.to(device = 'cuda'),torch.tensor(y)\n",
    "            y_pred = the_model(X)\n",
    "            the_loss += weight_func(y)*(y_pred-torch.log(1+y))**2\n",
    "            if count == batch_size:\n",
    "                the_loss = the_loss/count\n",
    "                optimizer.zero_grad()\n",
    "                the_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_losses.append(round(the_loss.item(),5))\n",
    "                print(f'Current_batch_loss: {batch_losses[-1]}')\n",
    "                count,the_loss = 0,0\n",
    "\n",
    "        epoch_losses.append(round(sum(batch_losses)/len(batch_losses),5))                                                                                                                   \n",
    "        print(f'Epoch #{ep+1} | Loss: {epoch_losses[-1]}')\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c227e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GloveModel(\n",
       "  (embed): Embedding(57568, 150, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING Glove Model \n",
    "vocab_size = Train_tokenizer.get_vocab_size()\n",
    "glove_mod = GloveModel(vocab_size,embed_dim = 150)\n",
    "glove_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e61cbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(glove_mod.parameters(),lr = lr,momentum = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a064fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_batch_loss: 1080.91602\n",
      "Current_batch_loss: 1065.20007\n",
      "Current_batch_loss: 1116.8429\n",
      "Current_batch_loss: 1074.82983\n",
      "Current_batch_loss: 1094.55798\n",
      "Current_batch_loss: 1071.53796\n",
      "Current_batch_loss: 1084.45752\n",
      "Current_batch_loss: 1033.94348\n",
      "Current_batch_loss: 1021.28461\n",
      "Current_batch_loss: 1168.81372\n",
      "Current_batch_loss: 1083.32605\n",
      "Current_batch_loss: 1049.55676\n",
      "Current_batch_loss: 1104.68298\n",
      "Current_batch_loss: 1124.02393\n",
      "Current_batch_loss: 1098.63525\n",
      "Current_batch_loss: 1036.30396\n",
      "Current_batch_loss: 1063.91687\n",
      "Current_batch_loss: 1077.20691\n",
      "Current_batch_loss: 1074.6853\n",
      "Current_batch_loss: 989.42993\n",
      "Current_batch_loss: 1044.04626\n",
      "Current_batch_loss: 1094.26917\n",
      "Current_batch_loss: 1063.41589\n",
      "Current_batch_loss: 1018.3913\n",
      "Current_batch_loss: 1051.63086\n",
      "Current_batch_loss: 1020.46454\n",
      "Current_batch_loss: 1010.55573\n",
      "Current_batch_loss: 959.34668\n",
      "Current_batch_loss: 1004.01965\n",
      "Current_batch_loss: 1013.87982\n",
      "Current_batch_loss: 1034.73682\n",
      "Current_batch_loss: 973.00208\n",
      "Current_batch_loss: 1016.28503\n",
      "Current_batch_loss: 1072.84814\n",
      "Current_batch_loss: 1026.3949\n",
      "Current_batch_loss: 1028.86426\n",
      "Current_batch_loss: 994.79236\n",
      "Current_batch_loss: 980.33911\n",
      "Current_batch_loss: 1008.28241\n",
      "Current_batch_loss: 971.95032\n",
      "Current_batch_loss: 1082.54419\n",
      "Current_batch_loss: 1004.99463\n",
      "Current_batch_loss: 983.08875\n",
      "Current_batch_loss: 979.48993\n",
      "Current_batch_loss: 939.07721\n",
      "Current_batch_loss: 973.67987\n",
      "Current_batch_loss: 1087.79041\n",
      "Current_batch_loss: 1016.8454\n",
      "Current_batch_loss: 1018.43268\n",
      "Current_batch_loss: 994.22522\n",
      "Current_batch_loss: 959.38892\n",
      "Current_batch_loss: 946.3847\n",
      "Current_batch_loss: 1036.12756\n",
      "Current_batch_loss: 950.21417\n",
      "Current_batch_loss: 966.42822\n",
      "Current_batch_loss: 1045.92285\n",
      "Current_batch_loss: 1014.65387\n",
      "Current_batch_loss: 1027.48743\n",
      "Current_batch_loss: 948.35736\n",
      "Current_batch_loss: 970.76288\n",
      "Current_batch_loss: 974.22552\n",
      "Current_batch_loss: 935.80084\n",
      "Current_batch_loss: 952.2439\n",
      "Current_batch_loss: 944.85461\n",
      "Current_batch_loss: 989.54932\n",
      "Current_batch_loss: 956.00903\n",
      "Current_batch_loss: 940.9729\n",
      "Current_batch_loss: 1014.37372\n",
      "Current_batch_loss: 965.02765\n",
      "Current_batch_loss: 920.86877\n",
      "Current_batch_loss: 966.50006\n",
      "Current_batch_loss: 921.34753\n",
      "Current_batch_loss: 963.68317\n",
      "Current_batch_loss: 902.27344\n",
      "Current_batch_loss: 895.08972\n",
      "Current_batch_loss: 910.6524\n",
      "Current_batch_loss: 903.64703\n",
      "Current_batch_loss: 967.00623\n",
      "Current_batch_loss: 834.35248\n",
      "Current_batch_loss: 992.56396\n",
      "Current_batch_loss: 965.43799\n",
      "Current_batch_loss: 939.71533\n",
      "Current_batch_loss: 1007.409\n",
      "Current_batch_loss: 954.55865\n",
      "Current_batch_loss: 893.7085\n",
      "Current_batch_loss: 961.55072\n",
      "Current_batch_loss: 913.41699\n",
      "Current_batch_loss: 852.31335\n",
      "Current_batch_loss: 907.03564\n",
      "Current_batch_loss: 839.3053\n",
      "Current_batch_loss: 997.10132\n",
      "Current_batch_loss: 802.30322\n",
      "Current_batch_loss: 1002.60052\n",
      "Current_batch_loss: 965.64417\n",
      "Current_batch_loss: 901.78137\n",
      "Current_batch_loss: 951.31372\n",
      "Current_batch_loss: 923.07043\n",
      "Current_batch_loss: 895.32935\n",
      "Current_batch_loss: 915.1618\n",
      "Current_batch_loss: 874.25977\n",
      "Current_batch_loss: 940.43024\n",
      "Current_batch_loss: 920.45776\n",
      "Current_batch_loss: 886.05042\n",
      "Current_batch_loss: 908.44104\n",
      "Current_batch_loss: 942.64294\n",
      "Current_batch_loss: 852.58484\n",
      "Current_batch_loss: 870.8454\n",
      "Current_batch_loss: 818.71466\n",
      "Current_batch_loss: 954.69635\n",
      "Current_batch_loss: 923.62097\n",
      "Current_batch_loss: 998.32837\n",
      "Current_batch_loss: 930.20068\n",
      "Current_batch_loss: 942.19141\n",
      "Current_batch_loss: 833.6076\n",
      "Current_batch_loss: 959.53107\n",
      "Current_batch_loss: 804.935\n",
      "Current_batch_loss: 795.40521\n",
      "Current_batch_loss: 861.97107\n",
      "Current_batch_loss: 830.95844\n",
      "Current_batch_loss: 959.90247\n",
      "Current_batch_loss: 852.21466\n",
      "Current_batch_loss: 755.51691\n",
      "Current_batch_loss: 872.10535\n",
      "Current_batch_loss: 847.54474\n",
      "Current_batch_loss: 963.45282\n",
      "Current_batch_loss: 789.55878\n",
      "Current_batch_loss: 779.31903\n",
      "Current_batch_loss: 938.27478\n",
      "Current_batch_loss: 835.32379\n",
      "Current_batch_loss: 863.49304\n",
      "Current_batch_loss: 937.40106\n",
      "Current_batch_loss: 792.74463\n",
      "Current_batch_loss: 924.64984\n",
      "Current_batch_loss: 966.29065\n",
      "Current_batch_loss: 873.24182\n",
      "Current_batch_loss: 876.20667\n",
      "Current_batch_loss: 796.82416\n",
      "Current_batch_loss: 872.18488\n",
      "Current_batch_loss: 821.19116\n",
      "Current_batch_loss: 900.69971\n",
      "Current_batch_loss: 863.85522\n",
      "Current_batch_loss: 803.78082\n",
      "Current_batch_loss: 915.20575\n",
      "Current_batch_loss: 862.19525\n",
      "Current_batch_loss: 960.99115\n",
      "Current_batch_loss: 902.46985\n",
      "Current_batch_loss: 574.03503\n",
      "Current_batch_loss: 847.54962\n",
      "Current_batch_loss: 854.45129\n",
      "Current_batch_loss: 845.79968\n",
      "Current_batch_loss: 904.11346\n",
      "Current_batch_loss: 916.97455\n",
      "Current_batch_loss: 822.87317\n",
      "Current_batch_loss: 908.40009\n",
      "Current_batch_loss: 856.55511\n",
      "Current_batch_loss: 852.26727\n",
      "Current_batch_loss: 737.28955\n",
      "Current_batch_loss: 753.66412\n",
      "Current_batch_loss: 882.7533\n",
      "Current_batch_loss: 923.1734\n",
      "Current_batch_loss: 733.74396\n",
      "Current_batch_loss: 842.21185\n",
      "Current_batch_loss: 910.94098\n",
      "Current_batch_loss: 935.79504\n",
      "Current_batch_loss: 891.09796\n",
      "Current_batch_loss: 833.94879\n",
      "Current_batch_loss: 883.50757\n",
      "Current_batch_loss: 838.72345\n",
      "Current_batch_loss: 821.73505\n",
      "Current_batch_loss: 730.58551\n",
      "Current_batch_loss: 744.79675\n",
      "Current_batch_loss: 599.43207\n",
      "Current_batch_loss: 930.64581\n",
      "Current_batch_loss: 842.53082\n",
      "Current_batch_loss: 710.14398\n",
      "Current_batch_loss: 822.11133\n",
      "Current_batch_loss: 839.0625\n",
      "Current_batch_loss: 892.72577\n",
      "Current_batch_loss: 848.51453\n",
      "Current_batch_loss: 928.04163\n",
      "Current_batch_loss: 824.81567\n",
      "Current_batch_loss: 850.43591\n",
      "Current_batch_loss: 850.97467\n",
      "Current_batch_loss: 841.75879\n",
      "Current_batch_loss: 839.07849\n",
      "Current_batch_loss: 800.57013\n",
      "Current_batch_loss: 914.30878\n",
      "Current_batch_loss: 805.97449\n",
      "Current_batch_loss: 812.76849\n",
      "Current_batch_loss: 835.26147\n",
      "Current_batch_loss: 773.34229\n",
      "Current_batch_loss: 737.02271\n",
      "Current_batch_loss: 723.88068\n",
      "Current_batch_loss: 785.03229\n",
      "Current_batch_loss: 836.315\n",
      "Current_batch_loss: 804.06232\n",
      "Current_batch_loss: 817.4267\n",
      "Current_batch_loss: 614.77557\n",
      "Current_batch_loss: 798.29779\n",
      "Current_batch_loss: 941.96814\n",
      "Current_batch_loss: 850.5108\n",
      "Current_batch_loss: 860.40039\n",
      "Current_batch_loss: 758.76385\n",
      "Current_batch_loss: 794.59436\n",
      "Current_batch_loss: 670.26801\n",
      "Current_batch_loss: 886.4704\n",
      "Current_batch_loss: 762.91058\n",
      "Current_batch_loss: 826.09167\n",
      "Current_batch_loss: 892.20032\n",
      "Current_batch_loss: 718.58832\n",
      "Current_batch_loss: 814.95453\n",
      "Current_batch_loss: 725.58124\n",
      "Current_batch_loss: 618.08789\n",
      "Current_batch_loss: 732.48877\n",
      "Current_batch_loss: 812.48798\n",
      "Current_batch_loss: 764.13538\n",
      "Current_batch_loss: 721.67139\n",
      "Current_batch_loss: 718.02997\n",
      "Current_batch_loss: 765.18036\n",
      "Current_batch_loss: 824.91211\n",
      "Current_batch_loss: 649.37195\n",
      "Current_batch_loss: 680.93475\n",
      "Current_batch_loss: 860.19586\n",
      "Current_batch_loss: 800.60443\n",
      "Current_batch_loss: 761.15125\n",
      "Current_batch_loss: 586.3006\n",
      "Current_batch_loss: 769.19281\n",
      "Current_batch_loss: 807.99023\n",
      "Current_batch_loss: 636.71631\n",
      "Current_batch_loss: 792.7926\n",
      "Current_batch_loss: 738.0177\n",
      "Current_batch_loss: 686.49518\n",
      "Current_batch_loss: 708.20502\n",
      "Current_batch_loss: 811.33801\n",
      "Current_batch_loss: 647.79474\n",
      "Current_batch_loss: 886.34552\n",
      "Current_batch_loss: 730.27649\n",
      "Current_batch_loss: 835.784\n",
      "Current_batch_loss: 786.53125\n",
      "Current_batch_loss: 666.37164\n",
      "Current_batch_loss: 711.47601\n",
      "Current_batch_loss: 701.43671\n",
      "Current_batch_loss: 901.04248\n",
      "Current_batch_loss: 826.43903\n",
      "Current_batch_loss: 747.70728\n",
      "Current_batch_loss: 709.94769\n",
      "Current_batch_loss: 688.9812\n",
      "Current_batch_loss: 752.00793\n",
      "Current_batch_loss: 791.92377\n",
      "Current_batch_loss: 831.61389\n",
      "Current_batch_loss: 842.23785\n",
      "Current_batch_loss: 599.87048\n",
      "Current_batch_loss: 762.34625\n",
      "Current_batch_loss: 810.07135\n",
      "Current_batch_loss: 803.82391\n",
      "Current_batch_loss: 771.8208\n",
      "Current_batch_loss: 606.88159\n",
      "Current_batch_loss: 763.53436\n",
      "Current_batch_loss: 817.07819\n",
      "Current_batch_loss: 757.02563\n",
      "Current_batch_loss: 761.77905\n",
      "Current_batch_loss: 674.51215\n",
      "Current_batch_loss: 807.48914\n",
      "Current_batch_loss: 692.74792\n",
      "Current_batch_loss: 771.1084\n",
      "Current_batch_loss: 668.51245\n",
      "Current_batch_loss: 748.81183\n",
      "Current_batch_loss: 642.49432\n",
      "Current_batch_loss: 760.69562\n",
      "Current_batch_loss: 696.74091\n",
      "Current_batch_loss: 562.46918\n",
      "Current_batch_loss: 791.68671\n",
      "Current_batch_loss: 544.09875\n",
      "Current_batch_loss: 622.28784\n",
      "Current_batch_loss: 709.08636\n",
      "Current_batch_loss: 931.1123\n",
      "Current_batch_loss: 776.93793\n",
      "Current_batch_loss: 656.08521\n",
      "Current_batch_loss: 795.87653\n",
      "Current_batch_loss: 759.90033\n",
      "Current_batch_loss: 800.54846\n",
      "Current_batch_loss: 863.25793\n",
      "Current_batch_loss: 670.565\n",
      "Current_batch_loss: 751.94879\n",
      "Current_batch_loss: 782.73334\n",
      "Current_batch_loss: 711.15344\n",
      "Current_batch_loss: 686.19385\n",
      "Current_batch_loss: 969.50427\n",
      "Current_batch_loss: 715.27698\n",
      "Current_batch_loss: 768.1911\n",
      "Current_batch_loss: 647.39703\n",
      "Current_batch_loss: 766.23669\n",
      "Current_batch_loss: 622.44965\n",
      "Current_batch_loss: 752.51813\n",
      "Current_batch_loss: 668.51929\n",
      "Current_batch_loss: 717.05029\n",
      "Current_batch_loss: 704.74609\n",
      "Current_batch_loss: 717.32422\n",
      "Current_batch_loss: 734.6178\n",
      "Current_batch_loss: 726.43341\n",
      "Current_batch_loss: 766.86047\n",
      "Current_batch_loss: 759.27393\n",
      "Current_batch_loss: 724.93005\n",
      "Current_batch_loss: 717.62781\n",
      "Current_batch_loss: 725.30316\n",
      "Current_batch_loss: 822.06152\n",
      "Current_batch_loss: 810.84509\n",
      "Current_batch_loss: 754.80298\n",
      "Current_batch_loss: 647.77478\n",
      "Current_batch_loss: 675.16681\n",
      "Current_batch_loss: 731.25574\n",
      "Current_batch_loss: 585.05554\n",
      "Current_batch_loss: 768.82336\n",
      "Current_batch_loss: 590.51172\n",
      "Current_batch_loss: 698.66028\n",
      "Current_batch_loss: 739.94189\n",
      "Current_batch_loss: 658.42621\n",
      "Current_batch_loss: 755.45154\n",
      "Current_batch_loss: 812.68121\n",
      "Current_batch_loss: 648.0791\n",
      "Current_batch_loss: 808.69116\n",
      "Current_batch_loss: 528.98663\n",
      "Current_batch_loss: 689.39423\n",
      "Current_batch_loss: 679.07635\n",
      "Current_batch_loss: 648.2514\n",
      "Current_batch_loss: 585.71021\n",
      "Current_batch_loss: 586.23126\n",
      "Current_batch_loss: 683.82587\n",
      "Current_batch_loss: 698.71521\n",
      "Current_batch_loss: 710.75793\n",
      "Current_batch_loss: 576.30652\n",
      "Current_batch_loss: 803.22858\n",
      "Current_batch_loss: 579.88324\n",
      "Current_batch_loss: 654.61731\n",
      "Current_batch_loss: 596.52026\n",
      "Current_batch_loss: 683.46918\n",
      "Current_batch_loss: 664.2171\n",
      "Current_batch_loss: 827.86932\n",
      "Current_batch_loss: 772.01196\n",
      "Current_batch_loss: 700.80786\n",
      "Current_batch_loss: 608.71478\n",
      "Current_batch_loss: 772.04419\n",
      "Current_batch_loss: 682.07452\n",
      "Current_batch_loss: 708.08167\n",
      "Current_batch_loss: 523.2002\n",
      "Current_batch_loss: 662.43994\n",
      "Current_batch_loss: 617.16113\n",
      "Current_batch_loss: 699.09869\n",
      "Current_batch_loss: 703.4762\n",
      "Current_batch_loss: 644.90033\n",
      "Current_batch_loss: 680.79022\n",
      "Current_batch_loss: 639.74841\n",
      "Current_batch_loss: 602.41193\n",
      "Current_batch_loss: 550.70453\n",
      "Current_batch_loss: 709.99683\n",
      "Current_batch_loss: 601.63428\n",
      "Current_batch_loss: 557.21582\n",
      "Current_batch_loss: 623.01581\n",
      "Current_batch_loss: 735.96558\n",
      "Current_batch_loss: 721.97498\n",
      "Current_batch_loss: 595.85901\n",
      "Current_batch_loss: 582.93591\n",
      "Current_batch_loss: 632.73718\n",
      "Current_batch_loss: 737.81915\n",
      "Current_batch_loss: 717.26923\n",
      "Current_batch_loss: 652.69257\n",
      "Current_batch_loss: 593.98547\n",
      "Current_batch_loss: 652.71393\n",
      "Current_batch_loss: 588.91559\n",
      "Current_batch_loss: 671.78333\n",
      "Current_batch_loss: 658.39417\n",
      "Current_batch_loss: 615.64417\n",
      "Current_batch_loss: 587.15601\n",
      "Current_batch_loss: 684.38623\n",
      "Current_batch_loss: 640.18567\n",
      "Current_batch_loss: 605.51117\n",
      "Current_batch_loss: 724.742\n",
      "Current_batch_loss: 622.09473\n",
      "Current_batch_loss: 641.45074\n",
      "Current_batch_loss: 590.91406\n",
      "Current_batch_loss: 703.15906\n",
      "Current_batch_loss: 697.1756\n",
      "Current_batch_loss: 581.94952\n",
      "Current_batch_loss: 539.7179\n",
      "Current_batch_loss: 699.33167\n",
      "Current_batch_loss: 610.58899\n",
      "Current_batch_loss: 678.86377\n",
      "Current_batch_loss: 633.70526\n",
      "Current_batch_loss: 688.92731\n",
      "Current_batch_loss: 737.91205\n",
      "Current_batch_loss: 605.41876\n",
      "Current_batch_loss: 642.55432\n",
      "Current_batch_loss: 482.35419\n",
      "Current_batch_loss: 692.07123\n",
      "Current_batch_loss: 628.07422\n",
      "Current_batch_loss: 569.1413\n",
      "Current_batch_loss: 656.38043\n",
      "Current_batch_loss: 538.55981\n",
      "Current_batch_loss: 570.3941\n",
      "Current_batch_loss: 567.31067\n",
      "Current_batch_loss: 579.90527\n",
      "Current_batch_loss: 536.41815\n",
      "Current_batch_loss: 608.83484\n",
      "Current_batch_loss: 685.81934\n",
      "Current_batch_loss: 720.20923\n",
      "Current_batch_loss: 709.52124\n",
      "Current_batch_loss: 548.31519\n",
      "Current_batch_loss: 532.79016\n",
      "Current_batch_loss: 576.16516\n",
      "Current_batch_loss: 678.35522\n",
      "Current_batch_loss: 585.2674\n",
      "Current_batch_loss: 596.04321\n",
      "Current_batch_loss: 566.21484\n",
      "Current_batch_loss: 740.01898\n",
      "Current_batch_loss: 497.33582\n",
      "Current_batch_loss: 587.54852\n",
      "Current_batch_loss: 750.25562\n",
      "Current_batch_loss: 632.58905\n",
      "Current_batch_loss: 657.76886\n",
      "Current_batch_loss: 742.3479\n",
      "Current_batch_loss: 587.45953\n",
      "Current_batch_loss: 665.31189\n",
      "Current_batch_loss: 475.71658\n",
      "Current_batch_loss: 542.76331\n",
      "Current_batch_loss: 461.01212\n",
      "Current_batch_loss: 597.73242\n",
      "Current_batch_loss: 740.66522\n",
      "Current_batch_loss: 724.10199\n",
      "Current_batch_loss: 602.02545\n",
      "Current_batch_loss: 577.78308\n",
      "Current_batch_loss: 706.76544\n",
      "Current_batch_loss: 554.5769\n",
      "Current_batch_loss: 732.93457\n",
      "Current_batch_loss: 708.90747\n",
      "Current_batch_loss: 637.83789\n",
      "Current_batch_loss: 640.12543\n",
      "Current_batch_loss: 525.12384\n",
      "Current_batch_loss: 562.7149\n",
      "Current_batch_loss: 687.26251\n",
      "Current_batch_loss: 639.97028\n",
      "Current_batch_loss: 665.00665\n",
      "Current_batch_loss: 648.22729\n",
      "Current_batch_loss: 527.96216\n",
      "Current_batch_loss: 796.60559\n",
      "Current_batch_loss: 512.46295\n",
      "Current_batch_loss: 580.35669\n",
      "Current_batch_loss: 577.31964\n",
      "Current_batch_loss: 593.57159\n",
      "Current_batch_loss: 659.26379\n",
      "Current_batch_loss: 679.74652\n",
      "Current_batch_loss: 611.56366\n",
      "Current_batch_loss: 692.13208\n",
      "Current_batch_loss: 589.35327\n",
      "Current_batch_loss: 580.69873\n",
      "Current_batch_loss: 603.28448\n",
      "Current_batch_loss: 478.70435\n",
      "Current_batch_loss: 572.66217\n",
      "Current_batch_loss: 698.73865\n",
      "Current_batch_loss: 476.42307\n",
      "Current_batch_loss: 607.42175\n",
      "Current_batch_loss: 427.75455\n",
      "Current_batch_loss: 735.18494\n",
      "Current_batch_loss: 574.74646\n",
      "Current_batch_loss: 602.45032\n",
      "Current_batch_loss: 586.54285\n",
      "Current_batch_loss: 532.85663\n",
      "Current_batch_loss: 563.78839\n",
      "Current_batch_loss: 737.48108\n",
      "Current_batch_loss: 541.68323\n",
      "Current_batch_loss: 425.39145\n",
      "Current_batch_loss: 516.07422\n",
      "Current_batch_loss: 607.03857\n",
      "Current_batch_loss: 550.56726\n",
      "Current_batch_loss: 566.44458\n",
      "Current_batch_loss: 527.81549\n",
      "Current_batch_loss: 595.87921\n",
      "Current_batch_loss: 582.1554\n",
      "Current_batch_loss: 486.25467\n",
      "Current_batch_loss: 529.92908\n",
      "Current_batch_loss: 533.98315\n",
      "Current_batch_loss: 406.77887\n",
      "Current_batch_loss: 609.24335\n",
      "Current_batch_loss: 542.40454\n",
      "Current_batch_loss: 573.60455\n",
      "Current_batch_loss: 619.76996\n",
      "Current_batch_loss: 445.94446\n",
      "Current_batch_loss: 598.00757\n",
      "Current_batch_loss: 606.36554\n",
      "Current_batch_loss: 526.96332\n",
      "Current_batch_loss: 605.38995\n",
      "Current_batch_loss: 509.39075\n",
      "Current_batch_loss: 759.95056\n",
      "Current_batch_loss: 591.85602\n",
      "Current_batch_loss: 600.77692\n",
      "Current_batch_loss: 573.47607\n",
      "Current_batch_loss: 593.88464\n",
      "Current_batch_loss: 618.45282\n",
      "Current_batch_loss: 550.43427\n",
      "Current_batch_loss: 572.36566\n",
      "Current_batch_loss: 469.24365\n",
      "Current_batch_loss: 632.44653\n",
      "Current_batch_loss: 465.31125\n",
      "Current_batch_loss: 542.953\n",
      "Current_batch_loss: 551.23004\n",
      "Current_batch_loss: 491.75131\n",
      "Current_batch_loss: 583.64258\n",
      "Current_batch_loss: 539.69366\n",
      "Current_batch_loss: 580.05029\n",
      "Current_batch_loss: 372.45428\n",
      "Current_batch_loss: 475.91714\n",
      "Current_batch_loss: 432.99844\n",
      "Current_batch_loss: 515.08911\n",
      "Current_batch_loss: 492.74963\n",
      "Current_batch_loss: 582.19739\n",
      "Current_batch_loss: 508.58496\n",
      "Current_batch_loss: 486.44601\n",
      "Current_batch_loss: 539.00238\n",
      "Current_batch_loss: 620.77734\n",
      "Current_batch_loss: 332.08447\n",
      "Current_batch_loss: 413.63\n",
      "Current_batch_loss: 596.62659\n",
      "Current_batch_loss: 465.845\n",
      "Current_batch_loss: 477.31464\n",
      "Current_batch_loss: 450.55081\n",
      "Current_batch_loss: 559.10516\n",
      "Current_batch_loss: 622.70441\n",
      "Current_batch_loss: 492.15735\n",
      "Current_batch_loss: 502.95621\n",
      "Current_batch_loss: 523.41437\n",
      "Current_batch_loss: 442.99332\n",
      "Current_batch_loss: 609.87238\n",
      "Current_batch_loss: 531.04639\n",
      "Current_batch_loss: 395.62912\n",
      "Current_batch_loss: 449.24344\n",
      "Current_batch_loss: 538.83209\n",
      "Current_batch_loss: 565.33478\n",
      "Current_batch_loss: 417.8212\n",
      "Current_batch_loss: 349.01004\n",
      "Current_batch_loss: 406.02441\n",
      "Current_batch_loss: 430.65466\n",
      "Current_batch_loss: 662.7041\n",
      "Current_batch_loss: 507.54352\n",
      "Current_batch_loss: 528.82391\n",
      "Current_batch_loss: 357.22424\n",
      "Current_batch_loss: 579.70111\n",
      "Current_batch_loss: 603.47614\n",
      "Current_batch_loss: 407.27179\n",
      "Current_batch_loss: 488.18164\n",
      "Current_batch_loss: 474.25336\n",
      "Current_batch_loss: 439.20248\n",
      "Current_batch_loss: 560.22205\n",
      "Current_batch_loss: 466.17505\n",
      "Current_batch_loss: 458.38474\n",
      "Current_batch_loss: 432.91446\n",
      "Current_batch_loss: 467.08948\n",
      "Current_batch_loss: 437.03836\n",
      "Current_batch_loss: 447.57312\n",
      "Current_batch_loss: 483.57837\n",
      "Current_batch_loss: 491.67993\n",
      "Current_batch_loss: 543.03625\n",
      "Current_batch_loss: 491.50778\n",
      "Current_batch_loss: 549.21136\n",
      "Current_batch_loss: 368.9357\n",
      "Current_batch_loss: 540.67786\n",
      "Current_batch_loss: 444.32153\n",
      "Current_batch_loss: 496.97931\n",
      "Current_batch_loss: 458.65012\n",
      "Current_batch_loss: 387.97336\n",
      "Current_batch_loss: 529.60498\n",
      "Current_batch_loss: 438.37854\n",
      "Current_batch_loss: 367.45078\n",
      "Current_batch_loss: 522.34741\n",
      "Current_batch_loss: 527.90906\n",
      "Current_batch_loss: 361.81647\n",
      "Current_batch_loss: 434.11853\n",
      "Current_batch_loss: 455.61447\n",
      "Current_batch_loss: 628.44141\n",
      "Current_batch_loss: 499.99637\n",
      "Current_batch_loss: 539.37067\n",
      "Current_batch_loss: 591.36725\n",
      "Current_batch_loss: 611.48669\n",
      "Current_batch_loss: 552.94049\n",
      "Current_batch_loss: 554.91528\n",
      "Current_batch_loss: 387.97571\n",
      "Current_batch_loss: 448.58401\n",
      "Current_batch_loss: 443.61636\n",
      "Current_batch_loss: 340.32529\n",
      "Current_batch_loss: 417.85455\n",
      "Current_batch_loss: 486.85733\n",
      "Current_batch_loss: 495.40842\n",
      "Current_batch_loss: 464.17258\n",
      "Current_batch_loss: 481.29074\n",
      "Current_batch_loss: 411.41174\n",
      "Current_batch_loss: 401.10373\n",
      "Current_batch_loss: 512.94928\n",
      "Current_batch_loss: 405.57504\n",
      "Current_batch_loss: 498.32819\n",
      "Current_batch_loss: 442.83429\n",
      "Current_batch_loss: 512.47375\n",
      "Current_batch_loss: 560.88654\n",
      "Current_batch_loss: 375.81845\n",
      "Current_batch_loss: 427.34222\n",
      "Current_batch_loss: 520.43805\n",
      "Current_batch_loss: 386.62473\n",
      "Current_batch_loss: 317.33838\n",
      "Current_batch_loss: 396.29776\n",
      "Current_batch_loss: 470.38058\n",
      "Current_batch_loss: 494.42859\n",
      "Current_batch_loss: 503.06857\n",
      "Current_batch_loss: 319.35248\n",
      "Current_batch_loss: 467.53391\n",
      "Current_batch_loss: 617.23389\n",
      "Current_batch_loss: 330.4436\n",
      "Current_batch_loss: 568.23962\n",
      "Current_batch_loss: 486.42801\n",
      "Current_batch_loss: 440.97412\n",
      "Current_batch_loss: 510.26932\n",
      "Current_batch_loss: 462.93744\n",
      "Current_batch_loss: 358.55545\n",
      "Current_batch_loss: 475.01108\n",
      "Current_batch_loss: 430.37909\n",
      "Current_batch_loss: 437.37872\n",
      "Current_batch_loss: 494.79501\n",
      "Current_batch_loss: 356.48715\n",
      "Current_batch_loss: 356.44989\n",
      "Current_batch_loss: 460.05908\n",
      "Current_batch_loss: 473.14743\n",
      "Current_batch_loss: 449.18042\n",
      "Current_batch_loss: 411.70343\n",
      "Current_batch_loss: 428.97119\n",
      "Current_batch_loss: 422.75613\n",
      "Current_batch_loss: 459.90692\n",
      "Current_batch_loss: 592.50342\n",
      "Current_batch_loss: 400.05136\n",
      "Current_batch_loss: 363.46423\n",
      "Current_batch_loss: 493.70432\n",
      "Current_batch_loss: 340.16388\n",
      "Current_batch_loss: 495.08511\n",
      "Current_batch_loss: 296.53461\n",
      "Current_batch_loss: 445.08633\n",
      "Current_batch_loss: 488.39066\n",
      "Current_batch_loss: 483.34973\n",
      "Current_batch_loss: 328.31726\n",
      "Current_batch_loss: 332.3132\n",
      "Current_batch_loss: 294.39087\n",
      "Current_batch_loss: 409.59662\n",
      "Current_batch_loss: 510.58539\n",
      "Current_batch_loss: 431.66339\n",
      "Current_batch_loss: 387.71719\n",
      "Current_batch_loss: 536.94116\n",
      "Current_batch_loss: 445.83643\n",
      "Current_batch_loss: 349.84579\n",
      "Current_batch_loss: 440.24075\n",
      "Current_batch_loss: 391.77985\n",
      "Current_batch_loss: 435.26663\n",
      "Current_batch_loss: 340.77319\n",
      "Current_batch_loss: 359.78442\n",
      "Current_batch_loss: 491.8544\n",
      "Current_batch_loss: 315.68811\n",
      "Current_batch_loss: 538.29095\n",
      "Current_batch_loss: 465.38794\n",
      "Current_batch_loss: 340.50598\n",
      "Current_batch_loss: 525.4458\n",
      "Current_batch_loss: 427.52231\n",
      "Current_batch_loss: 298.70178\n",
      "Current_batch_loss: 494.00232\n",
      "Current_batch_loss: 511.10196\n",
      "Current_batch_loss: 440.94839\n",
      "Current_batch_loss: 237.07217\n",
      "Current_batch_loss: 448.16946\n",
      "Current_batch_loss: 325.61783\n",
      "Current_batch_loss: 384.91458\n",
      "Current_batch_loss: 357.96078\n",
      "Current_batch_loss: 376.10144\n",
      "Current_batch_loss: 371.42505\n",
      "Current_batch_loss: 402.43436\n",
      "Current_batch_loss: 373.90735\n",
      "Current_batch_loss: 388.05267\n",
      "Current_batch_loss: 255.39737\n",
      "Current_batch_loss: 470.44733\n",
      "Current_batch_loss: 168.48608\n",
      "Current_batch_loss: 314.91922\n",
      "Current_batch_loss: 548.16217\n",
      "Current_batch_loss: 394.68283\n",
      "Current_batch_loss: 368.53357\n",
      "Current_batch_loss: 314.03757\n",
      "Current_batch_loss: 295.63422\n",
      "Current_batch_loss: 193.92926\n",
      "Current_batch_loss: 444.44736\n",
      "Current_batch_loss: 420.00952\n",
      "Current_batch_loss: 429.7785\n",
      "Current_batch_loss: 490.89493\n",
      "Current_batch_loss: 308.55276\n",
      "Current_batch_loss: 309.14014\n",
      "Current_batch_loss: 342.95844\n",
      "Current_batch_loss: 363.35486\n",
      "Current_batch_loss: 256.66315\n",
      "Current_batch_loss: 413.60956\n",
      "Current_batch_loss: 374.92245\n",
      "Current_batch_loss: 394.07953\n",
      "Current_batch_loss: 337.18063\n",
      "Current_batch_loss: 538.3407\n",
      "Current_batch_loss: 382.38931\n",
      "Current_batch_loss: 348.06601\n",
      "Current_batch_loss: 470.49756\n",
      "Current_batch_loss: 429.01093\n",
      "Current_batch_loss: 498.65375\n",
      "Current_batch_loss: 327.02899\n",
      "Current_batch_loss: 429.7348\n",
      "Current_batch_loss: 451.56631\n",
      "Current_batch_loss: 288.37067\n",
      "Current_batch_loss: 408.92532\n",
      "Current_batch_loss: 365.0065\n",
      "Current_batch_loss: 347.55161\n",
      "Current_batch_loss: 350.41199\n",
      "Current_batch_loss: 440.08102\n",
      "Current_batch_loss: 442.332\n",
      "Current_batch_loss: 377.79269\n",
      "Current_batch_loss: 315.1102\n",
      "Current_batch_loss: 325.13272\n",
      "Current_batch_loss: 339.44949\n",
      "Current_batch_loss: 318.87323\n",
      "Current_batch_loss: 289.19818\n",
      "Current_batch_loss: 304.58148\n",
      "Current_batch_loss: 417.10498\n",
      "Current_batch_loss: 258.7785\n",
      "Current_batch_loss: 365.36304\n",
      "Current_batch_loss: 382.3078\n",
      "Current_batch_loss: 215.09421\n",
      "Current_batch_loss: 323.5502\n",
      "Current_batch_loss: 395.06995\n",
      "Current_batch_loss: 344.51111\n",
      "Current_batch_loss: 289.17853\n",
      "Current_batch_loss: 386.08691\n",
      "Current_batch_loss: 245.79419\n",
      "Current_batch_loss: 357.09546\n",
      "Current_batch_loss: 352.17023\n",
      "Current_batch_loss: 319.37964\n",
      "Current_batch_loss: 405.84799\n",
      "Current_batch_loss: 277.41177\n",
      "Current_batch_loss: 324.22751\n",
      "Current_batch_loss: 347.54959\n",
      "Current_batch_loss: 329.53339\n",
      "Current_batch_loss: 331.04919\n",
      "Current_batch_loss: 254.37076\n",
      "Current_batch_loss: 344.16815\n",
      "Current_batch_loss: 256.55212\n",
      "Current_batch_loss: 334.23285\n",
      "Current_batch_loss: 357.63354\n",
      "Current_batch_loss: 355.0722\n",
      "Current_batch_loss: 293.40958\n",
      "Current_batch_loss: 445.6665\n",
      "Current_batch_loss: 475.64099\n",
      "Current_batch_loss: 348.6062\n",
      "Current_batch_loss: 437.61823\n",
      "Current_batch_loss: 393.81424\n",
      "Current_batch_loss: 211.47758\n",
      "Current_batch_loss: 299.04379\n",
      "Current_batch_loss: 433.0372\n",
      "Current_batch_loss: 341.63364\n",
      "Current_batch_loss: 410.51413\n",
      "Current_batch_loss: 309.49249\n",
      "Current_batch_loss: 403.53052\n",
      "Current_batch_loss: 297.03528\n",
      "Current_batch_loss: 314.25403\n",
      "Current_batch_loss: 464.85162\n",
      "Current_batch_loss: 303.63016\n",
      "Current_batch_loss: 259.68381\n",
      "Current_batch_loss: 361.71991\n",
      "Current_batch_loss: 283.84799\n",
      "Current_batch_loss: 274.93979\n",
      "Current_batch_loss: 275.0961\n",
      "Current_batch_loss: 299.41125\n",
      "Current_batch_loss: 472.52469\n",
      "Current_batch_loss: 296.22\n",
      "Current_batch_loss: 498.9678\n",
      "Current_batch_loss: 358.92947\n",
      "Current_batch_loss: 304.3923\n",
      "Current_batch_loss: 300.3876\n",
      "Current_batch_loss: 259.17444\n",
      "Current_batch_loss: 199.50192\n",
      "Current_batch_loss: 306.98361\n",
      "Current_batch_loss: 263.01511\n",
      "Current_batch_loss: 341.3822\n",
      "Current_batch_loss: 277.49936\n",
      "Current_batch_loss: 258.23581\n",
      "Current_batch_loss: 340.72955\n",
      "Current_batch_loss: 370.60519\n",
      "Current_batch_loss: 291.54251\n",
      "Current_batch_loss: 328.81027\n",
      "Current_batch_loss: 347.54703\n",
      "Current_batch_loss: 259.63156\n",
      "Current_batch_loss: 391.74463\n",
      "Current_batch_loss: 374.43607\n",
      "Current_batch_loss: 328.42349\n",
      "Current_batch_loss: 402.27487\n",
      "Current_batch_loss: 290.42398\n",
      "Current_batch_loss: 275.88226\n",
      "Current_batch_loss: 223.49661\n",
      "Current_batch_loss: 260.69345\n",
      "Current_batch_loss: 335.87411\n",
      "Current_batch_loss: 369.29831\n",
      "Current_batch_loss: 196.41318\n",
      "Current_batch_loss: 346.85483\n",
      "Current_batch_loss: 362.51489\n",
      "Current_batch_loss: 311.88489\n",
      "Current_batch_loss: 437.39651\n",
      "Current_batch_loss: 302.0155\n",
      "Current_batch_loss: 327.27328\n",
      "Current_batch_loss: 231.47757\n",
      "Current_batch_loss: 257.71695\n",
      "Current_batch_loss: 164.34155\n",
      "Current_batch_loss: 270.9653\n",
      "Current_batch_loss: 216.21863\n",
      "Current_batch_loss: 358.76315\n",
      "Current_batch_loss: 351.40805\n",
      "Current_batch_loss: 243.59714\n",
      "Current_batch_loss: 314.08972\n",
      "Current_batch_loss: 334.78671\n",
      "Current_batch_loss: 379.6156\n",
      "Current_batch_loss: 283.30753\n",
      "Current_batch_loss: 261.84131\n",
      "Current_batch_loss: 393.84045\n",
      "Current_batch_loss: 212.35367\n",
      "Current_batch_loss: 272.61951\n",
      "Current_batch_loss: 362.99741\n",
      "Current_batch_loss: 385.73587\n",
      "Current_batch_loss: 260.10489\n",
      "Current_batch_loss: 313.21783\n",
      "Current_batch_loss: 317.95108\n",
      "Current_batch_loss: 378.75296\n",
      "Current_batch_loss: 362.95609\n",
      "Current_batch_loss: 384.13095\n",
      "Current_batch_loss: 305.2312\n",
      "Current_batch_loss: 276.74991\n",
      "Current_batch_loss: 271.19254\n",
      "Current_batch_loss: 294.72675\n",
      "Current_batch_loss: 256.04227\n",
      "Current_batch_loss: 300.04147\n",
      "Current_batch_loss: 253.58281\n",
      "Current_batch_loss: 284.21973\n",
      "Current_batch_loss: 361.63937\n",
      "Current_batch_loss: 335.2706\n",
      "Current_batch_loss: 354.05948\n",
      "Current_batch_loss: 265.08057\n",
      "Current_batch_loss: 275.81943\n",
      "Current_batch_loss: 305.1478\n",
      "Current_batch_loss: 271.44537\n",
      "Current_batch_loss: 317.56796\n",
      "Current_batch_loss: 234.55954\n",
      "Current_batch_loss: 284.55875\n",
      "Current_batch_loss: 381.54703\n",
      "Current_batch_loss: 264.327\n",
      "Current_batch_loss: 223.76541\n",
      "Current_batch_loss: 464.60025\n",
      "Current_batch_loss: 330.22238\n",
      "Current_batch_loss: 249.31995\n",
      "Current_batch_loss: 283.24902\n",
      "Current_batch_loss: 251.07851\n",
      "Current_batch_loss: 205.2189\n",
      "Current_batch_loss: 319.40463\n",
      "Current_batch_loss: 289.33737\n",
      "Current_batch_loss: 185.99231\n",
      "Current_batch_loss: 309.20605\n",
      "Current_batch_loss: 258.76816\n",
      "Current_batch_loss: 284.89645\n",
      "Current_batch_loss: 277.75766\n",
      "Current_batch_loss: 244.72913\n",
      "Current_batch_loss: 258.88065\n",
      "Current_batch_loss: 246.69141\n",
      "Current_batch_loss: 348.91415\n",
      "Current_batch_loss: 275.75027\n",
      "Current_batch_loss: 337.76505\n",
      "Current_batch_loss: 262.63898\n",
      "Current_batch_loss: 264.67471\n",
      "Current_batch_loss: 104.84264\n",
      "Current_batch_loss: 366.26151\n",
      "Current_batch_loss: 300.97159\n",
      "Current_batch_loss: 174.64861\n",
      "Current_batch_loss: 233.5405\n",
      "Current_batch_loss: 260.59705\n",
      "Current_batch_loss: 361.91647\n",
      "Current_batch_loss: 317.36203\n",
      "Current_batch_loss: 174.26457\n",
      "Current_batch_loss: 200.22737\n",
      "Current_batch_loss: 216.81689\n",
      "Current_batch_loss: 429.2648\n",
      "Current_batch_loss: 247.78574\n",
      "Current_batch_loss: 207.63634\n",
      "Current_batch_loss: 216.42337\n",
      "Current_batch_loss: 155.92265\n",
      "Current_batch_loss: 239.56812\n",
      "Current_batch_loss: 256.72693\n",
      "Current_batch_loss: 304.54037\n",
      "Current_batch_loss: 311.16498\n",
      "Current_batch_loss: 251.2562\n",
      "Current_batch_loss: 237.15469\n",
      "Current_batch_loss: 246.44986\n",
      "Current_batch_loss: 248.75998\n",
      "Current_batch_loss: 270.79141\n",
      "Current_batch_loss: 305.9407\n",
      "Current_batch_loss: 219.93642\n",
      "Current_batch_loss: 263.55695\n",
      "Current_batch_loss: 227.42674\n",
      "Current_batch_loss: 191.14688\n",
      "Current_batch_loss: 308.14365\n",
      "Current_batch_loss: 389.23972\n",
      "Current_batch_loss: 235.96153\n",
      "Current_batch_loss: 207.30673\n",
      "Current_batch_loss: 235.80937\n",
      "Current_batch_loss: 232.74956\n",
      "Current_batch_loss: 256.52246\n",
      "Current_batch_loss: 237.605\n",
      "Current_batch_loss: 209.78632\n",
      "Current_batch_loss: 262.23956\n",
      "Current_batch_loss: 282.18582\n",
      "Current_batch_loss: 206.4731\n",
      "Current_batch_loss: 285.39569\n",
      "Current_batch_loss: 254.97458\n",
      "Current_batch_loss: 246.46999\n",
      "Current_batch_loss: 200.72403\n",
      "Current_batch_loss: 204.23299\n",
      "Current_batch_loss: 330.67648\n",
      "Current_batch_loss: 173.67598\n",
      "Current_batch_loss: 269.45969\n",
      "Current_batch_loss: 255.21405\n",
      "Current_batch_loss: 309.37146\n",
      "Current_batch_loss: 202.27414\n",
      "Current_batch_loss: 164.71893\n",
      "Current_batch_loss: 282.94583\n",
      "Current_batch_loss: 222.02139\n",
      "Current_batch_loss: 246.6183\n",
      "Current_batch_loss: 382.79446\n",
      "Current_batch_loss: 208.65689\n",
      "Current_batch_loss: 245.89578\n",
      "Current_batch_loss: 243.22415\n",
      "Current_batch_loss: 198.12469\n",
      "Current_batch_loss: 199.12085\n",
      "Current_batch_loss: 226.57568\n",
      "Current_batch_loss: 260.59799\n",
      "Current_batch_loss: 172.29257\n",
      "Current_batch_loss: 256.78586\n",
      "Current_batch_loss: 142.95743\n",
      "Current_batch_loss: 190.65427\n",
      "Current_batch_loss: 313.80255\n",
      "Current_batch_loss: 310.81201\n",
      "Current_batch_loss: 190.83052\n",
      "Current_batch_loss: 235.36143\n",
      "Current_batch_loss: 168.64752\n",
      "Current_batch_loss: 244.66811\n",
      "Current_batch_loss: 301.6954\n",
      "Current_batch_loss: 223.63011\n",
      "Current_batch_loss: 293.9982\n",
      "Current_batch_loss: 172.19539\n",
      "Current_batch_loss: 180.26956\n",
      "Current_batch_loss: 266.29977\n",
      "Current_batch_loss: 234.07445\n",
      "Current_batch_loss: 237.50667\n",
      "Current_batch_loss: 270.76321\n",
      "Current_batch_loss: 173.89934\n",
      "Current_batch_loss: 319.15921\n",
      "Current_batch_loss: 228.22183\n",
      "Current_batch_loss: 274.66794\n",
      "Current_batch_loss: 193.27554\n",
      "Current_batch_loss: 227.76372\n",
      "Current_batch_loss: 267.58649\n",
      "Current_batch_loss: 282.2536\n",
      "Current_batch_loss: 237.29941\n",
      "Current_batch_loss: 212.88576\n",
      "Current_batch_loss: 238.48933\n",
      "Current_batch_loss: 195.90236\n",
      "Current_batch_loss: 276.57428\n",
      "Current_batch_loss: 182.42964\n",
      "Current_batch_loss: 170.09358\n",
      "Current_batch_loss: 181.56703\n",
      "Current_batch_loss: 96.37173\n",
      "Current_batch_loss: 210.8539\n",
      "Current_batch_loss: 248.6084\n",
      "Current_batch_loss: 167.88588\n",
      "Current_batch_loss: 192.86006\n",
      "Current_batch_loss: 145.45486\n",
      "Current_batch_loss: 271.13858\n",
      "Current_batch_loss: 176.44792\n",
      "Current_batch_loss: 164.40327\n",
      "Current_batch_loss: 253.32434\n",
      "Current_batch_loss: 275.36987\n",
      "Current_batch_loss: 134.87964\n",
      "Current_batch_loss: 170.13805\n",
      "Current_batch_loss: 251.77991\n",
      "Current_batch_loss: 306.9639\n",
      "Current_batch_loss: 243.09363\n",
      "Current_batch_loss: 217.86702\n",
      "Current_batch_loss: 226.78589\n",
      "Current_batch_loss: 228.04951\n",
      "Current_batch_loss: 223.6806\n",
      "Current_batch_loss: 212.64629\n",
      "Current_batch_loss: 147.84291\n",
      "Current_batch_loss: 203.31439\n",
      "Current_batch_loss: 293.27051\n",
      "Current_batch_loss: 191.02252\n",
      "Current_batch_loss: 263.99954\n",
      "Current_batch_loss: 142.082\n",
      "Current_batch_loss: 233.12482\n",
      "Current_batch_loss: 204.85574\n",
      "Current_batch_loss: 185.40894\n",
      "Current_batch_loss: 319.17731\n",
      "Current_batch_loss: 197.92355\n",
      "Current_batch_loss: 167.2925\n",
      "Current_batch_loss: 178.52814\n",
      "Current_batch_loss: 198.05711\n",
      "Current_batch_loss: 184.73941\n",
      "Current_batch_loss: 168.53854\n",
      "Current_batch_loss: 225.83812\n",
      "Current_batch_loss: 129.80731\n",
      "Current_batch_loss: 257.34277\n",
      "Current_batch_loss: 151.41093\n",
      "Current_batch_loss: 239.73337\n",
      "Current_batch_loss: 170.39278\n",
      "Current_batch_loss: 178.84732\n",
      "Current_batch_loss: 194.5296\n",
      "Current_batch_loss: 181.78796\n",
      "Current_batch_loss: 233.44913\n",
      "Current_batch_loss: 194.65128\n",
      "Current_batch_loss: 213.40697\n",
      "Current_batch_loss: 159.91298\n",
      "Current_batch_loss: 198.51001\n",
      "Current_batch_loss: 194.55695\n",
      "Current_batch_loss: 197.02324\n",
      "Current_batch_loss: 189.09447\n",
      "Current_batch_loss: 145.8577\n",
      "Current_batch_loss: 140.69339\n",
      "Current_batch_loss: 194.67899\n",
      "Current_batch_loss: 207.77655\n",
      "Current_batch_loss: 210.41972\n",
      "Current_batch_loss: 194.4744\n",
      "Current_batch_loss: 157.27586\n",
      "Current_batch_loss: 205.19492\n",
      "Current_batch_loss: 156.66772\n",
      "Current_batch_loss: 180.50813\n",
      "Current_batch_loss: 112.81976\n",
      "Current_batch_loss: 141.94475\n",
      "Current_batch_loss: 202.78619\n",
      "Current_batch_loss: 228.15349\n",
      "Current_batch_loss: 131.47569\n",
      "Current_batch_loss: 275.86566\n",
      "Current_batch_loss: 191.50262\n",
      "Current_batch_loss: 123.1041\n",
      "Current_batch_loss: 193.58282\n",
      "Current_batch_loss: 185.77026\n",
      "Current_batch_loss: 187.21754\n",
      "Current_batch_loss: 149.91696\n",
      "Current_batch_loss: 178.43301\n",
      "Current_batch_loss: 177.96432\n",
      "Current_batch_loss: 120.2386\n",
      "Current_batch_loss: 131.94235\n",
      "Current_batch_loss: 150.89653\n",
      "Current_batch_loss: 199.16183\n",
      "Current_batch_loss: 173.25565\n",
      "Current_batch_loss: 115.80635\n",
      "Current_batch_loss: 169.31081\n",
      "Current_batch_loss: 231.28255\n",
      "Current_batch_loss: 127.50343\n",
      "Current_batch_loss: 137.7025\n",
      "Current_batch_loss: 144.94563\n",
      "Current_batch_loss: 168.01967\n",
      "Current_batch_loss: 132.36629\n",
      "Current_batch_loss: 166.28458\n",
      "Current_batch_loss: 129.31696\n",
      "Current_batch_loss: 140.13498\n",
      "Current_batch_loss: 175.79782\n",
      "Current_batch_loss: 155.31528\n",
      "Current_batch_loss: 208.7323\n",
      "Current_batch_loss: 196.67284\n",
      "Current_batch_loss: 179.47134\n",
      "Current_batch_loss: 102.23418\n",
      "Current_batch_loss: 133.85417\n",
      "Current_batch_loss: 173.91412\n",
      "Current_batch_loss: 228.9911\n",
      "Current_batch_loss: 82.99229\n",
      "Current_batch_loss: 146.57932\n",
      "Current_batch_loss: 143.84064\n",
      "Current_batch_loss: 143.04309\n",
      "Current_batch_loss: 255.14903\n",
      "Current_batch_loss: 251.43723\n",
      "Current_batch_loss: 112.28549\n",
      "Current_batch_loss: 181.53676\n",
      "Current_batch_loss: 146.23152\n",
      "Current_batch_loss: 196.22102\n",
      "Current_batch_loss: 136.07027\n",
      "Current_batch_loss: 176.48257\n",
      "Current_batch_loss: 97.90791\n",
      "Current_batch_loss: 132.41618\n",
      "Current_batch_loss: 162.2119\n",
      "Current_batch_loss: 223.34395\n",
      "Current_batch_loss: 161.68503\n",
      "Current_batch_loss: 236.97154\n",
      "Current_batch_loss: 123.83866\n",
      "Current_batch_loss: 101.88686\n",
      "Current_batch_loss: 93.98255\n",
      "Current_batch_loss: 154.12733\n",
      "Current_batch_loss: 107.4696\n",
      "Current_batch_loss: 74.91792\n",
      "Current_batch_loss: 169.88223\n",
      "Current_batch_loss: 187.5343\n",
      "Current_batch_loss: 158.64169\n",
      "Current_batch_loss: 179.15703\n",
      "Current_batch_loss: 140.69917\n",
      "Current_batch_loss: 91.60287\n",
      "Current_batch_loss: 140.98413\n",
      "Current_batch_loss: 116.04471\n",
      "Current_batch_loss: 175.87302\n",
      "Current_batch_loss: 185.6873\n",
      "Current_batch_loss: 121.64334\n",
      "Current_batch_loss: 205.36377\n",
      "Current_batch_loss: 197.50992\n",
      "Current_batch_loss: 108.66274\n",
      "Current_batch_loss: 144.69775\n",
      "Current_batch_loss: 219.00041\n",
      "Current_batch_loss: 98.401\n",
      "Current_batch_loss: 140.03415\n",
      "Current_batch_loss: 151.58568\n",
      "Current_batch_loss: 119.7132\n",
      "Current_batch_loss: 194.55968\n",
      "Current_batch_loss: 155.56352\n",
      "Current_batch_loss: 138.49165\n",
      "Current_batch_loss: 135.51799\n",
      "Current_batch_loss: 147.19592\n",
      "Current_batch_loss: 163.19447\n",
      "Current_batch_loss: 185.51111\n",
      "Current_batch_loss: 147.97696\n",
      "Current_batch_loss: 61.19196\n",
      "Current_batch_loss: 147.28766\n",
      "Current_batch_loss: 150.64035\n",
      "Current_batch_loss: 165.80621\n",
      "Current_batch_loss: 154.5172\n",
      "Current_batch_loss: 141.35858\n",
      "Current_batch_loss: 70.58782\n",
      "Current_batch_loss: 168.46477\n",
      "Current_batch_loss: 103.15714\n",
      "Current_batch_loss: 163.74443\n",
      "Current_batch_loss: 102.0495\n",
      "Current_batch_loss: 129.19711\n",
      "Current_batch_loss: 82.3868\n",
      "Current_batch_loss: 119.88319\n",
      "Current_batch_loss: 155.14413\n",
      "Current_batch_loss: 104.7326\n",
      "Current_batch_loss: 140.15179\n",
      "Current_batch_loss: 151.22585\n",
      "Current_batch_loss: 124.44421\n",
      "Current_batch_loss: 182.40007\n",
      "Current_batch_loss: 104.2495\n",
      "Current_batch_loss: 98.32286\n",
      "Current_batch_loss: 157.4595\n",
      "Current_batch_loss: 124.75761\n",
      "Current_batch_loss: 172.76358\n",
      "Current_batch_loss: 122.16136\n",
      "Current_batch_loss: 99.59189\n",
      "Current_batch_loss: 159.02925\n",
      "Current_batch_loss: 103.19659\n",
      "Current_batch_loss: 86.21755\n",
      "Current_batch_loss: 140.78189\n",
      "Current_batch_loss: 162.24654\n",
      "Current_batch_loss: 141.44739\n",
      "Current_batch_loss: 36.24771\n",
      "Current_batch_loss: 159.4664\n",
      "Current_batch_loss: 143.85358\n",
      "Current_batch_loss: 146.61311\n",
      "Current_batch_loss: 104.37698\n",
      "Current_batch_loss: 101.85083\n",
      "Current_batch_loss: 85.93604\n",
      "Current_batch_loss: 126.39735\n",
      "Current_batch_loss: 111.6199\n",
      "Current_batch_loss: 101.76727\n",
      "Current_batch_loss: 115.18478\n",
      "Current_batch_loss: 169.99164\n",
      "Current_batch_loss: 109.41127\n",
      "Current_batch_loss: 126.69147\n",
      "Current_batch_loss: 124.79241\n",
      "Current_batch_loss: 131.7549\n",
      "Current_batch_loss: 174.76764\n",
      "Current_batch_loss: 82.13707\n",
      "Current_batch_loss: 188.66243\n",
      "Current_batch_loss: 106.74599\n",
      "Current_batch_loss: 154.09488\n",
      "Current_batch_loss: 109.04633\n",
      "Current_batch_loss: 54.08785\n",
      "Current_batch_loss: 111.15054\n",
      "Current_batch_loss: 154.3445\n",
      "Current_batch_loss: 101.12788\n",
      "Current_batch_loss: 105.65874\n",
      "Current_batch_loss: 132.96925\n",
      "Current_batch_loss: 141.11005\n",
      "Current_batch_loss: 120.63805\n",
      "Current_batch_loss: 155.22894\n",
      "Current_batch_loss: 108.50918\n",
      "Current_batch_loss: 111.91878\n",
      "Current_batch_loss: 156.37704\n",
      "Current_batch_loss: 162.38657\n",
      "Current_batch_loss: 74.78374\n",
      "Current_batch_loss: 147.91243\n",
      "Current_batch_loss: 126.64058\n",
      "Current_batch_loss: 150.05153\n",
      "Current_batch_loss: 166.83589\n",
      "Current_batch_loss: 158.11084\n",
      "Current_batch_loss: 85.34143\n",
      "Current_batch_loss: 76.00098\n",
      "Current_batch_loss: 162.82861\n",
      "Current_batch_loss: 156.27856\n",
      "Current_batch_loss: 166.92618\n",
      "Current_batch_loss: 105.12094\n",
      "Current_batch_loss: 124.22474\n",
      "Current_batch_loss: 72.87738\n",
      "Current_batch_loss: 146.26888\n",
      "Current_batch_loss: 61.86683\n",
      "Current_batch_loss: 157.2162\n",
      "Current_batch_loss: 77.81367\n",
      "Current_batch_loss: 139.36206\n",
      "Current_batch_loss: 136.26508\n",
      "Current_batch_loss: 104.09096\n",
      "Current_batch_loss: 107.17878\n",
      "Current_batch_loss: 126.69569\n",
      "Current_batch_loss: 69.58413\n",
      "Current_batch_loss: 106.76687\n",
      "Current_batch_loss: 100.18907\n",
      "Current_batch_loss: 94.99859\n",
      "Current_batch_loss: 97.59817\n",
      "Current_batch_loss: 66.84274\n",
      "Current_batch_loss: 102.80428\n",
      "Current_batch_loss: 92.5617\n",
      "Current_batch_loss: 164.84253\n",
      "Current_batch_loss: 130.94339\n",
      "Current_batch_loss: 162.03239\n",
      "Current_batch_loss: 59.66016\n",
      "Current_batch_loss: 165.67624\n",
      "Current_batch_loss: 103.62402\n",
      "Current_batch_loss: 149.70938\n",
      "Current_batch_loss: 41.40258\n",
      "Current_batch_loss: 88.47811\n",
      "Current_batch_loss: 80.41582\n",
      "Current_batch_loss: 82.22004\n",
      "Current_batch_loss: 98.46192\n",
      "Current_batch_loss: 102.07233\n",
      "Current_batch_loss: 63.71017\n",
      "Current_batch_loss: 119.89985\n",
      "Current_batch_loss: 81.9984\n",
      "Current_batch_loss: 133.39505\n",
      "Current_batch_loss: 115.30598\n",
      "Current_batch_loss: 101.43481\n",
      "Current_batch_loss: 84.60637\n",
      "Current_batch_loss: 111.585\n",
      "Current_batch_loss: 87.26384\n",
      "Current_batch_loss: 87.6386\n",
      "Current_batch_loss: 96.514\n",
      "Current_batch_loss: 100.32375\n",
      "Current_batch_loss: 39.8287\n",
      "Current_batch_loss: 131.02469\n",
      "Current_batch_loss: 49.74937\n",
      "Current_batch_loss: 90.24281\n",
      "Current_batch_loss: 59.67191\n",
      "Current_batch_loss: 92.05756\n",
      "Current_batch_loss: 139.09671\n",
      "Current_batch_loss: 44.78572\n",
      "Current_batch_loss: 142.48274\n",
      "Current_batch_loss: 75.3395\n",
      "Current_batch_loss: 53.45947\n",
      "Current_batch_loss: 139.04861\n",
      "Current_batch_loss: 109.19753\n",
      "Current_batch_loss: 81.87731\n",
      "Current_batch_loss: 96.35232\n",
      "Current_batch_loss: 95.82337\n",
      "Current_batch_loss: 84.70124\n",
      "Current_batch_loss: 147.47285\n",
      "Current_batch_loss: 171.90453\n",
      "Current_batch_loss: 65.3615\n",
      "Current_batch_loss: 78.54964\n",
      "Current_batch_loss: 82.36217\n",
      "Current_batch_loss: 147.10649\n",
      "Current_batch_loss: 61.27271\n",
      "Current_batch_loss: 110.02915\n",
      "Current_batch_loss: 80.17965\n",
      "Current_batch_loss: 84.8316\n",
      "Current_batch_loss: 102.22405\n",
      "Current_batch_loss: 160.27809\n",
      "Current_batch_loss: 71.46434\n",
      "Current_batch_loss: 166.69278\n",
      "Current_batch_loss: 100.35246\n",
      "Current_batch_loss: 101.44051\n",
      "Current_batch_loss: 97.17455\n",
      "Current_batch_loss: 43.07211\n",
      "Current_batch_loss: 175.37547\n",
      "Current_batch_loss: 144.75681\n",
      "Current_batch_loss: 158.79643\n",
      "Current_batch_loss: 89.44981\n",
      "Current_batch_loss: 27.35254\n",
      "Current_batch_loss: 75.27598\n",
      "Current_batch_loss: 101.15154\n",
      "Current_batch_loss: 73.7793\n",
      "Current_batch_loss: 118.40259\n",
      "Current_batch_loss: 41.04919\n",
      "Current_batch_loss: 87.7049\n",
      "Current_batch_loss: 109.07245\n",
      "Current_batch_loss: 62.49798\n",
      "Current_batch_loss: 25.72227\n",
      "Current_batch_loss: 40.65453\n",
      "Current_batch_loss: 121.61927\n",
      "Current_batch_loss: 69.0452\n",
      "Current_batch_loss: 104.60963\n",
      "Current_batch_loss: 106.68754\n",
      "Current_batch_loss: 76.3785\n",
      "Current_batch_loss: 99.76613\n",
      "Current_batch_loss: 51.97637\n",
      "Current_batch_loss: 34.18189\n",
      "Current_batch_loss: 70.88075\n",
      "Current_batch_loss: 89.15033\n",
      "Current_batch_loss: 49.74739\n",
      "Current_batch_loss: 91.65222\n",
      "Current_batch_loss: 81.37989\n",
      "Current_batch_loss: 74.19312\n",
      "Current_batch_loss: 31.96805\n",
      "Current_batch_loss: 65.87779\n",
      "Current_batch_loss: 94.1341\n",
      "Current_batch_loss: 106.96939\n",
      "Current_batch_loss: 82.67862\n",
      "Current_batch_loss: 110.27596\n",
      "Current_batch_loss: 52.80689\n",
      "Current_batch_loss: 80.44343\n",
      "Current_batch_loss: 45.05113\n",
      "Current_batch_loss: 87.36654\n",
      "Current_batch_loss: 177.7946\n",
      "Current_batch_loss: 45.60839\n",
      "Current_batch_loss: 91.61172\n",
      "Current_batch_loss: 46.78058\n",
      "Current_batch_loss: 56.64935\n",
      "Current_batch_loss: 58.5205\n",
      "Current_batch_loss: 25.97631\n",
      "Current_batch_loss: 58.75453\n",
      "Current_batch_loss: 74.39966\n",
      "Current_batch_loss: 91.98015\n",
      "Current_batch_loss: 52.56712\n",
      "Current_batch_loss: 123.55391\n",
      "Current_batch_loss: 35.98912\n",
      "Current_batch_loss: 82.93401\n",
      "Current_batch_loss: 51.79281\n",
      "Current_batch_loss: 78.80009\n",
      "Current_batch_loss: 77.01289\n",
      "Current_batch_loss: 50.6727\n",
      "Current_batch_loss: 95.87935\n",
      "Current_batch_loss: 62.71315\n",
      "Current_batch_loss: 68.43716\n",
      "Current_batch_loss: 103.49921\n",
      "Current_batch_loss: 37.60351\n",
      "Current_batch_loss: 49.0379\n",
      "Current_batch_loss: 31.47366\n",
      "Current_batch_loss: 91.14984\n",
      "Current_batch_loss: 148.2261\n",
      "Current_batch_loss: 60.89525\n",
      "Current_batch_loss: 46.8396\n",
      "Current_batch_loss: 33.67304\n",
      "Current_batch_loss: 30.09294\n",
      "Current_batch_loss: 31.91141\n",
      "Current_batch_loss: 69.366\n",
      "Current_batch_loss: 65.38304\n",
      "Current_batch_loss: 96.82149\n",
      "Current_batch_loss: 263.15149\n",
      "Current_batch_loss: 27.95254\n",
      "Current_batch_loss: 49.05918\n",
      "Current_batch_loss: 42.90371\n",
      "Current_batch_loss: 66.47581\n",
      "Current_batch_loss: 75.38772\n",
      "Current_batch_loss: 58.63816\n",
      "Current_batch_loss: 36.42346\n",
      "Current_batch_loss: 70.36356\n",
      "Current_batch_loss: 61.9021\n",
      "Current_batch_loss: 81.44244\n",
      "Current_batch_loss: 72.70107\n",
      "Current_batch_loss: 55.93427\n",
      "Current_batch_loss: 85.3908\n",
      "Current_batch_loss: 28.45035\n",
      "Current_batch_loss: 104.85464\n",
      "Current_batch_loss: 62.74221\n",
      "Current_batch_loss: 53.70464\n",
      "Current_batch_loss: 138.01164\n",
      "Current_batch_loss: 77.67922\n",
      "Current_batch_loss: 75.60879\n",
      "Current_batch_loss: 62.72669\n",
      "Current_batch_loss: 84.17229\n",
      "Current_batch_loss: 128.46368\n",
      "Current_batch_loss: 79.16943\n",
      "Current_batch_loss: 63.89032\n",
      "Current_batch_loss: 34.19038\n",
      "Current_batch_loss: 95.35718\n",
      "Current_batch_loss: 29.04177\n",
      "Current_batch_loss: 62.67112\n",
      "Current_batch_loss: 130.95738\n",
      "Current_batch_loss: 19.25434\n",
      "Current_batch_loss: 84.18732\n",
      "Current_batch_loss: 39.03137\n",
      "Current_batch_loss: 86.7159\n",
      "Current_batch_loss: 38.59127\n",
      "Current_batch_loss: 47.39771\n",
      "Current_batch_loss: 47.0738\n",
      "Current_batch_loss: 64.34734\n",
      "Current_batch_loss: 52.19235\n",
      "Current_batch_loss: 43.9929\n",
      "Current_batch_loss: 72.12691\n",
      "Current_batch_loss: 29.51424\n",
      "Current_batch_loss: 71.83851\n",
      "Current_batch_loss: 88.9497\n",
      "Current_batch_loss: 71.07127\n",
      "Current_batch_loss: 46.53578\n",
      "Current_batch_loss: 112.52392\n",
      "Current_batch_loss: 34.81136\n",
      "Current_batch_loss: 60.74422\n",
      "Current_batch_loss: 43.66042\n",
      "Current_batch_loss: 37.24292\n",
      "Current_batch_loss: 52.19997\n",
      "Current_batch_loss: 63.3013\n",
      "Current_batch_loss: 50.86815\n",
      "Current_batch_loss: 69.75394\n",
      "Current_batch_loss: 33.00558\n",
      "Current_batch_loss: 27.38196\n",
      "Current_batch_loss: 65.5732\n",
      "Current_batch_loss: 23.2293\n",
      "Current_batch_loss: 75.2451\n",
      "Current_batch_loss: 37.64797\n",
      "Current_batch_loss: 43.76197\n",
      "Current_batch_loss: 42.45964\n",
      "Current_batch_loss: 50.94033\n",
      "Current_batch_loss: 79.32394\n",
      "Current_batch_loss: 78.5745\n",
      "Current_batch_loss: 18.37526\n",
      "Current_batch_loss: 31.23076\n",
      "Current_batch_loss: 72.00636\n",
      "Current_batch_loss: 8.77238\n",
      "Current_batch_loss: 36.8386\n",
      "Current_batch_loss: 26.85984\n",
      "Current_batch_loss: 67.68644\n",
      "Current_batch_loss: 39.08533\n",
      "Current_batch_loss: 51.58186\n",
      "Current_batch_loss: 90.79897\n",
      "Current_batch_loss: 49.93649\n",
      "Current_batch_loss: 60.60096\n",
      "Current_batch_loss: 46.3846\n",
      "Current_batch_loss: 37.45142\n",
      "Current_batch_loss: 59.97559\n",
      "Current_batch_loss: 25.1638\n",
      "Current_batch_loss: 57.19336\n",
      "Current_batch_loss: 51.7464\n",
      "Current_batch_loss: 23.6869\n",
      "Current_batch_loss: 38.60956\n",
      "Current_batch_loss: 50.68703\n",
      "Current_batch_loss: 16.23446\n",
      "Current_batch_loss: 29.3653\n",
      "Current_batch_loss: 47.32815\n",
      "Current_batch_loss: 35.0059\n",
      "Current_batch_loss: 103.53347\n",
      "Current_batch_loss: 16.53235\n",
      "Current_batch_loss: 85.99717\n",
      "Current_batch_loss: 46.61061\n",
      "Current_batch_loss: 67.16512\n",
      "Current_batch_loss: 32.40916\n",
      "Current_batch_loss: 55.09003\n",
      "Current_batch_loss: 19.92541\n",
      "Current_batch_loss: 28.60518\n",
      "Current_batch_loss: 54.58189\n",
      "Current_batch_loss: 28.40528\n",
      "Current_batch_loss: 44.382\n",
      "Current_batch_loss: 18.46805\n",
      "Current_batch_loss: 49.42391\n",
      "Current_batch_loss: 34.38388\n",
      "Current_batch_loss: 23.90827\n",
      "Current_batch_loss: 47.94843\n",
      "Current_batch_loss: 58.14559\n",
      "Current_batch_loss: 79.31014\n",
      "Current_batch_loss: 36.87685\n",
      "Current_batch_loss: 47.32943\n",
      "Current_batch_loss: 16.39313\n",
      "Current_batch_loss: 19.75589\n",
      "Current_batch_loss: 51.66959\n",
      "Current_batch_loss: 34.35012\n",
      "Current_batch_loss: 56.72792\n",
      "Current_batch_loss: 48.66261\n",
      "Current_batch_loss: 32.28457\n",
      "Current_batch_loss: 70.88524\n",
      "Current_batch_loss: 64.64785\n",
      "Current_batch_loss: 33.0402\n",
      "Current_batch_loss: 27.2931\n",
      "Current_batch_loss: 40.5917\n",
      "Current_batch_loss: 53.17151\n",
      "Current_batch_loss: 32.71069\n",
      "Current_batch_loss: 40.50345\n",
      "Current_batch_loss: 35.96584\n",
      "Current_batch_loss: 65.47341\n",
      "Current_batch_loss: 30.47127\n",
      "Current_batch_loss: 39.6096\n",
      "Current_batch_loss: 36.33369\n",
      "Current_batch_loss: 40.20028\n",
      "Current_batch_loss: 22.61942\n",
      "Current_batch_loss: 64.01051\n",
      "Current_batch_loss: 53.49677\n",
      "Current_batch_loss: 24.11386\n",
      "Current_batch_loss: 8.51063\n",
      "Current_batch_loss: 38.26259\n",
      "Current_batch_loss: 15.15028\n",
      "Current_batch_loss: 20.46323\n",
      "Current_batch_loss: 26.35869\n",
      "Current_batch_loss: 56.77197\n",
      "Current_batch_loss: 45.69302\n",
      "Current_batch_loss: 25.48653\n",
      "Current_batch_loss: 10.59631\n",
      "Current_batch_loss: 65.43378\n",
      "Current_batch_loss: 7.95413\n",
      "Current_batch_loss: 30.1007\n",
      "Current_batch_loss: 49.19968\n",
      "Current_batch_loss: 28.29567\n",
      "Current_batch_loss: 27.91623\n",
      "Current_batch_loss: 23.83479\n",
      "Current_batch_loss: 47.2187\n",
      "Current_batch_loss: 11.38874\n",
      "Current_batch_loss: 81.78508\n",
      "Current_batch_loss: 42.78592\n",
      "Current_batch_loss: 0.0693\n",
      "Current_batch_loss: 18.0496\n",
      "Current_batch_loss: 44.3883\n",
      "Current_batch_loss: 31.43891\n",
      "Current_batch_loss: 10.97137\n",
      "Current_batch_loss: 10.08669\n",
      "Current_batch_loss: 11.57822\n",
      "Current_batch_loss: 68.50323\n",
      "Current_batch_loss: 40.19099\n",
      "Current_batch_loss: 27.04848\n",
      "Current_batch_loss: 20.15302\n",
      "Current_batch_loss: 30.83058\n",
      "Current_batch_loss: 22.62203\n",
      "Current_batch_loss: 33.53942\n",
      "Current_batch_loss: 34.03297\n",
      "Current_batch_loss: 15.84866\n",
      "Current_batch_loss: 89.69343\n",
      "Current_batch_loss: 61.5525\n",
      "Current_batch_loss: 35.52967\n",
      "Current_batch_loss: 21.86803\n",
      "Current_batch_loss: 0.08119\n",
      "Current_batch_loss: 49.3987\n",
      "Current_batch_loss: 43.91986\n",
      "Current_batch_loss: 54.46633\n",
      "Current_batch_loss: 0.06805\n",
      "Current_batch_loss: 6.06148\n",
      "Current_batch_loss: 47.89071\n",
      "Current_batch_loss: 19.82413\n",
      "Current_batch_loss: 0.08775\n",
      "Current_batch_loss: 52.23076\n",
      "Current_batch_loss: 72.5428\n",
      "Current_batch_loss: 45.2828\n",
      "Current_batch_loss: 33.64283\n",
      "Current_batch_loss: 29.68923\n",
      "Current_batch_loss: 76.80257\n",
      "Current_batch_loss: 18.95554\n",
      "Current_batch_loss: 21.35087\n",
      "Current_batch_loss: 21.47576\n",
      "Current_batch_loss: 32.35342\n",
      "Current_batch_loss: 11.24767\n",
      "Current_batch_loss: 51.00445\n",
      "Current_batch_loss: 26.93288\n",
      "Current_batch_loss: 31.46605\n",
      "Current_batch_loss: 7.79199\n",
      "Current_batch_loss: 7.77604\n",
      "Current_batch_loss: 17.99052\n",
      "Current_batch_loss: 81.92088\n",
      "Current_batch_loss: 20.11865\n",
      "Current_batch_loss: 15.00721\n",
      "Current_batch_loss: 12.30779\n",
      "Current_batch_loss: 30.46291\n",
      "Current_batch_loss: 37.34785\n",
      "Current_batch_loss: 33.51387\n",
      "Current_batch_loss: 12.31791\n",
      "Current_batch_loss: 43.45917\n",
      "Current_batch_loss: 63.8922\n",
      "Current_batch_loss: 23.65937\n",
      "Current_batch_loss: 17.75983\n",
      "Current_batch_loss: 44.14726\n",
      "Current_batch_loss: 32.18207\n",
      "Current_batch_loss: 7.04142\n",
      "Current_batch_loss: 59.05115\n",
      "Current_batch_loss: 22.19544\n",
      "Current_batch_loss: 35.74547\n",
      "Current_batch_loss: 30.43907\n",
      "Current_batch_loss: 51.96262\n",
      "Current_batch_loss: 36.27585\n",
      "Current_batch_loss: 48.1543\n",
      "Current_batch_loss: 8.33864\n",
      "Current_batch_loss: 0.06044\n",
      "Current_batch_loss: 54.57711\n",
      "Current_batch_loss: 15.31296\n",
      "Current_batch_loss: 16.69232\n",
      "Current_batch_loss: 36.18738\n",
      "Current_batch_loss: 28.68994\n",
      "Current_batch_loss: 27.66445\n",
      "Current_batch_loss: 40.43935\n",
      "Current_batch_loss: 0.08517\n",
      "Current_batch_loss: 18.10489\n",
      "Current_batch_loss: 8.45263\n",
      "Current_batch_loss: 28.70714\n",
      "Current_batch_loss: 36.17941\n",
      "Current_batch_loss: 17.36794\n",
      "Current_batch_loss: 9.81816\n",
      "Current_batch_loss: 22.90033\n",
      "Current_batch_loss: 9.23226\n",
      "Current_batch_loss: 35.34063\n",
      "Current_batch_loss: 20.60112\n",
      "Current_batch_loss: 28.44667\n",
      "Current_batch_loss: 28.46017\n",
      "Current_batch_loss: 17.30597\n",
      "Current_batch_loss: 38.6439\n",
      "Current_batch_loss: 22.89965\n",
      "Current_batch_loss: 34.53008\n",
      "Current_batch_loss: 38.02913\n",
      "Current_batch_loss: 34.84138\n",
      "Current_batch_loss: 8.02781\n",
      "Current_batch_loss: 19.6672\n",
      "Current_batch_loss: 17.64988\n",
      "Current_batch_loss: 21.87174\n",
      "Current_batch_loss: 0.06151\n",
      "Current_batch_loss: 26.48885\n",
      "Current_batch_loss: 23.59537\n",
      "Current_batch_loss: 26.02557\n",
      "Current_batch_loss: 25.72263\n",
      "Current_batch_loss: 0.08027\n",
      "Current_batch_loss: 50.85037\n",
      "Current_batch_loss: 32.85971\n",
      "Current_batch_loss: 23.01486\n",
      "Current_batch_loss: 39.94193\n",
      "Current_batch_loss: 19.75215\n",
      "Current_batch_loss: 28.12305\n",
      "Current_batch_loss: 28.27674\n",
      "Current_batch_loss: 36.91631\n",
      "Current_batch_loss: 31.15886\n",
      "Current_batch_loss: 12.02934\n",
      "Current_batch_loss: 8.3094\n",
      "Current_batch_loss: 43.99586\n",
      "Current_batch_loss: 47.24241\n",
      "Current_batch_loss: 66.11562\n",
      "Current_batch_loss: 22.18769\n",
      "Current_batch_loss: 8.3247\n",
      "Current_batch_loss: 19.53505\n",
      "Current_batch_loss: 26.02242\n",
      "Current_batch_loss: 43.01568\n",
      "Current_batch_loss: 27.24243\n",
      "Current_batch_loss: 0.06004\n",
      "Current_batch_loss: 36.25829\n",
      "Current_batch_loss: 9.46585\n",
      "Current_batch_loss: 47.88318\n",
      "Current_batch_loss: 31.46985\n",
      "Current_batch_loss: 11.07554\n",
      "Current_batch_loss: 48.16757\n",
      "Current_batch_loss: 18.87053\n",
      "Current_batch_loss: 24.28427\n",
      "Current_batch_loss: 5.94106\n",
      "Current_batch_loss: 33.51954\n",
      "Current_batch_loss: 11.48767\n",
      "Current_batch_loss: 21.50411\n",
      "Current_batch_loss: 8.52326\n",
      "Current_batch_loss: 0.07964\n",
      "Current_batch_loss: 7.66921\n",
      "Current_batch_loss: 7.98391\n",
      "Current_batch_loss: 36.75021\n",
      "Current_batch_loss: 33.08629\n",
      "Current_batch_loss: 16.39313\n",
      "Current_batch_loss: 30.72933\n",
      "Current_batch_loss: 6.8506\n",
      "Current_batch_loss: 19.75159\n",
      "Current_batch_loss: 0.0614\n",
      "Current_batch_loss: 9.5367\n",
      "Current_batch_loss: 0.05495\n",
      "Current_batch_loss: 0.05874\n",
      "Current_batch_loss: 25.00038\n",
      "Current_batch_loss: 26.38886\n",
      "Current_batch_loss: 6.51053\n",
      "Current_batch_loss: 9.75048\n",
      "Current_batch_loss: 17.54587\n",
      "Current_batch_loss: 13.36986\n",
      "Current_batch_loss: 10.93694\n",
      "Current_batch_loss: 9.55258\n",
      "Current_batch_loss: 14.26443\n",
      "Current_batch_loss: 9.15119\n",
      "Current_batch_loss: 22.51615\n",
      "Current_batch_loss: 6.73554\n",
      "Current_batch_loss: 0.06444\n",
      "Current_batch_loss: 0.05307\n",
      "Current_batch_loss: 7.99904\n",
      "Current_batch_loss: 21.68591\n",
      "Current_batch_loss: 25.78622\n",
      "Current_batch_loss: 12.95237\n",
      "Current_batch_loss: 24.17611\n",
      "Current_batch_loss: 19.27982\n",
      "Current_batch_loss: 7.2343\n",
      "Current_batch_loss: 19.1755\n",
      "Current_batch_loss: 49.56038\n",
      "Current_batch_loss: 8.58994\n",
      "Current_batch_loss: 35.38589\n",
      "Current_batch_loss: 0.06892\n",
      "Current_batch_loss: 0.05941\n",
      "Current_batch_loss: 0.05722\n",
      "Current_batch_loss: 22.00072\n",
      "Current_batch_loss: 9.6345\n",
      "Current_batch_loss: 44.68187\n",
      "Current_batch_loss: 31.60192\n",
      "Current_batch_loss: 0.07086\n",
      "Current_batch_loss: 30.65238\n",
      "Current_batch_loss: 26.38088\n",
      "Current_batch_loss: 10.82476\n",
      "Current_batch_loss: 22.36344\n",
      "Current_batch_loss: 35.30135\n",
      "Current_batch_loss: 24.5503\n",
      "Current_batch_loss: 25.08523\n",
      "Current_batch_loss: 54.41118\n",
      "Current_batch_loss: 6.50396\n",
      "Current_batch_loss: 0.05139\n",
      "Current_batch_loss: 60.93615\n",
      "Current_batch_loss: 12.80191\n",
      "Current_batch_loss: 17.10397\n",
      "Current_batch_loss: 8.13884\n",
      "Current_batch_loss: 10.88725\n",
      "Current_batch_loss: 14.44423\n",
      "Current_batch_loss: 24.81305\n",
      "Current_batch_loss: 0.07128\n",
      "Current_batch_loss: 39.81058\n",
      "Current_batch_loss: 17.09459\n",
      "Current_batch_loss: 9.45608\n",
      "Current_batch_loss: 12.89698\n",
      "Current_batch_loss: 8.04171\n",
      "Current_batch_loss: 8.57763\n",
      "Current_batch_loss: 19.76427\n",
      "Current_batch_loss: 19.58555\n",
      "Current_batch_loss: 8.05513\n",
      "Current_batch_loss: 9.27813\n",
      "Current_batch_loss: 24.22044\n",
      "Current_batch_loss: 10.55512\n",
      "Current_batch_loss: 7.24449\n",
      "Current_batch_loss: 0.04882\n",
      "Current_batch_loss: 18.72195\n",
      "Current_batch_loss: 14.89059\n",
      "Current_batch_loss: 0.05453\n",
      "Current_batch_loss: 0.03666\n",
      "Current_batch_loss: 16.41466\n",
      "Current_batch_loss: 7.19269\n",
      "Current_batch_loss: 25.63433\n",
      "Current_batch_loss: 24.50498\n",
      "Current_batch_loss: 8.31062\n",
      "Current_batch_loss: 0.06092\n",
      "Current_batch_loss: 0.06187\n",
      "Current_batch_loss: 0.04605\n",
      "Current_batch_loss: 6.90582\n",
      "Current_batch_loss: 0.0481\n",
      "Current_batch_loss: 8.10807\n",
      "Current_batch_loss: 0.07233\n",
      "Current_batch_loss: 12.11635\n",
      "Current_batch_loss: 9.48182\n",
      "Current_batch_loss: 0.04723\n",
      "Current_batch_loss: 0.06499\n",
      "Current_batch_loss: 11.55741\n",
      "Current_batch_loss: 9.25239\n",
      "Current_batch_loss: 10.63494\n",
      "Current_batch_loss: 13.41906\n",
      "Current_batch_loss: 0.07393\n",
      "Current_batch_loss: 8.88347\n",
      "Current_batch_loss: 0.05032\n",
      "Current_batch_loss: 12.70316\n",
      "Current_batch_loss: 12.55079\n",
      "Current_batch_loss: 0.0538\n",
      "Current_batch_loss: 0.043\n",
      "Current_batch_loss: 22.90359\n",
      "Current_batch_loss: 14.06072\n",
      "Current_batch_loss: 26.81188\n",
      "Current_batch_loss: 52.26779\n",
      "Current_batch_loss: 22.38374\n",
      "Current_batch_loss: 15.61848\n",
      "Current_batch_loss: 11.47113\n",
      "Current_batch_loss: 0.05309\n",
      "Current_batch_loss: 7.63054\n",
      "Current_batch_loss: 29.22368\n",
      "Current_batch_loss: 18.70203\n",
      "Current_batch_loss: 0.05851\n",
      "Current_batch_loss: 9.64193\n",
      "Current_batch_loss: 16.84314\n",
      "Current_batch_loss: 10.27593\n",
      "Current_batch_loss: 15.50236\n",
      "Current_batch_loss: 12.00052\n",
      "Current_batch_loss: 0.04018\n",
      "Current_batch_loss: 0.05048\n",
      "Current_batch_loss: 8.53894\n",
      "Current_batch_loss: 9.78326\n",
      "Current_batch_loss: 28.22634\n",
      "Current_batch_loss: 0.04437\n",
      "Current_batch_loss: 0.05372\n",
      "Current_batch_loss: 0.05192\n",
      "Current_batch_loss: 7.21633\n",
      "Current_batch_loss: 9.01951\n",
      "Current_batch_loss: 0.07928\n",
      "Current_batch_loss: 0.05132\n",
      "Current_batch_loss: 0.04599\n",
      "Current_batch_loss: 21.88783\n",
      "Current_batch_loss: 21.24793\n",
      "Current_batch_loss: 25.39178\n",
      "Current_batch_loss: 0.04853\n",
      "Current_batch_loss: 0.05122\n",
      "Current_batch_loss: 0.04732\n",
      "Current_batch_loss: 15.56167\n",
      "Current_batch_loss: 0.05635\n",
      "Current_batch_loss: 0.05416\n",
      "Current_batch_loss: 5.47195\n",
      "Current_batch_loss: 28.80895\n",
      "Current_batch_loss: 0.04967\n",
      "Current_batch_loss: 9.77131\n",
      "Current_batch_loss: 0.04615\n",
      "Current_batch_loss: 0.05658\n",
      "Current_batch_loss: 0.05937\n",
      "Current_batch_loss: 6.02702\n",
      "Current_batch_loss: 25.51336\n",
      "Current_batch_loss: 0.0458\n",
      "Current_batch_loss: 16.47944\n",
      "Current_batch_loss: 0.06416\n",
      "Current_batch_loss: 10.94383\n",
      "Current_batch_loss: 9.85211\n",
      "Current_batch_loss: 14.03108\n",
      "Current_batch_loss: 0.05852\n",
      "Current_batch_loss: 0.03958\n",
      "Current_batch_loss: 0.04315\n",
      "Current_batch_loss: 12.84329\n",
      "Current_batch_loss: 6.41812\n",
      "Current_batch_loss: 12.00312\n",
      "Current_batch_loss: 0.06404\n",
      "Current_batch_loss: 20.95508\n",
      "Current_batch_loss: 21.5302\n",
      "Current_batch_loss: 17.91218\n",
      "Current_batch_loss: 13.96359\n",
      "Current_batch_loss: 0.06801\n",
      "Current_batch_loss: 0.04625\n",
      "Current_batch_loss: 11.64191\n",
      "Current_batch_loss: 10.90369\n",
      "Current_batch_loss: 0.06817\n",
      "Current_batch_loss: 15.32426\n",
      "Current_batch_loss: 14.25383\n",
      "Current_batch_loss: 12.06556\n",
      "Current_batch_loss: 35.08001\n",
      "Current_batch_loss: 0.05141\n",
      "Current_batch_loss: 7.75963\n",
      "Current_batch_loss: 0.04158\n",
      "Current_batch_loss: 8.89646\n",
      "Current_batch_loss: 15.58258\n",
      "Current_batch_loss: 5.7055\n",
      "Current_batch_loss: 22.16877\n",
      "Current_batch_loss: 11.32188\n",
      "Current_batch_loss: 0.04232\n",
      "Current_batch_loss: 24.79535\n",
      "Current_batch_loss: 6.07285\n",
      "Current_batch_loss: 33.84289\n",
      "Current_batch_loss: 15.0822\n",
      "Current_batch_loss: 23.34368\n",
      "Current_batch_loss: 12.28633\n",
      "Current_batch_loss: 18.06581\n",
      "Current_batch_loss: 16.74791\n",
      "Current_batch_loss: 0.05688\n",
      "Current_batch_loss: 13.22853\n",
      "Current_batch_loss: 7.61997\n",
      "Current_batch_loss: 0.0443\n",
      "Current_batch_loss: 0.0311\n",
      "Current_batch_loss: 0.03995\n",
      "Current_batch_loss: 10.84096\n",
      "Current_batch_loss: 10.24521\n",
      "Current_batch_loss: 12.02395\n",
      "Current_batch_loss: 0.0362\n",
      "Current_batch_loss: 11.50064\n",
      "Current_batch_loss: 0.04883\n",
      "Current_batch_loss: 0.05061\n",
      "Current_batch_loss: 0.04318\n",
      "Current_batch_loss: 8.28782\n",
      "Current_batch_loss: 0.04681\n",
      "Current_batch_loss: 0.04938\n",
      "Current_batch_loss: 9.46316\n",
      "Current_batch_loss: 0.04727\n",
      "Current_batch_loss: 0.04848\n",
      "Current_batch_loss: 19.85041\n",
      "Current_batch_loss: 21.57818\n",
      "Current_batch_loss: 0.04508\n",
      "Current_batch_loss: 0.04935\n",
      "Current_batch_loss: 12.62675\n",
      "Current_batch_loss: 23.79984\n",
      "Current_batch_loss: 0.04638\n",
      "Current_batch_loss: 11.99386\n",
      "Current_batch_loss: 18.77596\n",
      "Current_batch_loss: 0.06029\n",
      "Current_batch_loss: 24.50799\n",
      "Current_batch_loss: 10.83238\n",
      "Current_batch_loss: 18.76779\n",
      "Current_batch_loss: 0.04892\n",
      "Current_batch_loss: 0.03614\n",
      "Current_batch_loss: 0.04346\n",
      "Current_batch_loss: 14.99726\n",
      "Current_batch_loss: 0.04015\n",
      "Current_batch_loss: 10.11705\n",
      "Current_batch_loss: 0.04027\n",
      "Current_batch_loss: 0.05034\n",
      "Current_batch_loss: 11.37311\n",
      "Current_batch_loss: 8.46191\n",
      "Current_batch_loss: 9.0694\n",
      "Current_batch_loss: 16.95573\n",
      "Current_batch_loss: 0.04335\n",
      "Current_batch_loss: 14.51011\n",
      "Current_batch_loss: 20.18664\n",
      "Current_batch_loss: 9.25926\n",
      "Current_batch_loss: 11.57664\n",
      "Current_batch_loss: 0.04342\n",
      "Current_batch_loss: 8.96633\n",
      "Current_batch_loss: 0.06153\n",
      "Current_batch_loss: 0.05146\n",
      "Current_batch_loss: 10.03806\n",
      "Current_batch_loss: 21.55527\n",
      "Current_batch_loss: 0.03744\n",
      "Current_batch_loss: 10.5375\n",
      "Current_batch_loss: 0.04058\n",
      "Current_batch_loss: 0.03451\n",
      "Current_batch_loss: 0.04152\n",
      "Current_batch_loss: 0.03292\n",
      "Current_batch_loss: 0.047\n",
      "Current_batch_loss: 0.03928\n",
      "Current_batch_loss: 0.03688\n",
      "Current_batch_loss: 47.85579\n",
      "Current_batch_loss: 0.03934\n",
      "Current_batch_loss: 0.04832\n",
      "Current_batch_loss: 0.03918\n",
      "Current_batch_loss: 0.04667\n",
      "Current_batch_loss: 7.79442\n",
      "Current_batch_loss: 18.72123\n",
      "Current_batch_loss: 8.217\n",
      "Current_batch_loss: 0.04544\n",
      "Current_batch_loss: 24.77775\n",
      "Current_batch_loss: 0.03227\n",
      "Current_batch_loss: 12.46697\n",
      "Current_batch_loss: 16.62719\n",
      "Current_batch_loss: 11.09696\n",
      "Current_batch_loss: 0.03691\n",
      "Current_batch_loss: 0.03201\n",
      "Current_batch_loss: 0.03991\n",
      "Current_batch_loss: 0.04003\n",
      "Current_batch_loss: 0.04195\n",
      "Current_batch_loss: 17.11928\n",
      "Current_batch_loss: 0.04462\n",
      "Current_batch_loss: 31.27263\n",
      "Current_batch_loss: 0.03937\n",
      "Current_batch_loss: 11.12513\n",
      "Current_batch_loss: 6.64951\n",
      "Current_batch_loss: 0.04505\n",
      "Current_batch_loss: 0.03275\n",
      "Current_batch_loss: 11.81523\n",
      "Current_batch_loss: 8.3184\n",
      "Current_batch_loss: 0.02537\n",
      "Current_batch_loss: 8.03822\n",
      "Current_batch_loss: 12.11732\n",
      "Current_batch_loss: 0.03626\n",
      "Current_batch_loss: 8.75408\n",
      "Current_batch_loss: 0.0415\n",
      "Current_batch_loss: 0.031\n",
      "Current_batch_loss: 0.04719\n",
      "Current_batch_loss: 8.6655\n",
      "Current_batch_loss: 6.94342\n",
      "Current_batch_loss: 0.04479\n",
      "Current_batch_loss: 0.04773\n",
      "Current_batch_loss: 15.69805\n",
      "Current_batch_loss: 0.0432\n",
      "Current_batch_loss: 12.26766\n",
      "Current_batch_loss: 0.03272\n",
      "Current_batch_loss: 0.03261\n",
      "Current_batch_loss: 0.04644\n",
      "Current_batch_loss: 13.04237\n",
      "Current_batch_loss: 9.0918\n",
      "Current_batch_loss: 0.03773\n",
      "Current_batch_loss: 0.05294\n",
      "Current_batch_loss: 0.04052\n",
      "Current_batch_loss: 0.04727\n",
      "Current_batch_loss: 10.47768\n",
      "Current_batch_loss: 9.95789\n",
      "Current_batch_loss: 0.0355\n",
      "Current_batch_loss: 0.03346\n",
      "Current_batch_loss: 0.04617\n",
      "Current_batch_loss: 0.0329\n",
      "Current_batch_loss: 0.03551\n",
      "Current_batch_loss: 22.03827\n",
      "Current_batch_loss: 10.91128\n",
      "Current_batch_loss: 11.14113\n",
      "Current_batch_loss: 0.02856\n",
      "Current_batch_loss: 0.03571\n",
      "Current_batch_loss: 0.04928\n",
      "Current_batch_loss: 9.06343\n",
      "Current_batch_loss: 0.02981\n",
      "Current_batch_loss: 0.03364\n",
      "Current_batch_loss: 12.02134\n",
      "Current_batch_loss: 0.03265\n",
      "Current_batch_loss: 10.75034\n",
      "Current_batch_loss: 0.05768\n",
      "Current_batch_loss: 0.03874\n",
      "Current_batch_loss: 4.54891\n",
      "Current_batch_loss: 0.0309\n",
      "Current_batch_loss: 0.04381\n",
      "Current_batch_loss: 0.04471\n",
      "Current_batch_loss: 6.06907\n",
      "Current_batch_loss: 0.02937\n",
      "Current_batch_loss: 0.04275\n",
      "Current_batch_loss: 0.03136\n",
      "Current_batch_loss: 0.02758\n",
      "Current_batch_loss: 0.03069\n",
      "Current_batch_loss: 10.03272\n",
      "Current_batch_loss: 0.05782\n",
      "Current_batch_loss: 15.15937\n",
      "Current_batch_loss: 0.04913\n",
      "Current_batch_loss: 24.92535\n",
      "Current_batch_loss: 0.02875\n",
      "Current_batch_loss: 0.03474\n",
      "Current_batch_loss: 13.06198\n",
      "Current_batch_loss: 0.05482\n",
      "Current_batch_loss: 0.03983\n",
      "Current_batch_loss: 14.78364\n",
      "Current_batch_loss: 0.03784\n",
      "Current_batch_loss: 15.26653\n",
      "Current_batch_loss: 12.32477\n",
      "Current_batch_loss: 0.03292\n",
      "Current_batch_loss: 0.02946\n",
      "Current_batch_loss: 0.02867\n",
      "Current_batch_loss: 6.86482\n",
      "Current_batch_loss: 10.92434\n",
      "Current_batch_loss: 0.03209\n",
      "Current_batch_loss: 0.03539\n",
      "Current_batch_loss: 0.04397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [2:00:37<58:18:18, 7237.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_batch_loss: 0.03363\n",
      "Epoch #1 | Loss: 305.93445\n",
      "Current_batch_loss: 1043.64624\n",
      "Current_batch_loss: 984.08398\n",
      "Current_batch_loss: 905.46088\n",
      "Current_batch_loss: 1000.42682\n",
      "Current_batch_loss: 1050.29456\n",
      "Current_batch_loss: 944.00891\n",
      "Current_batch_loss: 1006.75024\n",
      "Current_batch_loss: 950.53937\n",
      "Current_batch_loss: 898.73578\n",
      "Current_batch_loss: 808.61267\n",
      "Current_batch_loss: 1016.75659\n",
      "Current_batch_loss: 943.31726\n",
      "Current_batch_loss: 911.22229\n",
      "Current_batch_loss: 895.33551\n",
      "Current_batch_loss: 961.39771\n",
      "Current_batch_loss: 950.77045\n",
      "Current_batch_loss: 1034.05139\n",
      "Current_batch_loss: 976.70477\n",
      "Current_batch_loss: 910.37878\n",
      "Current_batch_loss: 944.24518\n",
      "Current_batch_loss: 996.86432\n",
      "Current_batch_loss: 1039.28809\n",
      "Current_batch_loss: 1043.17896\n",
      "Current_batch_loss: 988.28741\n",
      "Current_batch_loss: 1006.1615\n",
      "Current_batch_loss: 963.9248\n",
      "Current_batch_loss: 847.99585\n",
      "Current_batch_loss: 882.40314\n",
      "Current_batch_loss: 931.89026\n",
      "Current_batch_loss: 883.41455\n",
      "Current_batch_loss: 977.09021\n",
      "Current_batch_loss: 912.5639\n",
      "Current_batch_loss: 987.04279\n",
      "Current_batch_loss: 1041.30725\n",
      "Current_batch_loss: 980.98456\n",
      "Current_batch_loss: 901.21014\n",
      "Current_batch_loss: 955.59003\n",
      "Current_batch_loss: 945.94965\n",
      "Current_batch_loss: 801.40363\n",
      "Current_batch_loss: 905.46497\n",
      "Current_batch_loss: 1009.78339\n",
      "Current_batch_loss: 866.50012\n",
      "Current_batch_loss: 880.60681\n",
      "Current_batch_loss: 902.18671\n",
      "Current_batch_loss: 837.27557\n",
      "Current_batch_loss: 795.05121\n",
      "Current_batch_loss: 734.81464\n",
      "Current_batch_loss: 813.56366\n",
      "Current_batch_loss: 983.04718\n",
      "Current_batch_loss: 923.03314\n",
      "Current_batch_loss: 884.91107\n",
      "Current_batch_loss: 822.43854\n",
      "Current_batch_loss: 983.13727\n",
      "Current_batch_loss: 906.56091\n",
      "Current_batch_loss: 901.02039\n",
      "Current_batch_loss: 1020.17676\n",
      "Current_batch_loss: 983.05334\n",
      "Current_batch_loss: 976.37018\n",
      "Current_batch_loss: 883.83826\n",
      "Current_batch_loss: 874.43439\n",
      "Current_batch_loss: 909.98822\n",
      "Current_batch_loss: 850.75433\n",
      "Current_batch_loss: 884.29706\n",
      "Current_batch_loss: 797.0932\n",
      "Current_batch_loss: 912.07416\n",
      "Current_batch_loss: 883.24103\n",
      "Current_batch_loss: 869.75525\n",
      "Current_batch_loss: 954.57288\n",
      "Current_batch_loss: 907.59656\n",
      "Current_batch_loss: 780.82141\n",
      "Current_batch_loss: 925.18011\n",
      "Current_batch_loss: 851.7511\n",
      "Current_batch_loss: 898.0415\n",
      "Current_batch_loss: 856.0813\n",
      "Current_batch_loss: 768.5249\n",
      "Current_batch_loss: 836.37964\n",
      "Current_batch_loss: 800.62061\n",
      "Current_batch_loss: 915.16473\n",
      "Current_batch_loss: 710.88678\n",
      "Current_batch_loss: 967.94983\n",
      "Current_batch_loss: 900.75507\n",
      "Current_batch_loss: 912.24829\n",
      "Current_batch_loss: 958.21014\n",
      "Current_batch_loss: 922.72003\n",
      "Current_batch_loss: 858.62286\n",
      "Current_batch_loss: 900.90033\n",
      "Current_batch_loss: 875.11652\n",
      "Current_batch_loss: 772.94849\n",
      "Current_batch_loss: 859.65277\n",
      "Current_batch_loss: 726.15173\n",
      "Current_batch_loss: 937.94604\n",
      "Current_batch_loss: 665.93573\n",
      "Current_batch_loss: 965.73706\n",
      "Current_batch_loss: 924.8335\n",
      "Current_batch_loss: 848.9173\n",
      "Current_batch_loss: 924.42499\n",
      "Current_batch_loss: 808.83618\n",
      "Current_batch_loss: 849.47498\n",
      "Current_batch_loss: 837.95569\n",
      "Current_batch_loss: 793.66864\n",
      "Current_batch_loss: 906.94464\n",
      "Current_batch_loss: 887.43304\n",
      "Current_batch_loss: 843.1629\n",
      "Current_batch_loss: 871.23175\n",
      "Current_batch_loss: 898.3222\n",
      "Current_batch_loss: 808.35144\n",
      "Current_batch_loss: 822.17816\n",
      "Current_batch_loss: 744.10437\n",
      "Current_batch_loss: 919.54761\n",
      "Current_batch_loss: 882.32703\n",
      "Current_batch_loss: 974.09589\n",
      "Current_batch_loss: 896.50659\n",
      "Current_batch_loss: 912.0235\n",
      "Current_batch_loss: 769.50281\n",
      "Current_batch_loss: 932.42426\n",
      "Current_batch_loss: 706.22955\n",
      "Current_batch_loss: 716.39221\n",
      "Current_batch_loss: 800.77252\n",
      "Current_batch_loss: 776.21716\n",
      "Current_batch_loss: 945.84961\n",
      "Current_batch_loss: 789.22876\n",
      "Current_batch_loss: 662.12341\n",
      "Current_batch_loss: 836.45776\n",
      "Current_batch_loss: 789.40369\n",
      "Current_batch_loss: 939.44727\n",
      "Current_batch_loss: 725.23749\n",
      "Current_batch_loss: 718.53595\n",
      "Current_batch_loss: 920.2533\n",
      "Current_batch_loss: 784.06604\n",
      "Current_batch_loss: 823.01111\n",
      "Current_batch_loss: 912.80835\n",
      "Current_batch_loss: 714.23663\n",
      "Current_batch_loss: 898.56348\n",
      "Current_batch_loss: 951.60791\n",
      "Current_batch_loss: 831.46246\n",
      "Current_batch_loss: 833.53009\n",
      "Current_batch_loss: 754.75262\n",
      "Current_batch_loss: 846.06073\n",
      "Current_batch_loss: 772.8985\n",
      "Current_batch_loss: 886.66187\n",
      "Current_batch_loss: 835.03058\n",
      "Current_batch_loss: 767.72491\n",
      "Current_batch_loss: 885.67944\n",
      "Current_batch_loss: 836.39795\n",
      "Current_batch_loss: 938.9494\n",
      "Current_batch_loss: 877.44928\n",
      "Current_batch_loss: 431.14212\n",
      "Current_batch_loss: 820.47552\n",
      "Current_batch_loss: 822.89685\n",
      "Current_batch_loss: 817.63647\n",
      "Current_batch_loss: 884.17957\n",
      "Current_batch_loss: 892.22522\n",
      "Current_batch_loss: 792.79309\n",
      "Current_batch_loss: 896.33252\n",
      "Current_batch_loss: 829.21075\n",
      "Current_batch_loss: 835.32495\n",
      "Current_batch_loss: 701.63385\n",
      "Current_batch_loss: 719.24658\n",
      "Current_batch_loss: 862.85986\n",
      "Current_batch_loss: 906.47424\n",
      "Current_batch_loss: 694.40704\n",
      "Current_batch_loss: 824.07019\n",
      "Current_batch_loss: 892.18848\n",
      "Current_batch_loss: 919.9248\n",
      "Current_batch_loss: 874.46661\n",
      "Current_batch_loss: 814.63989\n",
      "Current_batch_loss: 864.6239\n",
      "Current_batch_loss: 818.3078\n",
      "Current_batch_loss: 796.19733\n",
      "Current_batch_loss: 681.3382\n",
      "Current_batch_loss: 706.86963\n",
      "Current_batch_loss: 537.60577\n",
      "Current_batch_loss: 917.83228\n",
      "Current_batch_loss: 819.53357\n",
      "Current_batch_loss: 671.3468\n",
      "Current_batch_loss: 784.58386\n",
      "Current_batch_loss: 815.01904\n",
      "Current_batch_loss: 876.17944\n",
      "Current_batch_loss: 821.00391\n",
      "Current_batch_loss: 919.98895\n",
      "Current_batch_loss: 799.94562\n",
      "Current_batch_loss: 833.74017\n",
      "Current_batch_loss: 831.7594\n",
      "Current_batch_loss: 821.93555\n",
      "Current_batch_loss: 815.17896\n",
      "Current_batch_loss: 777.12195\n",
      "Current_batch_loss: 905.99359\n",
      "Current_batch_loss: 787.59668\n",
      "Current_batch_loss: 792.87573\n",
      "Current_batch_loss: 810.8443\n",
      "Current_batch_loss: 745.69189\n",
      "Current_batch_loss: 713.50446\n",
      "Current_batch_loss: 691.92932\n",
      "Current_batch_loss: 764.13116\n",
      "Current_batch_loss: 822.98718\n",
      "Current_batch_loss: 781.70465\n",
      "Current_batch_loss: 793.5943\n",
      "Current_batch_loss: 572.83026\n",
      "Current_batch_loss: 785.11841\n",
      "Current_batch_loss: 934.61938\n",
      "Current_batch_loss: 832.15875\n",
      "Current_batch_loss: 842.78632\n",
      "Current_batch_loss: 742.00311\n",
      "Current_batch_loss: 772.99506\n",
      "Current_batch_loss: 634.1344\n",
      "Current_batch_loss: 874.77429\n",
      "Current_batch_loss: 742.422\n",
      "Current_batch_loss: 809.75647\n",
      "Current_batch_loss: 877.2981\n",
      "Current_batch_loss: 695.14453\n",
      "Current_batch_loss: 801.14661\n",
      "Current_batch_loss: 688.9624\n",
      "Current_batch_loss: 586.62091\n",
      "Current_batch_loss: 702.02838\n",
      "Current_batch_loss: 794.54065\n",
      "Current_batch_loss: 747.68451\n",
      "Current_batch_loss: 702.79663\n",
      "Current_batch_loss: 694.03046\n",
      "Current_batch_loss: 737.24597\n",
      "Current_batch_loss: 805.16187\n",
      "Current_batch_loss: 621.14227\n",
      "Current_batch_loss: 661.25952\n",
      "Current_batch_loss: 851.51746\n",
      "Current_batch_loss: 784.69946\n",
      "Current_batch_loss: 745.66077\n",
      "Current_batch_loss: 560.53796\n",
      "Current_batch_loss: 754.72821\n",
      "Current_batch_loss: 788.67657\n",
      "Current_batch_loss: 617.21735\n",
      "Current_batch_loss: 779.6225\n",
      "Current_batch_loss: 715.01666\n",
      "Current_batch_loss: 655.71466\n",
      "Current_batch_loss: 690.48138\n",
      "Current_batch_loss: 790.93781\n",
      "Current_batch_loss: 609.76239\n",
      "Current_batch_loss: 878.24323\n",
      "Current_batch_loss: 713.88055\n",
      "Current_batch_loss: 822.9787\n",
      "Current_batch_loss: 774.71283\n",
      "Current_batch_loss: 646.85596\n",
      "Current_batch_loss: 695.14526\n",
      "Current_batch_loss: 680.79236\n",
      "Current_batch_loss: 891.73724\n",
      "Current_batch_loss: 815.07581\n",
      "Current_batch_loss: 728.98431\n",
      "Current_batch_loss: 697.08929\n",
      "Current_batch_loss: 668.57025\n",
      "Current_batch_loss: 739.94861\n",
      "Current_batch_loss: 780.60754\n",
      "Current_batch_loss: 822.04529\n",
      "Current_batch_loss: 831.98584\n",
      "Current_batch_loss: 569.12634\n",
      "Current_batch_loss: 746.11011\n",
      "Current_batch_loss: 801.0238\n",
      "Current_batch_loss: 786.74133\n",
      "Current_batch_loss: 757.94824\n",
      "Current_batch_loss: 577.62079\n",
      "Current_batch_loss: 753.8125\n",
      "Current_batch_loss: 807.48163\n",
      "Current_batch_loss: 745.28979\n",
      "Current_batch_loss: 748.51105\n",
      "Current_batch_loss: 660.20428\n",
      "Current_batch_loss: 799.81134\n",
      "Current_batch_loss: 678.63531\n",
      "Current_batch_loss: 760.53101\n",
      "Current_batch_loss: 653.80188\n",
      "Current_batch_loss: 736.41516\n",
      "Current_batch_loss: 627.36316\n",
      "Current_batch_loss: 747.88055\n",
      "Current_batch_loss: 678.10028\n",
      "Current_batch_loss: 542.21368\n",
      "Current_batch_loss: 783.30493\n",
      "Current_batch_loss: 520.76971\n",
      "Current_batch_loss: 604.84814\n",
      "Current_batch_loss: 697.36755\n",
      "Current_batch_loss: 924.68951\n",
      "Current_batch_loss: 766.85181\n",
      "Current_batch_loss: 638.83014\n",
      "Current_batch_loss: 787.01306\n",
      "Current_batch_loss: 749.98029\n",
      "Current_batch_loss: 791.10712\n",
      "Current_batch_loss: 857.13495\n",
      "Current_batch_loss: 652.95471\n",
      "Current_batch_loss: 741.55627\n",
      "Current_batch_loss: 775.1864\n",
      "Current_batch_loss: 699.32629\n",
      "Current_batch_loss: 672.91309\n",
      "Current_batch_loss: 964.24347\n",
      "Current_batch_loss: 705.22803\n",
      "Current_batch_loss: 756.10034\n",
      "Current_batch_loss: 630.25122\n",
      "Current_batch_loss: 754.90729\n",
      "Current_batch_loss: 606.02399\n",
      "Current_batch_loss: 742.67426\n",
      "Current_batch_loss: 657.14172\n",
      "Current_batch_loss: 704.14154\n",
      "Current_batch_loss: 691.2226\n",
      "Current_batch_loss: 707.2807\n",
      "Current_batch_loss: 723.94373\n",
      "Current_batch_loss: 717.37189\n",
      "Current_batch_loss: 757.65033\n",
      "Current_batch_loss: 747.06519\n",
      "Current_batch_loss: 714.88464\n",
      "Current_batch_loss: 704.69196\n",
      "Current_batch_loss: 715.92499\n",
      "Current_batch_loss: 816.32532\n",
      "Current_batch_loss: 803.32941\n",
      "Current_batch_loss: 746.49091\n",
      "Current_batch_loss: 638.05188\n",
      "Current_batch_loss: 659.35767\n",
      "Current_batch_loss: 722.77246\n",
      "Current_batch_loss: 569.26172\n",
      "Current_batch_loss: 762.26624\n",
      "Current_batch_loss: 575.9765\n",
      "Current_batch_loss: 690.25354\n",
      "Current_batch_loss: 730.54346\n",
      "Current_batch_loss: 647.50104\n",
      "Current_batch_loss: 748.4245\n",
      "Current_batch_loss: 806.39746\n",
      "Current_batch_loss: 637.79956\n",
      "Current_batch_loss: 799.88324\n",
      "Current_batch_loss: 512.70221\n",
      "Current_batch_loss: 679.98578\n",
      "Current_batch_loss: 669.19983\n",
      "Current_batch_loss: 637.74292\n",
      "Current_batch_loss: 571.82159\n",
      "Current_batch_loss: 572.94208\n",
      "Current_batch_loss: 674.94037\n",
      "Current_batch_loss: 690.53357\n",
      "Current_batch_loss: 703.56519\n",
      "Current_batch_loss: 562.61774\n",
      "Current_batch_loss: 797.1969\n",
      "Current_batch_loss: 566.04095\n",
      "Current_batch_loss: 645.56085\n",
      "Current_batch_loss: 584.61761\n",
      "Current_batch_loss: 674.40967\n",
      "Current_batch_loss: 656.04962\n",
      "Current_batch_loss: 823.12048\n",
      "Current_batch_loss: 766.30347\n",
      "Current_batch_loss: 692.25128\n",
      "Current_batch_loss: 595.9679\n",
      "Current_batch_loss: 765.92047\n",
      "Current_batch_loss: 671.58649\n",
      "Current_batch_loss: 701.87842\n",
      "Current_batch_loss: 508.62784\n",
      "Current_batch_loss: 653.22369\n",
      "Current_batch_loss: 608.23773\n",
      "Current_batch_loss: 693.52014\n",
      "Current_batch_loss: 696.26648\n",
      "Current_batch_loss: 636.08051\n",
      "Current_batch_loss: 672.50354\n",
      "Current_batch_loss: 631.47284\n",
      "Current_batch_loss: 592.41718\n",
      "Current_batch_loss: 536.56726\n",
      "Current_batch_loss: 703.19891\n",
      "Current_batch_loss: 593.86371\n",
      "Current_batch_loss: 547.97931\n",
      "Current_batch_loss: 613.41553\n",
      "Current_batch_loss: 730.0589\n",
      "Current_batch_loss: 715.12292\n",
      "Current_batch_loss: 588.99902\n",
      "Current_batch_loss: 572.66437\n",
      "Current_batch_loss: 626.42694\n",
      "Current_batch_loss: 731.0141\n",
      "Current_batch_loss: 711.9408\n",
      "Current_batch_loss: 645.84662\n",
      "Current_batch_loss: 583.96851\n",
      "Current_batch_loss: 646.71173\n",
      "Current_batch_loss: 580.81964\n",
      "Current_batch_loss: 663.0694\n",
      "Current_batch_loss: 651.80707\n",
      "Current_batch_loss: 607.5636\n",
      "Current_batch_loss: 579.48541\n",
      "Current_batch_loss: 678.48352\n",
      "Current_batch_loss: 632.42456\n",
      "Current_batch_loss: 597.64008\n",
      "Current_batch_loss: 718.68439\n",
      "Current_batch_loss: 614.03082\n",
      "Current_batch_loss: 634.76917\n",
      "Current_batch_loss: 584.05145\n",
      "Current_batch_loss: 697.71198\n",
      "Current_batch_loss: 692.05255\n",
      "Current_batch_loss: 574.90314\n",
      "Current_batch_loss: 528.03851\n",
      "Current_batch_loss: 693.58514\n",
      "Current_batch_loss: 603.3172\n",
      "Current_batch_loss: 671.94403\n",
      "Current_batch_loss: 626.95441\n",
      "Current_batch_loss: 682.66766\n",
      "Current_batch_loss: 733.53876\n",
      "Current_batch_loss: 599.10406\n",
      "Current_batch_loss: 636.78918\n",
      "Current_batch_loss: 471.04822\n",
      "Current_batch_loss: 685.94495\n",
      "Current_batch_loss: 622.06873\n",
      "Current_batch_loss: 560.00104\n",
      "Current_batch_loss: 650.64642\n",
      "Current_batch_loss: 531.48779\n",
      "Current_batch_loss: 562.67548\n",
      "Current_batch_loss: 561.16919\n",
      "Current_batch_loss: 572.96295\n",
      "Current_batch_loss: 528.25525\n",
      "Current_batch_loss: 603.63269\n",
      "Current_batch_loss: 680.91718\n",
      "Current_batch_loss: 715.74597\n",
      "Current_batch_loss: 703.94629\n",
      "Current_batch_loss: 542.29254\n",
      "Current_batch_loss: 525.17609\n",
      "Current_batch_loss: 570.25775\n",
      "Current_batch_loss: 673.44421\n",
      "Current_batch_loss: 578.91461\n",
      "Current_batch_loss: 591.29211\n",
      "Current_batch_loss: 559.64154\n",
      "Current_batch_loss: 735.82196\n",
      "Current_batch_loss: 488.63986\n",
      "Current_batch_loss: 582.1615\n",
      "Current_batch_loss: 746.05798\n",
      "Current_batch_loss: 627.54419\n",
      "Current_batch_loss: 651.98621\n",
      "Current_batch_loss: 738.70776\n",
      "Current_batch_loss: 579.74243\n",
      "Current_batch_loss: 660.28693\n",
      "Current_batch_loss: 466.48615\n",
      "Current_batch_loss: 536.29248\n",
      "Current_batch_loss: 452.68115\n",
      "Current_batch_loss: 593.11804\n",
      "Current_batch_loss: 736.672\n",
      "Current_batch_loss: 720.34296\n",
      "Current_batch_loss: 595.79425\n",
      "Current_batch_loss: 572.53967\n",
      "Current_batch_loss: 701.29376\n",
      "Current_batch_loss: 548.92346\n",
      "Current_batch_loss: 729.0741\n",
      "Current_batch_loss: 705.2041\n",
      "Current_batch_loss: 632.86066\n",
      "Current_batch_loss: 635.27008\n",
      "Current_batch_loss: 518.34772\n",
      "Current_batch_loss: 557.85522\n",
      "Current_batch_loss: 682.55151\n",
      "Current_batch_loss: 635.61456\n",
      "Current_batch_loss: 660.99286\n",
      "Current_batch_loss: 643.76715\n",
      "Current_batch_loss: 521.19489\n",
      "Current_batch_loss: 793.16583\n",
      "Current_batch_loss: 506.83389\n",
      "Current_batch_loss: 574.04926\n",
      "Current_batch_loss: 571.81848\n",
      "Current_batch_loss: 589.40979\n",
      "Current_batch_loss: 654.53564\n",
      "Current_batch_loss: 676.38171\n",
      "Current_batch_loss: 607.47021\n",
      "Current_batch_loss: 688.62457\n",
      "Current_batch_loss: 582.88574\n",
      "Current_batch_loss: 575.82501\n",
      "Current_batch_loss: 597.37787\n",
      "Current_batch_loss: 473.25409\n",
      "Current_batch_loss: 568.15918\n",
      "Current_batch_loss: 695.54651\n",
      "Current_batch_loss: 470.14447\n",
      "Current_batch_loss: 603.50671\n",
      "Current_batch_loss: 421.50333\n",
      "Current_batch_loss: 731.62769\n",
      "Current_batch_loss: 569.13959\n",
      "Current_batch_loss: 598.24652\n",
      "Current_batch_loss: 581.62842\n",
      "Current_batch_loss: 527.29578\n",
      "Current_batch_loss: 559.55676\n",
      "Current_batch_loss: 734.09802\n",
      "Current_batch_loss: 537.62543\n",
      "Current_batch_loss: 417.5094\n",
      "Current_batch_loss: 511.91733\n",
      "Current_batch_loss: 602.84772\n",
      "Current_batch_loss: 546.14966\n",
      "Current_batch_loss: 562.06659\n",
      "Current_batch_loss: 522.27466\n",
      "Current_batch_loss: 590.62799\n",
      "Current_batch_loss: 576.60559\n",
      "Current_batch_loss: 481.27216\n",
      "Current_batch_loss: 525.54547\n",
      "Current_batch_loss: 529.54541\n",
      "Current_batch_loss: 401.74994\n",
      "Current_batch_loss: 605.39093\n",
      "Current_batch_loss: 538.26624\n",
      "Current_batch_loss: 569.83923\n",
      "Current_batch_loss: 615.69427\n",
      "Current_batch_loss: 441.14719\n",
      "Current_batch_loss: 594.23694\n",
      "Current_batch_loss: 602.75702\n",
      "Current_batch_loss: 521.90302\n",
      "Current_batch_loss: 601.56219\n",
      "Current_batch_loss: 504.20813\n",
      "Current_batch_loss: 757.06146\n",
      "Current_batch_loss: 588.53149\n",
      "Current_batch_loss: 597.54449\n",
      "Current_batch_loss: 569.93469\n",
      "Current_batch_loss: 590.45807\n",
      "Current_batch_loss: 615.45435\n",
      "Current_batch_loss: 546.29919\n",
      "Current_batch_loss: 568.84943\n",
      "Current_batch_loss: 464.70197\n",
      "Current_batch_loss: 629.51068\n",
      "Current_batch_loss: 460.54831\n",
      "Current_batch_loss: 538.45605\n",
      "Current_batch_loss: 548.13464\n",
      "Current_batch_loss: 487.45312\n",
      "Current_batch_loss: 579.72699\n",
      "Current_batch_loss: 535.974\n",
      "Current_batch_loss: 576.67816\n",
      "Current_batch_loss: 367.00345\n",
      "Current_batch_loss: 472.07706\n",
      "Current_batch_loss: 428.59302\n",
      "Current_batch_loss: 511.63248\n",
      "Current_batch_loss: 488.35583\n",
      "Current_batch_loss: 578.66083\n",
      "Current_batch_loss: 504.61337\n",
      "Current_batch_loss: 481.74924\n",
      "Current_batch_loss: 534.62384\n",
      "Current_batch_loss: 617.75397\n",
      "Current_batch_loss: 323.93008\n",
      "Current_batch_loss: 408.71851\n",
      "Current_batch_loss: 593.56891\n",
      "Current_batch_loss: 461.64801\n",
      "Current_batch_loss: 473.19598\n",
      "Current_batch_loss: 446.47025\n",
      "Current_batch_loss: 556.0686\n",
      "Current_batch_loss: 619.49298\n",
      "Current_batch_loss: 488.36484\n",
      "Current_batch_loss: 499.08011\n",
      "Current_batch_loss: 519.75635\n",
      "Current_batch_loss: 439.51471\n",
      "Current_batch_loss: 606.56775\n",
      "Current_batch_loss: 527.53912\n",
      "Current_batch_loss: 390.24048\n",
      "Current_batch_loss: 446.2171\n",
      "Current_batch_loss: 534.78967\n",
      "Current_batch_loss: 562.41479\n",
      "Current_batch_loss: 413.44757\n",
      "Current_batch_loss: 343.79111\n",
      "Current_batch_loss: 401.95334\n",
      "Current_batch_loss: 426.30518\n",
      "Current_batch_loss: 659.60339\n",
      "Current_batch_loss: 503.25403\n",
      "Current_batch_loss: 525.49701\n",
      "Current_batch_loss: 352.89993\n",
      "Current_batch_loss: 576.92047\n",
      "Current_batch_loss: 600.66705\n",
      "Current_batch_loss: 403.4899\n",
      "Current_batch_loss: 484.01566\n",
      "Current_batch_loss: 470.79971\n",
      "Current_batch_loss: 435.78735\n",
      "Current_batch_loss: 557.73523\n",
      "Current_batch_loss: 461.77606\n",
      "Current_batch_loss: 454.85745\n",
      "Current_batch_loss: 429.91324\n",
      "Current_batch_loss: 463.29739\n",
      "Current_batch_loss: 434.07974\n",
      "Current_batch_loss: 444.78946\n",
      "Current_batch_loss: 480.66071\n",
      "Current_batch_loss: 488.5596\n",
      "Current_batch_loss: 540.18854\n",
      "Current_batch_loss: 488.36127\n",
      "Current_batch_loss: 546.54248\n",
      "Current_batch_loss: 365.38644\n",
      "Current_batch_loss: 538.50696\n",
      "Current_batch_loss: 440.75934\n",
      "Current_batch_loss: 494.08887\n",
      "Current_batch_loss: 455.62708\n",
      "Current_batch_loss: 383.44495\n",
      "Current_batch_loss: 527.10968\n",
      "Current_batch_loss: 435.2912\n",
      "Current_batch_loss: 363.86664\n",
      "Current_batch_loss: 519.83698\n",
      "Current_batch_loss: 525.29114\n",
      "Current_batch_loss: 358.04321\n",
      "Current_batch_loss: 431.37943\n",
      "Current_batch_loss: 452.3707\n",
      "Current_batch_loss: 626.19244\n",
      "Current_batch_loss: 497.0213\n",
      "Current_batch_loss: 536.5838\n",
      "Current_batch_loss: 588.99451\n",
      "Current_batch_loss: 609.18781\n",
      "Current_batch_loss: 550.36377\n",
      "Current_batch_loss: 552.56439\n",
      "Current_batch_loss: 384.58667\n",
      "Current_batch_loss: 446.02951\n",
      "Current_batch_loss: 441.065\n",
      "Current_batch_loss: 336.18768\n",
      "Current_batch_loss: 415.09613\n",
      "Current_batch_loss: 484.05652\n",
      "Current_batch_loss: 493.17715\n",
      "Current_batch_loss: 461.31366\n",
      "Current_batch_loss: 478.61084\n",
      "Current_batch_loss: 408.80695\n",
      "Current_batch_loss: 398.20804\n",
      "Current_batch_loss: 510.49533\n",
      "Current_batch_loss: 402.13055\n",
      "Current_batch_loss: 495.89948\n",
      "Current_batch_loss: 440.42401\n",
      "Current_batch_loss: 510.14389\n",
      "Current_batch_loss: 558.59375\n",
      "Current_batch_loss: 373.384\n",
      "Current_batch_loss: 424.04239\n",
      "Current_batch_loss: 517.43347\n",
      "Current_batch_loss: 383.64667\n",
      "Current_batch_loss: 314.60129\n",
      "Current_batch_loss: 392.92111\n",
      "Current_batch_loss: 468.02777\n",
      "Current_batch_loss: 491.93909\n",
      "Current_batch_loss: 500.71744\n",
      "Current_batch_loss: 316.66666\n",
      "Current_batch_loss: 465.21045\n",
      "Current_batch_loss: 614.93274\n",
      "Current_batch_loss: 327.35257\n",
      "Current_batch_loss: 566.0224\n",
      "Current_batch_loss: 484.16858\n",
      "Current_batch_loss: 438.62119\n",
      "Current_batch_loss: 508.20599\n",
      "Current_batch_loss: 460.08426\n",
      "Current_batch_loss: 355.89243\n",
      "Current_batch_loss: 472.93948\n",
      "Current_batch_loss: 427.9772\n",
      "Current_batch_loss: 434.97998\n",
      "Current_batch_loss: 492.54327\n",
      "Current_batch_loss: 353.8468\n",
      "Current_batch_loss: 354.25424\n",
      "Current_batch_loss: 457.90054\n",
      "Current_batch_loss: 471.07239\n",
      "Current_batch_loss: 447.0116\n",
      "Current_batch_loss: 409.37592\n",
      "Current_batch_loss: 426.53204\n",
      "Current_batch_loss: 420.44873\n",
      "Current_batch_loss: 458.00195\n",
      "Current_batch_loss: 590.62836\n",
      "Current_batch_loss: 397.37302\n",
      "Current_batch_loss: 361.05679\n",
      "Current_batch_loss: 491.33865\n",
      "Current_batch_loss: 337.31656\n",
      "Current_batch_loss: 492.76086\n",
      "Current_batch_loss: 293.84137\n",
      "Current_batch_loss: 442.62918\n",
      "Current_batch_loss: 486.0589\n",
      "Current_batch_loss: 481.51309\n",
      "Current_batch_loss: 325.85339\n",
      "Current_batch_loss: 329.58176\n",
      "Current_batch_loss: 291.63525\n",
      "Current_batch_loss: 407.40405\n",
      "Current_batch_loss: 508.39166\n",
      "Current_batch_loss: 429.58197\n",
      "Current_batch_loss: 385.79388\n",
      "Current_batch_loss: 534.85919\n",
      "Current_batch_loss: 443.94427\n",
      "Current_batch_loss: 347.6279\n",
      "Current_batch_loss: 438.01498\n",
      "Current_batch_loss: 389.70514\n",
      "Current_batch_loss: 433.10342\n",
      "Current_batch_loss: 337.9639\n",
      "Current_batch_loss: 357.76709\n",
      "Current_batch_loss: 489.95035\n",
      "Current_batch_loss: 313.76318\n",
      "Current_batch_loss: 536.39642\n",
      "Current_batch_loss: 463.04523\n",
      "Current_batch_loss: 337.54956\n",
      "Current_batch_loss: 523.41919\n",
      "Current_batch_loss: 425.58014\n",
      "Current_batch_loss: 296.57513\n",
      "Current_batch_loss: 492.1817\n",
      "Current_batch_loss: 509.15018\n",
      "Current_batch_loss: 438.98672\n",
      "Current_batch_loss: 234.66951\n",
      "Current_batch_loss: 445.93848\n",
      "Current_batch_loss: 323.81104\n",
      "Current_batch_loss: 383.07315\n",
      "Current_batch_loss: 355.86978\n",
      "Current_batch_loss: 373.86905\n",
      "Current_batch_loss: 369.48929\n",
      "Current_batch_loss: 400.61411\n",
      "Current_batch_loss: 371.8511\n",
      "Current_batch_loss: 385.9249\n",
      "Current_batch_loss: 252.48268\n",
      "Current_batch_loss: 468.10577\n",
      "Current_batch_loss: 164.42987\n",
      "Current_batch_loss: 313.16882\n",
      "Current_batch_loss: 546.005\n",
      "Current_batch_loss: 392.97345\n",
      "Current_batch_loss: 366.5321\n",
      "Current_batch_loss: 311.88684\n",
      "Current_batch_loss: 293.69101\n",
      "Current_batch_loss: 191.64996\n",
      "Current_batch_loss: 442.38681\n",
      "Current_batch_loss: 417.91718\n",
      "Current_batch_loss: 428.12451\n",
      "Current_batch_loss: 489.31995\n",
      "Current_batch_loss: 306.76694\n",
      "Current_batch_loss: 307.28458\n",
      "Current_batch_loss: 341.10608\n",
      "Current_batch_loss: 361.54861\n",
      "Current_batch_loss: 254.57375\n",
      "Current_batch_loss: 411.97949\n",
      "Current_batch_loss: 373.08539\n",
      "Current_batch_loss: 392.271\n",
      "Current_batch_loss: 335.08865\n",
      "Current_batch_loss: 536.66895\n",
      "Current_batch_loss: 380.61017\n",
      "Current_batch_loss: 345.99698\n",
      "Current_batch_loss: 468.95233\n",
      "Current_batch_loss: 427.19247\n",
      "Current_batch_loss: 497.15009\n",
      "Current_batch_loss: 325.24384\n",
      "Current_batch_loss: 428.16504\n",
      "Current_batch_loss: 449.89548\n",
      "Current_batch_loss: 286.17709\n",
      "Current_batch_loss: 407.17816\n",
      "Current_batch_loss: 363.22821\n",
      "Current_batch_loss: 345.49924\n",
      "Current_batch_loss: 348.49219\n",
      "Current_batch_loss: 438.56805\n",
      "Current_batch_loss: 440.78577\n",
      "Current_batch_loss: 376.02142\n",
      "Current_batch_loss: 313.34991\n",
      "Current_batch_loss: 322.92169\n",
      "Current_batch_loss: 337.81851\n",
      "Current_batch_loss: 317.10672\n",
      "Current_batch_loss: 287.1723\n",
      "Current_batch_loss: 302.79178\n",
      "Current_batch_loss: 415.34113\n",
      "Current_batch_loss: 257.02356\n",
      "Current_batch_loss: 363.68591\n",
      "Current_batch_loss: 380.5275\n",
      "Current_batch_loss: 213.05154\n",
      "Current_batch_loss: 322.11661\n",
      "Current_batch_loss: 393.66675\n",
      "Current_batch_loss: 343.12473\n",
      "Current_batch_loss: 287.34238\n",
      "Current_batch_loss: 384.5383\n",
      "Current_batch_loss: 244.16794\n",
      "Current_batch_loss: 355.35553\n",
      "Current_batch_loss: 350.80014\n",
      "Current_batch_loss: 317.59576\n",
      "Current_batch_loss: 404.23233\n",
      "Current_batch_loss: 275.50793\n",
      "Current_batch_loss: 322.67371\n",
      "Current_batch_loss: 345.97134\n",
      "Current_batch_loss: 328.08752\n",
      "Current_batch_loss: 329.44107\n",
      "Current_batch_loss: 252.94397\n",
      "Current_batch_loss: 342.74399\n",
      "Current_batch_loss: 255.11217\n",
      "Current_batch_loss: 332.99182\n",
      "Current_batch_loss: 356.11642\n",
      "Current_batch_loss: 353.65219\n",
      "Current_batch_loss: 291.90128\n",
      "Current_batch_loss: 443.86026\n",
      "Current_batch_loss: 474.3475\n",
      "Current_batch_loss: 347.32687\n",
      "Current_batch_loss: 436.147\n",
      "Current_batch_loss: 392.4678\n",
      "Current_batch_loss: 209.49347\n",
      "Current_batch_loss: 297.74091\n",
      "Current_batch_loss: 431.56177\n",
      "Current_batch_loss: 340.1394\n",
      "Current_batch_loss: 408.99118\n",
      "Current_batch_loss: 307.5928\n",
      "Current_batch_loss: 402.18393\n",
      "Current_batch_loss: 295.57257\n",
      "Current_batch_loss: 312.7587\n",
      "Current_batch_loss: 463.4277\n",
      "Current_batch_loss: 301.72012\n",
      "Current_batch_loss: 257.71396\n",
      "Current_batch_loss: 360.33994\n",
      "Current_batch_loss: 282.38208\n",
      "Current_batch_loss: 273.52863\n",
      "Current_batch_loss: 273.83994\n",
      "Current_batch_loss: 298.03406\n",
      "Current_batch_loss: 471.12161\n",
      "Current_batch_loss: 294.77545\n",
      "Current_batch_loss: 497.62531\n",
      "Current_batch_loss: 357.64749\n",
      "Current_batch_loss: 302.90848\n",
      "Current_batch_loss: 298.68948\n",
      "Current_batch_loss: 257.61404\n",
      "Current_batch_loss: 198.26965\n",
      "Current_batch_loss: 305.54568\n",
      "Current_batch_loss: 261.77188\n",
      "Current_batch_loss: 339.99615\n",
      "Current_batch_loss: 276.29651\n",
      "Current_batch_loss: 256.93021\n",
      "Current_batch_loss: 339.32532\n",
      "Current_batch_loss: 369.28369\n",
      "Current_batch_loss: 290.43121\n",
      "Current_batch_loss: 327.54218\n",
      "Current_batch_loss: 346.15698\n",
      "Current_batch_loss: 258.12732\n",
      "Current_batch_loss: 390.50427\n",
      "Current_batch_loss: 373.26633\n",
      "Current_batch_loss: 327.05344\n",
      "Current_batch_loss: 401.03226\n",
      "Current_batch_loss: 289.37408\n",
      "Current_batch_loss: 274.38657\n",
      "Current_batch_loss: 222.07961\n",
      "Current_batch_loss: 259.18347\n",
      "Current_batch_loss: 334.73752\n",
      "Current_batch_loss: 368.07693\n",
      "Current_batch_loss: 194.90707\n",
      "Current_batch_loss: 345.67688\n",
      "Current_batch_loss: 361.36502\n",
      "Current_batch_loss: 310.69489\n",
      "Current_batch_loss: 436.21704\n",
      "Current_batch_loss: 300.73865\n",
      "Current_batch_loss: 325.90186\n",
      "Current_batch_loss: 229.98038\n",
      "Current_batch_loss: 256.40018\n",
      "Current_batch_loss: 162.97018\n",
      "Current_batch_loss: 269.78592\n",
      "Current_batch_loss: 214.83911\n",
      "Current_batch_loss: 357.50549\n",
      "Current_batch_loss: 350.21817\n",
      "Current_batch_loss: 242.33121\n",
      "Current_batch_loss: 312.96698\n",
      "Current_batch_loss: 333.59949\n",
      "Current_batch_loss: 378.38672\n",
      "Current_batch_loss: 281.93744\n",
      "Current_batch_loss: 260.44202\n",
      "Current_batch_loss: 392.64001\n",
      "Current_batch_loss: 211.22418\n",
      "Current_batch_loss: 271.53033\n",
      "Current_batch_loss: 361.82474\n",
      "Current_batch_loss: 384.50259\n",
      "Current_batch_loss: 258.92621\n",
      "Current_batch_loss: 312.12613\n",
      "Current_batch_loss: 316.93982\n",
      "Current_batch_loss: 377.4953\n",
      "Current_batch_loss: 361.65179\n",
      "Current_batch_loss: 382.99356\n",
      "Current_batch_loss: 304.07043\n",
      "Current_batch_loss: 275.31873\n",
      "Current_batch_loss: 270.10626\n",
      "Current_batch_loss: 293.53525\n",
      "Current_batch_loss: 254.92467\n",
      "Current_batch_loss: 298.85846\n",
      "Current_batch_loss: 252.58528\n",
      "Current_batch_loss: 283.16769\n",
      "Current_batch_loss: 360.44482\n",
      "Current_batch_loss: 334.01581\n",
      "Current_batch_loss: 352.88785\n",
      "Current_batch_loss: 264.12067\n",
      "Current_batch_loss: 274.80914\n",
      "Current_batch_loss: 304.12955\n",
      "Current_batch_loss: 270.36697\n",
      "Current_batch_loss: 316.55954\n",
      "Current_batch_loss: 233.51605\n",
      "Current_batch_loss: 283.34164\n",
      "Current_batch_loss: 380.34198\n",
      "Current_batch_loss: 263.43802\n",
      "Current_batch_loss: 222.66194\n",
      "Current_batch_loss: 463.4751\n",
      "Current_batch_loss: 329.19354\n",
      "Current_batch_loss: 248.22429\n",
      "Current_batch_loss: 282.30322\n",
      "Current_batch_loss: 250.03027\n",
      "Current_batch_loss: 204.0687\n",
      "Current_batch_loss: 318.24026\n",
      "Current_batch_loss: 288.16208\n",
      "Current_batch_loss: 185.01044\n",
      "Current_batch_loss: 308.18945\n",
      "Current_batch_loss: 257.72696\n",
      "Current_batch_loss: 283.94571\n",
      "Current_batch_loss: 276.7467\n",
      "Current_batch_loss: 243.72476\n",
      "Current_batch_loss: 257.84277\n",
      "Current_batch_loss: 245.60362\n",
      "Current_batch_loss: 347.87079\n",
      "Current_batch_loss: 274.67908\n",
      "Current_batch_loss: 336.68051\n",
      "Current_batch_loss: 261.73691\n",
      "Current_batch_loss: 263.68997\n",
      "Current_batch_loss: 103.85931\n",
      "Current_batch_loss: 365.10339\n",
      "Current_batch_loss: 300.02277\n",
      "Current_batch_loss: 173.63239\n",
      "Current_batch_loss: 232.60805\n",
      "Current_batch_loss: 259.51361\n",
      "Current_batch_loss: 360.91623\n",
      "Current_batch_loss: 316.11157\n",
      "Current_batch_loss: 173.33922\n",
      "Current_batch_loss: 199.12895\n",
      "Current_batch_loss: 215.66422\n",
      "Current_batch_loss: 428.06845\n",
      "Current_batch_loss: 246.84146\n",
      "Current_batch_loss: 206.74792\n",
      "Current_batch_loss: 215.53554\n",
      "Current_batch_loss: 155.08389\n",
      "Current_batch_loss: 238.589\n",
      "Current_batch_loss: 255.79137\n",
      "Current_batch_loss: 303.58588\n",
      "Current_batch_loss: 310.21698\n",
      "Current_batch_loss: 250.29553\n",
      "Current_batch_loss: 236.17557\n",
      "Current_batch_loss: 245.44632\n",
      "Current_batch_loss: 247.79045\n",
      "Current_batch_loss: 269.85977\n",
      "Current_batch_loss: 305.03137\n",
      "Current_batch_loss: 218.96329\n",
      "Current_batch_loss: 262.52429\n",
      "Current_batch_loss: 226.43237\n",
      "Current_batch_loss: 190.26344\n",
      "Current_batch_loss: 307.27444\n",
      "Current_batch_loss: 388.2099\n",
      "Current_batch_loss: 235.06023\n",
      "Current_batch_loss: 206.40598\n",
      "Current_batch_loss: 234.93054\n",
      "Current_batch_loss: 231.90717\n",
      "Current_batch_loss: 255.72731\n",
      "Current_batch_loss: 236.78398\n",
      "Current_batch_loss: 208.93732\n",
      "Current_batch_loss: 261.40335\n",
      "Current_batch_loss: 281.3663\n",
      "Current_batch_loss: 205.54735\n",
      "Current_batch_loss: 284.49448\n",
      "Current_batch_loss: 253.93909\n",
      "Current_batch_loss: 245.56656\n",
      "Current_batch_loss: 199.75873\n",
      "Current_batch_loss: 203.43024\n",
      "Current_batch_loss: 329.68829\n",
      "Current_batch_loss: 172.70996\n",
      "Current_batch_loss: 268.66779\n",
      "Current_batch_loss: 254.36481\n",
      "Current_batch_loss: 308.44214\n",
      "Current_batch_loss: 201.3642\n",
      "Current_batch_loss: 163.87242\n",
      "Current_batch_loss: 281.97003\n",
      "Current_batch_loss: 221.14154\n",
      "Current_batch_loss: 245.80579\n",
      "Current_batch_loss: 381.82516\n",
      "Current_batch_loss: 207.77448\n",
      "Current_batch_loss: 245.00655\n",
      "Current_batch_loss: 242.43225\n",
      "Current_batch_loss: 197.36153\n",
      "Current_batch_loss: 198.36115\n",
      "Current_batch_loss: 225.77632\n",
      "Current_batch_loss: 259.66174\n",
      "Current_batch_loss: 171.50012\n",
      "Current_batch_loss: 255.93181\n",
      "Current_batch_loss: 142.24879\n",
      "Current_batch_loss: 189.79424\n",
      "Current_batch_loss: 312.95395\n",
      "Current_batch_loss: 309.99515\n",
      "Current_batch_loss: 189.99153\n",
      "Current_batch_loss: 234.59949\n",
      "Current_batch_loss: 167.81386\n",
      "Current_batch_loss: 243.90031\n",
      "Current_batch_loss: 300.86423\n",
      "Current_batch_loss: 222.7661\n",
      "Current_batch_loss: 293.11194\n",
      "Current_batch_loss: 171.36266\n",
      "Current_batch_loss: 179.40224\n",
      "Current_batch_loss: 265.50363\n",
      "Current_batch_loss: 233.23067\n",
      "Current_batch_loss: 236.62212\n",
      "Current_batch_loss: 269.89841\n",
      "Current_batch_loss: 173.09828\n",
      "Current_batch_loss: 318.25848\n",
      "Current_batch_loss: 227.3143\n",
      "Current_batch_loss: 273.78201\n",
      "Current_batch_loss: 192.4507\n",
      "Current_batch_loss: 227.11086\n",
      "Current_batch_loss: 266.81265\n",
      "Current_batch_loss: 281.35855\n",
      "Current_batch_loss: 236.49956\n",
      "Current_batch_loss: 212.08974\n",
      "Current_batch_loss: 237.46291\n",
      "Current_batch_loss: 195.22261\n",
      "Current_batch_loss: 275.50854\n",
      "Current_batch_loss: 181.57852\n",
      "Current_batch_loss: 169.09045\n",
      "Current_batch_loss: 180.80984\n",
      "Current_batch_loss: 95.76078\n",
      "Current_batch_loss: 210.05507\n",
      "Current_batch_loss: 247.75928\n",
      "Current_batch_loss: 167.02318\n",
      "Current_batch_loss: 192.15125\n",
      "Current_batch_loss: 144.74496\n",
      "Current_batch_loss: 270.27847\n",
      "Current_batch_loss: 175.72337\n",
      "Current_batch_loss: 163.72839\n",
      "Current_batch_loss: 252.51144\n",
      "Current_batch_loss: 274.54984\n",
      "Current_batch_loss: 134.27997\n",
      "Current_batch_loss: 169.42468\n",
      "Current_batch_loss: 250.96098\n",
      "Current_batch_loss: 306.13818\n",
      "Current_batch_loss: 242.29724\n",
      "Current_batch_loss: 217.24944\n",
      "Current_batch_loss: 225.9682\n",
      "Current_batch_loss: 227.37846\n",
      "Current_batch_loss: 222.98254\n",
      "Current_batch_loss: 211.92587\n",
      "Current_batch_loss: 147.26361\n",
      "Current_batch_loss: 202.5732\n",
      "Current_batch_loss: 292.59134\n",
      "Current_batch_loss: 190.3\n",
      "Current_batch_loss: 263.27588\n",
      "Current_batch_loss: 141.5264\n",
      "Current_batch_loss: 232.40067\n",
      "Current_batch_loss: 204.18304\n",
      "Current_batch_loss: 184.78391\n",
      "Current_batch_loss: 318.40073\n",
      "Current_batch_loss: 197.23599\n",
      "Current_batch_loss: 166.57027\n",
      "Current_batch_loss: 177.86522\n",
      "Current_batch_loss: 197.43608\n",
      "Current_batch_loss: 184.05838\n",
      "Current_batch_loss: 167.88121\n",
      "Current_batch_loss: 225.1503\n",
      "Current_batch_loss: 129.22638\n",
      "Current_batch_loss: 256.6098\n",
      "Current_batch_loss: 150.80887\n",
      "Current_batch_loss: 238.90759\n",
      "Current_batch_loss: 169.6835\n",
      "Current_batch_loss: 178.20876\n",
      "Current_batch_loss: 193.86873\n",
      "Current_batch_loss: 181.13177\n",
      "Current_batch_loss: 232.80353\n",
      "Current_batch_loss: 194.01968\n",
      "Current_batch_loss: 212.80017\n",
      "Current_batch_loss: 159.37018\n",
      "Current_batch_loss: 197.89705\n",
      "Current_batch_loss: 193.92459\n",
      "Current_batch_loss: 196.34593\n",
      "Current_batch_loss: 188.53174\n",
      "Current_batch_loss: 145.25241\n",
      "Current_batch_loss: 140.04826\n",
      "Current_batch_loss: 194.07211\n",
      "Current_batch_loss: 207.11526\n",
      "Current_batch_loss: 209.79416\n",
      "Current_batch_loss: 193.86153\n",
      "Current_batch_loss: 156.69734\n",
      "Current_batch_loss: 204.54008\n",
      "Current_batch_loss: 156.12842\n",
      "Current_batch_loss: 179.91414\n",
      "Current_batch_loss: 112.20647\n",
      "Current_batch_loss: 141.43291\n",
      "Current_batch_loss: 202.21298\n",
      "Current_batch_loss: 227.46809\n",
      "Current_batch_loss: 130.96925\n",
      "Current_batch_loss: 275.14264\n",
      "Current_batch_loss: 190.83542\n",
      "Current_batch_loss: 122.56088\n",
      "Current_batch_loss: 192.95569\n",
      "Current_batch_loss: 185.20123\n",
      "Current_batch_loss: 186.67458\n",
      "Current_batch_loss: 149.35959\n",
      "Current_batch_loss: 177.85448\n",
      "Current_batch_loss: 177.23248\n",
      "Current_batch_loss: 119.66389\n",
      "Current_batch_loss: 131.44426\n",
      "Current_batch_loss: 150.33832\n",
      "Current_batch_loss: 198.57292\n",
      "Current_batch_loss: 172.57713\n",
      "Current_batch_loss: 115.27724\n",
      "Current_batch_loss: 168.64362\n",
      "Current_batch_loss: 230.68158\n",
      "Current_batch_loss: 126.92909\n",
      "Current_batch_loss: 137.20107\n",
      "Current_batch_loss: 144.33287\n",
      "Current_batch_loss: 167.47884\n",
      "Current_batch_loss: 131.85854\n",
      "Current_batch_loss: 165.76445\n",
      "Current_batch_loss: 128.75293\n",
      "Current_batch_loss: 139.50044\n",
      "Current_batch_loss: 175.18744\n",
      "Current_batch_loss: 154.75693\n",
      "Current_batch_loss: 208.15164\n",
      "Current_batch_loss: 196.09819\n",
      "Current_batch_loss: 178.90974\n",
      "Current_batch_loss: 101.741\n",
      "Current_batch_loss: 133.30841\n",
      "Current_batch_loss: 173.41075\n",
      "Current_batch_loss: 228.24117\n",
      "Current_batch_loss: 82.45901\n",
      "Current_batch_loss: 146.08264\n",
      "Current_batch_loss: 143.30775\n",
      "Current_batch_loss: 142.50211\n",
      "Current_batch_loss: 254.51163\n",
      "Current_batch_loss: 250.8231\n",
      "Current_batch_loss: 111.78661\n",
      "Current_batch_loss: 180.90141\n",
      "Current_batch_loss: 145.73984\n",
      "Current_batch_loss: 195.68649\n",
      "Current_batch_loss: 135.56564\n",
      "Current_batch_loss: 175.88115\n",
      "Current_batch_loss: 97.32996\n",
      "Current_batch_loss: 131.92577\n",
      "Current_batch_loss: 161.54123\n",
      "Current_batch_loss: 222.76048\n",
      "Current_batch_loss: 161.24092\n",
      "Current_batch_loss: 236.40208\n",
      "Current_batch_loss: 123.3711\n",
      "Current_batch_loss: 101.52474\n",
      "Current_batch_loss: 93.54046\n",
      "Current_batch_loss: 153.60405\n",
      "Current_batch_loss: 106.97797\n",
      "Current_batch_loss: 74.31636\n",
      "Current_batch_loss: 169.37204\n",
      "Current_batch_loss: 187.04868\n",
      "Current_batch_loss: 158.14224\n",
      "Current_batch_loss: 178.63983\n",
      "Current_batch_loss: 140.27853\n",
      "Current_batch_loss: 91.19289\n",
      "Current_batch_loss: 140.51128\n",
      "Current_batch_loss: 115.57942\n",
      "Current_batch_loss: 175.36351\n",
      "Current_batch_loss: 185.09537\n",
      "Current_batch_loss: 121.24854\n",
      "Current_batch_loss: 204.86115\n",
      "Current_batch_loss: 196.92825\n",
      "Current_batch_loss: 108.21314\n",
      "Current_batch_loss: 144.24774\n",
      "Current_batch_loss: 218.43063\n",
      "Current_batch_loss: 98.01721\n",
      "Current_batch_loss: 139.50015\n",
      "Current_batch_loss: 151.02466\n",
      "Current_batch_loss: 119.29845\n",
      "Current_batch_loss: 193.99545\n",
      "Current_batch_loss: 155.10806\n",
      "Current_batch_loss: 138.08527\n",
      "Current_batch_loss: 134.96887\n",
      "Current_batch_loss: 146.72194\n",
      "Current_batch_loss: 162.70844\n",
      "Current_batch_loss: 185.0209\n",
      "Current_batch_loss: 147.46596\n",
      "Current_batch_loss: 60.8044\n",
      "Current_batch_loss: 146.81078\n",
      "Current_batch_loss: 150.16473\n",
      "Current_batch_loss: 165.40562\n",
      "Current_batch_loss: 154.077\n",
      "Current_batch_loss: 140.89212\n",
      "Current_batch_loss: 70.13236\n",
      "Current_batch_loss: 167.98688\n",
      "Current_batch_loss: 102.76904\n",
      "Current_batch_loss: 163.2737\n",
      "Current_batch_loss: 101.67619\n",
      "Current_batch_loss: 128.66096\n",
      "Current_batch_loss: 81.9468\n",
      "Current_batch_loss: 119.45117\n",
      "Current_batch_loss: 154.73453\n",
      "Current_batch_loss: 104.2557\n",
      "Current_batch_loss: 139.74976\n",
      "Current_batch_loss: 150.76334\n",
      "Current_batch_loss: 124.00076\n",
      "Current_batch_loss: 181.89449\n",
      "Current_batch_loss: 103.88042\n",
      "Current_batch_loss: 97.87966\n",
      "Current_batch_loss: 156.96603\n",
      "Current_batch_loss: 124.31256\n",
      "Current_batch_loss: 172.30356\n",
      "Current_batch_loss: 121.75174\n",
      "Current_batch_loss: 99.21794\n",
      "Current_batch_loss: 158.60649\n",
      "Current_batch_loss: 102.7837\n",
      "Current_batch_loss: 85.8345\n",
      "Current_batch_loss: 140.37006\n",
      "Current_batch_loss: 161.83211\n",
      "Current_batch_loss: 141.0434\n",
      "Current_batch_loss: 35.97618\n",
      "Current_batch_loss: 159.02682\n",
      "Current_batch_loss: 143.40866\n",
      "Current_batch_loss: 146.19261\n",
      "Current_batch_loss: 104.04787\n",
      "Current_batch_loss: 101.4436\n",
      "Current_batch_loss: 85.53709\n",
      "Current_batch_loss: 125.93237\n",
      "Current_batch_loss: 111.2547\n",
      "Current_batch_loss: 101.39159\n",
      "Current_batch_loss: 114.76276\n",
      "Current_batch_loss: 169.50275\n",
      "Current_batch_loss: 109.06167\n",
      "Current_batch_loss: 126.26776\n",
      "Current_batch_loss: 124.43799\n",
      "Current_batch_loss: 131.384\n",
      "Current_batch_loss: 174.36478\n",
      "Current_batch_loss: 81.69247\n",
      "Current_batch_loss: 188.20798\n",
      "Current_batch_loss: 106.39757\n",
      "Current_batch_loss: 153.62361\n",
      "Current_batch_loss: 108.66383\n",
      "Current_batch_loss: 53.78906\n",
      "Current_batch_loss: 110.7661\n",
      "Current_batch_loss: 153.94341\n",
      "Current_batch_loss: 100.73649\n",
      "Current_batch_loss: 105.30033\n",
      "Current_batch_loss: 132.60269\n",
      "Current_batch_loss: 140.67046\n",
      "Current_batch_loss: 120.25474\n",
      "Current_batch_loss: 154.70897\n",
      "Current_batch_loss: 108.1732\n",
      "Current_batch_loss: 111.47058\n",
      "Current_batch_loss: 155.9696\n",
      "Current_batch_loss: 161.96808\n",
      "Current_batch_loss: 74.424\n",
      "Current_batch_loss: 147.4874\n",
      "Current_batch_loss: 126.2813\n",
      "Current_batch_loss: 149.66716\n",
      "Current_batch_loss: 166.37982\n",
      "Current_batch_loss: 157.68422\n",
      "Current_batch_loss: 85.01566\n",
      "Current_batch_loss: 75.68941\n",
      "Current_batch_loss: 162.40091\n",
      "Current_batch_loss: 155.87737\n",
      "Current_batch_loss: 166.53513\n",
      "Current_batch_loss: 104.79038\n",
      "Current_batch_loss: 123.87161\n",
      "Current_batch_loss: 72.60529\n",
      "Current_batch_loss: 145.86876\n",
      "Current_batch_loss: 61.59168\n",
      "Current_batch_loss: 156.79413\n",
      "Current_batch_loss: 77.52161\n",
      "Current_batch_loss: 138.96001\n",
      "Current_batch_loss: 135.75923\n",
      "Current_batch_loss: 103.77136\n",
      "Current_batch_loss: 106.84686\n",
      "Current_batch_loss: 126.33506\n",
      "Current_batch_loss: 69.20212\n",
      "Current_batch_loss: 106.38802\n",
      "Current_batch_loss: 99.85666\n",
      "Current_batch_loss: 94.68182\n",
      "Current_batch_loss: 97.27312\n",
      "Current_batch_loss: 66.56656\n",
      "Current_batch_loss: 102.47089\n",
      "Current_batch_loss: 92.2502\n",
      "Current_batch_loss: 164.459\n",
      "Current_batch_loss: 130.59483\n",
      "Current_batch_loss: 161.62193\n",
      "Current_batch_loss: 59.36266\n",
      "Current_batch_loss: 165.28706\n",
      "Current_batch_loss: 103.32314\n",
      "Current_batch_loss: 149.21642\n",
      "Current_batch_loss: 41.1601\n",
      "Current_batch_loss: 88.13978\n",
      "Current_batch_loss: 80.13264\n",
      "Current_batch_loss: 81.95404\n",
      "Current_batch_loss: 98.13268\n",
      "Current_batch_loss: 101.76418\n",
      "Current_batch_loss: 63.4385\n",
      "Current_batch_loss: 119.50352\n",
      "Current_batch_loss: 81.61684\n",
      "Current_batch_loss: 133.06075\n",
      "Current_batch_loss: 114.96143\n",
      "Current_batch_loss: 101.111\n",
      "Current_batch_loss: 84.30514\n",
      "Current_batch_loss: 111.23265\n",
      "Current_batch_loss: 86.95329\n",
      "Current_batch_loss: 87.34816\n",
      "Current_batch_loss: 96.1151\n",
      "Current_batch_loss: 99.97863\n",
      "Current_batch_loss: 39.58217\n",
      "Current_batch_loss: 130.67825\n",
      "Current_batch_loss: 49.46926\n",
      "Current_batch_loss: 89.96203\n",
      "Current_batch_loss: 59.43045\n",
      "Current_batch_loss: 91.78297\n",
      "Current_batch_loss: 138.71579\n",
      "Current_batch_loss: 44.58686\n",
      "Current_batch_loss: 142.11147\n",
      "Current_batch_loss: 75.07881\n",
      "Current_batch_loss: 53.21747\n",
      "Current_batch_loss: 138.71724\n",
      "Current_batch_loss: 108.87128\n",
      "Current_batch_loss: 81.60606\n",
      "Current_batch_loss: 96.03777\n",
      "Current_batch_loss: 95.50276\n",
      "Current_batch_loss: 84.43753\n",
      "Current_batch_loss: 147.12297\n",
      "Current_batch_loss: 171.41805\n",
      "Current_batch_loss: 65.09988\n",
      "Current_batch_loss: 78.27414\n",
      "Current_batch_loss: 82.07314\n",
      "Current_batch_loss: 146.72781\n",
      "Current_batch_loss: 61.01608\n",
      "Current_batch_loss: 109.70851\n",
      "Current_batch_loss: 79.90005\n",
      "Current_batch_loss: 84.50467\n",
      "Current_batch_loss: 101.94213\n",
      "Current_batch_loss: 159.91208\n",
      "Current_batch_loss: 71.20567\n",
      "Current_batch_loss: 166.29008\n",
      "Current_batch_loss: 100.06706\n",
      "Current_batch_loss: 101.0916\n",
      "Current_batch_loss: 96.89649\n",
      "Current_batch_loss: 42.86824\n",
      "Current_batch_loss: 174.96216\n",
      "Current_batch_loss: 144.41982\n",
      "Current_batch_loss: 158.43488\n",
      "Current_batch_loss: 89.1553\n",
      "Current_batch_loss: 27.17549\n",
      "Current_batch_loss: 75.03433\n",
      "Current_batch_loss: 100.82986\n",
      "Current_batch_loss: 73.53417\n",
      "Current_batch_loss: 118.02621\n",
      "Current_batch_loss: 40.74681\n",
      "Current_batch_loss: 87.46523\n",
      "Current_batch_loss: 108.76685\n",
      "Current_batch_loss: 62.241\n",
      "Current_batch_loss: 25.53021\n",
      "Current_batch_loss: 40.44146\n",
      "Current_batch_loss: 121.29845\n",
      "Current_batch_loss: 68.79688\n",
      "Current_batch_loss: 104.33836\n",
      "Current_batch_loss: 106.40321\n",
      "Current_batch_loss: 76.15571\n",
      "Current_batch_loss: 99.49487\n",
      "Current_batch_loss: 51.74858\n",
      "Current_batch_loss: 34.0103\n",
      "Current_batch_loss: 70.6543\n",
      "Current_batch_loss: 88.88926\n",
      "Current_batch_loss: 49.55753\n",
      "Current_batch_loss: 91.32147\n",
      "Current_batch_loss: 81.10898\n",
      "Current_batch_loss: 73.92765\n",
      "Current_batch_loss: 31.78427\n",
      "Current_batch_loss: 65.64765\n",
      "Current_batch_loss: 93.84326\n",
      "Current_batch_loss: 106.69653\n",
      "Current_batch_loss: 82.36469\n",
      "Current_batch_loss: 109.98262\n",
      "Current_batch_loss: 52.59295\n",
      "Current_batch_loss: 80.20536\n",
      "Current_batch_loss: 44.85733\n",
      "Current_batch_loss: 87.10136\n",
      "Current_batch_loss: 177.37648\n",
      "Current_batch_loss: 45.40834\n",
      "Current_batch_loss: 91.36131\n",
      "Current_batch_loss: 46.55062\n",
      "Current_batch_loss: 56.44458\n",
      "Current_batch_loss: 58.3107\n",
      "Current_batch_loss: 25.80886\n",
      "Current_batch_loss: 58.55636\n",
      "Current_batch_loss: 74.16029\n",
      "Current_batch_loss: 91.73038\n",
      "Current_batch_loss: 52.37861\n",
      "Current_batch_loss: 123.23773\n",
      "Current_batch_loss: 35.79675\n",
      "Current_batch_loss: 82.69683\n",
      "Current_batch_loss: 51.59951\n",
      "Current_batch_loss: 78.59483\n",
      "Current_batch_loss: 76.74877\n",
      "Current_batch_loss: 50.44556\n",
      "Current_batch_loss: 95.65862\n",
      "Current_batch_loss: 62.49634\n",
      "Current_batch_loss: 68.16972\n",
      "Current_batch_loss: 103.23322\n",
      "Current_batch_loss: 37.40051\n",
      "Current_batch_loss: 48.84502\n",
      "Current_batch_loss: 31.28597\n",
      "Current_batch_loss: 90.90445\n",
      "Current_batch_loss: 147.86514\n",
      "Current_batch_loss: 60.69647\n",
      "Current_batch_loss: 46.64279\n",
      "Current_batch_loss: 33.52818\n",
      "Current_batch_loss: 29.86427\n",
      "Current_batch_loss: 31.75026\n",
      "Current_batch_loss: 69.14935\n",
      "Current_batch_loss: 65.16835\n",
      "Current_batch_loss: 96.54973\n",
      "Current_batch_loss: 262.60846\n",
      "Current_batch_loss: 27.81001\n",
      "Current_batch_loss: 48.88843\n",
      "Current_batch_loss: 42.69323\n",
      "Current_batch_loss: 66.21277\n",
      "Current_batch_loss: 75.14902\n",
      "Current_batch_loss: 58.42815\n",
      "Current_batch_loss: 36.23824\n",
      "Current_batch_loss: 70.12448\n",
      "Current_batch_loss: 61.69313\n",
      "Current_batch_loss: 81.18192\n",
      "Current_batch_loss: 72.46179\n",
      "Current_batch_loss: 55.74589\n",
      "Current_batch_loss: 85.17656\n",
      "Current_batch_loss: 28.24193\n",
      "Current_batch_loss: 104.54852\n",
      "Current_batch_loss: 62.47611\n",
      "Current_batch_loss: 53.49695\n",
      "Current_batch_loss: 137.70183\n",
      "Current_batch_loss: 77.44177\n",
      "Current_batch_loss: 75.3907\n",
      "Current_batch_loss: 62.52678\n",
      "Current_batch_loss: 83.95406\n",
      "Current_batch_loss: 128.14732\n",
      "Current_batch_loss: 78.95788\n",
      "Current_batch_loss: 63.6889\n",
      "Current_batch_loss: 33.98268\n",
      "Current_batch_loss: 95.10857\n",
      "Current_batch_loss: 28.84566\n",
      "Current_batch_loss: 62.44718\n",
      "Current_batch_loss: 130.61142\n",
      "Current_batch_loss: 19.10469\n",
      "Current_batch_loss: 83.95841\n",
      "Current_batch_loss: 38.87151\n",
      "Current_batch_loss: 86.50099\n",
      "Current_batch_loss: 38.44026\n",
      "Current_batch_loss: 47.21772\n",
      "Current_batch_loss: 46.8862\n",
      "Current_batch_loss: 64.13681\n",
      "Current_batch_loss: 52.04943\n",
      "Current_batch_loss: 43.83788\n",
      "Current_batch_loss: 71.9505\n",
      "Current_batch_loss: 29.39853\n",
      "Current_batch_loss: 71.63805\n",
      "Current_batch_loss: 88.69933\n",
      "Current_batch_loss: 70.85268\n",
      "Current_batch_loss: 46.37109\n",
      "Current_batch_loss: 112.262\n",
      "Current_batch_loss: 34.63629\n",
      "Current_batch_loss: 60.54482\n",
      "Current_batch_loss: 43.49492\n",
      "Current_batch_loss: 37.09332\n",
      "Current_batch_loss: 52.02942\n",
      "Current_batch_loss: 63.12294\n",
      "Current_batch_loss: 50.63697\n",
      "Current_batch_loss: 69.54369\n",
      "Current_batch_loss: 32.86095\n",
      "Current_batch_loss: 27.2325\n",
      "Current_batch_loss: 65.37087\n",
      "Current_batch_loss: 23.0651\n",
      "Current_batch_loss: 75.02585\n",
      "Current_batch_loss: 37.49615\n",
      "Current_batch_loss: 43.59502\n",
      "Current_batch_loss: 42.28757\n",
      "Current_batch_loss: 50.7548\n",
      "Current_batch_loss: 79.09241\n",
      "Current_batch_loss: 78.35197\n",
      "Current_batch_loss: 18.26934\n",
      "Current_batch_loss: 31.09002\n",
      "Current_batch_loss: 71.79862\n",
      "Current_batch_loss: 8.58848\n",
      "Current_batch_loss: 36.63641\n",
      "Current_batch_loss: 26.73319\n",
      "Current_batch_loss: 67.49615\n",
      "Current_batch_loss: 38.93869\n",
      "Current_batch_loss: 51.3965\n",
      "Current_batch_loss: 90.56891\n",
      "Current_batch_loss: 49.76982\n",
      "Current_batch_loss: 60.41592\n",
      "Current_batch_loss: 46.20694\n",
      "Current_batch_loss: 37.2854\n",
      "Current_batch_loss: 59.799\n",
      "Current_batch_loss: 25.03772\n",
      "Current_batch_loss: 56.99401\n",
      "Current_batch_loss: 51.56358\n",
      "Current_batch_loss: 23.52962\n",
      "Current_batch_loss: 38.45212\n",
      "Current_batch_loss: 50.50829\n",
      "Current_batch_loss: 16.12861\n",
      "Current_batch_loss: 29.23913\n",
      "Current_batch_loss: 47.18299\n",
      "Current_batch_loss: 34.86098\n",
      "Current_batch_loss: 103.29169\n",
      "Current_batch_loss: 16.40425\n",
      "Current_batch_loss: 85.80069\n",
      "Current_batch_loss: 46.42477\n",
      "Current_batch_loss: 66.95663\n",
      "Current_batch_loss: 32.2662\n",
      "Current_batch_loss: 54.92258\n",
      "Current_batch_loss: 19.78743\n",
      "Current_batch_loss: 28.4635\n",
      "Current_batch_loss: 54.42106\n",
      "Current_batch_loss: 28.27256\n",
      "Current_batch_loss: 44.23001\n",
      "Current_batch_loss: 18.34707\n",
      "Current_batch_loss: 49.27858\n",
      "Current_batch_loss: 34.24429\n",
      "Current_batch_loss: 23.76423\n",
      "Current_batch_loss: 47.76122\n",
      "Current_batch_loss: 57.93629\n",
      "Current_batch_loss: 79.09881\n",
      "Current_batch_loss: 36.74289\n",
      "Current_batch_loss: 47.16547\n",
      "Current_batch_loss: 16.22324\n",
      "Current_batch_loss: 19.60697\n",
      "Current_batch_loss: 51.51416\n",
      "Current_batch_loss: 34.21239\n",
      "Current_batch_loss: 56.56309\n",
      "Current_batch_loss: 48.48923\n",
      "Current_batch_loss: 32.16422\n",
      "Current_batch_loss: 70.67448\n",
      "Current_batch_loss: 64.47639\n",
      "Current_batch_loss: 32.91191\n",
      "Current_batch_loss: 27.18653\n",
      "Current_batch_loss: 40.40698\n",
      "Current_batch_loss: 53.00775\n",
      "Current_batch_loss: 32.57038\n",
      "Current_batch_loss: 40.36583\n",
      "Current_batch_loss: 35.8299\n",
      "Current_batch_loss: 65.26781\n",
      "Current_batch_loss: 30.34077\n",
      "Current_batch_loss: 39.47758\n",
      "Current_batch_loss: 36.178\n",
      "Current_batch_loss: 40.06871\n",
      "Current_batch_loss: 22.52282\n",
      "Current_batch_loss: 63.85\n",
      "Current_batch_loss: 53.34805\n",
      "Current_batch_loss: 23.9936\n",
      "Current_batch_loss: 8.39911\n",
      "Current_batch_loss: 38.1176\n",
      "Current_batch_loss: 15.06178\n",
      "Current_batch_loss: 20.34146\n",
      "Current_batch_loss: 26.20495\n",
      "Current_batch_loss: 56.62789\n",
      "Current_batch_loss: 45.5075\n",
      "Current_batch_loss: 25.36365\n",
      "Current_batch_loss: 10.50386\n",
      "Current_batch_loss: 65.26746\n",
      "Current_batch_loss: 7.8709\n",
      "Current_batch_loss: 29.95737\n",
      "Current_batch_loss: 49.0614\n",
      "Current_batch_loss: 28.19604\n",
      "Current_batch_loss: 27.79147\n",
      "Current_batch_loss: 23.6952\n",
      "Current_batch_loss: 47.04004\n",
      "Current_batch_loss: 11.26561\n",
      "Current_batch_loss: 81.5793\n",
      "Current_batch_loss: 42.6268\n",
      "Current_batch_loss: 0.01268\n",
      "Current_batch_loss: 17.9503\n",
      "Current_batch_loss: 44.26419\n",
      "Current_batch_loss: 31.32655\n",
      "Current_batch_loss: 10.83737\n",
      "Current_batch_loss: 9.94617\n",
      "Current_batch_loss: 11.47969\n",
      "Current_batch_loss: 68.32938\n",
      "Current_batch_loss: 40.06806\n",
      "Current_batch_loss: 26.92173\n",
      "Current_batch_loss: 20.03706\n",
      "Current_batch_loss: 30.72632\n",
      "Current_batch_loss: 22.52154\n",
      "Current_batch_loss: 33.43951\n",
      "Current_batch_loss: 33.88876\n",
      "Current_batch_loss: 15.76033\n",
      "Current_batch_loss: 89.46658\n",
      "Current_batch_loss: 61.38659\n",
      "Current_batch_loss: 35.41283\n",
      "Current_batch_loss: 21.7656\n",
      "Current_batch_loss: 0.01209\n",
      "Current_batch_loss: 49.26461\n",
      "Current_batch_loss: 43.80018\n",
      "Current_batch_loss: 54.30326\n",
      "Current_batch_loss: 0.01311\n",
      "Current_batch_loss: 5.96648\n",
      "Current_batch_loss: 47.75335\n",
      "Current_batch_loss: 19.69162\n",
      "Current_batch_loss: 0.01249\n",
      "Current_batch_loss: 52.08643\n",
      "Current_batch_loss: 72.35792\n",
      "Current_batch_loss: 45.1413\n",
      "Current_batch_loss: 33.5154\n",
      "Current_batch_loss: 29.59845\n",
      "Current_batch_loss: 76.62504\n",
      "Current_batch_loss: 18.86292\n",
      "Current_batch_loss: 21.22476\n",
      "Current_batch_loss: 21.38144\n",
      "Current_batch_loss: 32.25116\n",
      "Current_batch_loss: 11.17729\n",
      "Current_batch_loss: 50.87983\n",
      "Current_batch_loss: 26.80686\n",
      "Current_batch_loss: 31.3547\n",
      "Current_batch_loss: 7.71009\n",
      "Current_batch_loss: 7.70212\n",
      "Current_batch_loss: 17.8715\n",
      "Current_batch_loss: 81.72767\n",
      "Current_batch_loss: 20.02901\n",
      "Current_batch_loss: 14.91294\n",
      "Current_batch_loss: 12.20982\n",
      "Current_batch_loss: 30.35055\n",
      "Current_batch_loss: 37.2349\n",
      "Current_batch_loss: 33.38008\n",
      "Current_batch_loss: 12.22783\n",
      "Current_batch_loss: 43.33424\n",
      "Current_batch_loss: 63.72528\n",
      "Current_batch_loss: 23.53891\n",
      "Current_batch_loss: 17.66956\n",
      "Current_batch_loss: 44.0042\n",
      "Current_batch_loss: 32.0643\n",
      "Current_batch_loss: 6.96818\n",
      "Current_batch_loss: 58.88897\n",
      "Current_batch_loss: 22.10208\n",
      "Current_batch_loss: 35.6366\n",
      "Current_batch_loss: 30.3104\n",
      "Current_batch_loss: 51.82169\n",
      "Current_batch_loss: 36.15393\n",
      "Current_batch_loss: 47.97623\n",
      "Current_batch_loss: 8.2517\n",
      "Current_batch_loss: 0.013\n",
      "Current_batch_loss: 54.43028\n",
      "Current_batch_loss: 15.19997\n",
      "Current_batch_loss: 16.62551\n",
      "Current_batch_loss: 36.06488\n",
      "Current_batch_loss: 28.58294\n",
      "Current_batch_loss: 27.57866\n",
      "Current_batch_loss: 40.32269\n",
      "Current_batch_loss: 0.01338\n",
      "Current_batch_loss: 18.00588\n",
      "Current_batch_loss: 8.36858\n",
      "Current_batch_loss: 28.59819\n",
      "Current_batch_loss: 36.03464\n",
      "Current_batch_loss: 17.27985\n",
      "Current_batch_loss: 9.73772\n",
      "Current_batch_loss: 22.80094\n",
      "Current_batch_loss: 9.17073\n",
      "Current_batch_loss: 35.15792\n",
      "Current_batch_loss: 20.5208\n",
      "Current_batch_loss: 28.34781\n",
      "Current_batch_loss: 28.37497\n",
      "Current_batch_loss: 17.20795\n",
      "Current_batch_loss: 38.51158\n",
      "Current_batch_loss: 22.80693\n",
      "Current_batch_loss: 34.41012\n",
      "Current_batch_loss: 37.90946\n",
      "Current_batch_loss: 34.7392\n",
      "Current_batch_loss: 7.95291\n",
      "Current_batch_loss: 19.57225\n",
      "Current_batch_loss: 17.56407\n",
      "Current_batch_loss: 21.78115\n",
      "Current_batch_loss: 0.01316\n",
      "Current_batch_loss: 26.37451\n",
      "Current_batch_loss: 23.48545\n",
      "Current_batch_loss: 25.93384\n",
      "Current_batch_loss: 25.61226\n",
      "Current_batch_loss: 0.01687\n",
      "Current_batch_loss: 50.70925\n",
      "Current_batch_loss: 32.76052\n",
      "Current_batch_loss: 22.92831\n",
      "Current_batch_loss: 39.8337\n",
      "Current_batch_loss: 19.67297\n",
      "Current_batch_loss: 28.02457\n",
      "Current_batch_loss: 28.1772\n",
      "Current_batch_loss: 36.79826\n",
      "Current_batch_loss: 31.06724\n",
      "Current_batch_loss: 11.96324\n",
      "Current_batch_loss: 8.19756\n",
      "Current_batch_loss: 43.88708\n",
      "Current_batch_loss: 47.09758\n",
      "Current_batch_loss: 65.95306\n",
      "Current_batch_loss: 22.08945\n",
      "Current_batch_loss: 8.26446\n",
      "Current_batch_loss: 19.4605\n",
      "Current_batch_loss: 25.92424\n",
      "Current_batch_loss: 42.8996\n",
      "Current_batch_loss: 27.14219\n",
      "Current_batch_loss: 0.01198\n",
      "Current_batch_loss: 36.14882\n",
      "Current_batch_loss: 9.38721\n",
      "Current_batch_loss: 47.75175\n",
      "Current_batch_loss: 31.35501\n",
      "Current_batch_loss: 11.01827\n",
      "Current_batch_loss: 48.02475\n",
      "Current_batch_loss: 18.78448\n",
      "Current_batch_loss: 24.20196\n",
      "Current_batch_loss: 5.8887\n",
      "Current_batch_loss: 33.4222\n",
      "Current_batch_loss: 11.42802\n",
      "Current_batch_loss: 21.38926\n",
      "Current_batch_loss: 8.46548\n",
      "Current_batch_loss: 0.01722\n",
      "Current_batch_loss: 7.58505\n",
      "Current_batch_loss: 7.93511\n",
      "Current_batch_loss: 36.65237\n",
      "Current_batch_loss: 32.97027\n",
      "Current_batch_loss: 16.31911\n",
      "Current_batch_loss: 30.63524\n",
      "Current_batch_loss: 6.78427\n",
      "Current_batch_loss: 19.67145\n",
      "Current_batch_loss: 0.01338\n",
      "Current_batch_loss: 9.47393\n",
      "Current_batch_loss: 0.00933\n",
      "Current_batch_loss: 0.01332\n",
      "Current_batch_loss: 24.91079\n",
      "Current_batch_loss: 26.31314\n",
      "Current_batch_loss: 6.45192\n",
      "Current_batch_loss: 9.63596\n",
      "Current_batch_loss: 17.47148\n",
      "Current_batch_loss: 13.281\n",
      "Current_batch_loss: 10.87087\n",
      "Current_batch_loss: 9.48969\n",
      "Current_batch_loss: 14.20138\n",
      "Current_batch_loss: 9.07091\n",
      "Current_batch_loss: 22.42539\n",
      "Current_batch_loss: 6.689\n",
      "Current_batch_loss: 0.01338\n",
      "Current_batch_loss: 0.01283\n",
      "Current_batch_loss: 7.94358\n",
      "Current_batch_loss: 21.601\n",
      "Current_batch_loss: 25.69653\n",
      "Current_batch_loss: 12.86665\n",
      "Current_batch_loss: 24.0842\n",
      "Current_batch_loss: 19.20164\n",
      "Current_batch_loss: 7.16923\n",
      "Current_batch_loss: 19.09259\n",
      "Current_batch_loss: 49.44758\n",
      "Current_batch_loss: 8.52875\n",
      "Current_batch_loss: 35.26817\n",
      "Current_batch_loss: 0.01481\n",
      "Current_batch_loss: 0.01205\n",
      "Current_batch_loss: 0.0118\n",
      "Current_batch_loss: 21.90403\n",
      "Current_batch_loss: 9.5671\n",
      "Current_batch_loss: 44.56847\n",
      "Current_batch_loss: 31.50078\n",
      "Current_batch_loss: 0.01494\n",
      "Current_batch_loss: 30.56287\n",
      "Current_batch_loss: 26.28313\n",
      "Current_batch_loss: 10.74877\n",
      "Current_batch_loss: 22.28546\n",
      "Current_batch_loss: 35.19661\n",
      "Current_batch_loss: 24.4754\n",
      "Current_batch_loss: 25.0063\n",
      "Current_batch_loss: 54.27868\n",
      "Current_batch_loss: 6.45151\n",
      "Current_batch_loss: 0.0125\n",
      "Current_batch_loss: 60.80356\n",
      "Current_batch_loss: 12.71806\n",
      "Current_batch_loss: 17.02883\n",
      "Current_batch_loss: 8.08807\n",
      "Current_batch_loss: 10.81002\n",
      "Current_batch_loss: 14.36819\n",
      "Current_batch_loss: 24.72709\n",
      "Current_batch_loss: 0.0144\n",
      "Current_batch_loss: 39.69675\n",
      "Current_batch_loss: 17.01598\n",
      "Current_batch_loss: 9.38929\n",
      "Current_batch_loss: 12.82782\n",
      "Current_batch_loss: 7.96246\n",
      "Current_batch_loss: 8.51323\n",
      "Current_batch_loss: 19.68281\n",
      "Current_batch_loss: 19.50359\n",
      "Current_batch_loss: 8.00668\n",
      "Current_batch_loss: 9.227\n",
      "Current_batch_loss: 24.13494\n",
      "Current_batch_loss: 10.48699\n",
      "Current_batch_loss: 7.18824\n",
      "Current_batch_loss: 0.01164\n",
      "Current_batch_loss: 18.65335\n",
      "Current_batch_loss: 14.81725\n",
      "Current_batch_loss: 0.01407\n",
      "Current_batch_loss: 0.00938\n",
      "Current_batch_loss: 16.34957\n",
      "Current_batch_loss: 7.14746\n",
      "Current_batch_loss: 25.55193\n",
      "Current_batch_loss: 24.41953\n",
      "Current_batch_loss: 8.22527\n",
      "Current_batch_loss: 0.0111\n",
      "Current_batch_loss: 0.01199\n",
      "Current_batch_loss: 0.00891\n",
      "Current_batch_loss: 6.84421\n",
      "Current_batch_loss: 0.00992\n",
      "Current_batch_loss: 8.06292\n",
      "Current_batch_loss: 0.01471\n",
      "Current_batch_loss: 12.05097\n",
      "Current_batch_loss: 9.42998\n",
      "Current_batch_loss: 0.01147\n",
      "Current_batch_loss: 0.01964\n",
      "Current_batch_loss: 11.49939\n",
      "Current_batch_loss: 9.1733\n",
      "Current_batch_loss: 10.56387\n",
      "Current_batch_loss: 13.34422\n",
      "Current_batch_loss: 0.01815\n",
      "Current_batch_loss: 8.82914\n",
      "Current_batch_loss: 0.01283\n",
      "Current_batch_loss: 12.63007\n",
      "Current_batch_loss: 12.4861\n",
      "Current_batch_loss: 0.01061\n",
      "Current_batch_loss: 0.00947\n",
      "Current_batch_loss: 22.81646\n",
      "Current_batch_loss: 14.01231\n",
      "Current_batch_loss: 26.72833\n",
      "Current_batch_loss: 52.15798\n",
      "Current_batch_loss: 22.31317\n",
      "Current_batch_loss: 15.55119\n",
      "Current_batch_loss: 11.41806\n",
      "Current_batch_loss: 0.01199\n",
      "Current_batch_loss: 7.5805\n",
      "Current_batch_loss: 29.13451\n",
      "Current_batch_loss: 18.63941\n",
      "Current_batch_loss: 0.01266\n",
      "Current_batch_loss: 9.59352\n",
      "Current_batch_loss: 16.78489\n",
      "Current_batch_loss: 10.22081\n",
      "Current_batch_loss: 15.4472\n",
      "Current_batch_loss: 11.94147\n",
      "Current_batch_loss: 0.00828\n",
      "Current_batch_loss: 0.01178\n",
      "Current_batch_loss: 8.48489\n",
      "Current_batch_loss: 9.73672\n",
      "Current_batch_loss: 28.14288\n",
      "Current_batch_loss: 0.01097\n",
      "Current_batch_loss: 0.00972\n",
      "Current_batch_loss: 0.01072\n",
      "Current_batch_loss: 7.15428\n",
      "Current_batch_loss: 8.97221\n",
      "Current_batch_loss: 0.01881\n",
      "Current_batch_loss: 0.01157\n",
      "Current_batch_loss: 0.01047\n",
      "Current_batch_loss: 21.82025\n",
      "Current_batch_loss: 21.18515\n",
      "Current_batch_loss: 25.31393\n",
      "Current_batch_loss: 0.00968\n",
      "Current_batch_loss: 0.00882\n",
      "Current_batch_loss: 0.01055\n",
      "Current_batch_loss: 15.50559\n",
      "Current_batch_loss: 0.01339\n",
      "Current_batch_loss: 0.01184\n",
      "Current_batch_loss: 5.43083\n",
      "Current_batch_loss: 28.71291\n",
      "Current_batch_loss: 0.01162\n",
      "Current_batch_loss: 9.71678\n",
      "Current_batch_loss: 0.00938\n",
      "Current_batch_loss: 0.01423\n",
      "Current_batch_loss: 0.01299\n",
      "Current_batch_loss: 5.9914\n",
      "Current_batch_loss: 25.4405\n",
      "Current_batch_loss: 0.00949\n",
      "Current_batch_loss: 16.41325\n",
      "Current_batch_loss: 0.01449\n",
      "Current_batch_loss: 10.89437\n",
      "Current_batch_loss: 9.79835\n",
      "Current_batch_loss: 13.96508\n",
      "Current_batch_loss: 0.01479\n",
      "Current_batch_loss: 0.00873\n",
      "Current_batch_loss: 0.01012\n",
      "Current_batch_loss: 12.77816\n",
      "Current_batch_loss: 6.37878\n",
      "Current_batch_loss: 11.9492\n",
      "Current_batch_loss: 0.01323\n",
      "Current_batch_loss: 20.89265\n",
      "Current_batch_loss: 21.46316\n",
      "Current_batch_loss: 17.85521\n",
      "Current_batch_loss: 13.90172\n",
      "Current_batch_loss: 0.01599\n",
      "Current_batch_loss: 0.01055\n",
      "Current_batch_loss: 11.58999\n",
      "Current_batch_loss: 10.83454\n",
      "Current_batch_loss: 0.0152\n",
      "Current_batch_loss: 15.26046\n",
      "Current_batch_loss: 14.19978\n",
      "Current_batch_loss: 12.01587\n",
      "Current_batch_loss: 34.99414\n",
      "Current_batch_loss: 0.01258\n",
      "Current_batch_loss: 7.72255\n",
      "Current_batch_loss: 0.0099\n",
      "Current_batch_loss: 8.85346\n",
      "Current_batch_loss: 15.5207\n",
      "Current_batch_loss: 5.65725\n",
      "Current_batch_loss: 22.09298\n",
      "Current_batch_loss: 11.27172\n",
      "Current_batch_loss: 0.00947\n",
      "Current_batch_loss: 24.72006\n",
      "Current_batch_loss: 6.03091\n",
      "Current_batch_loss: 33.75523\n",
      "Current_batch_loss: 15.0204\n",
      "Current_batch_loss: 23.2691\n",
      "Current_batch_loss: 12.23801\n",
      "Current_batch_loss: 18.00358\n",
      "Current_batch_loss: 16.67665\n",
      "Current_batch_loss: 0.01483\n",
      "Current_batch_loss: 13.15153\n",
      "Current_batch_loss: 7.58076\n",
      "Current_batch_loss: 0.00963\n",
      "Current_batch_loss: 0.00761\n",
      "Current_batch_loss: 0.00999\n",
      "Current_batch_loss: 10.7942\n",
      "Current_batch_loss: 10.18349\n",
      "Current_batch_loss: 11.97303\n",
      "Current_batch_loss: 0.01024\n",
      "Current_batch_loss: 11.44529\n",
      "Current_batch_loss: 0.01505\n",
      "Current_batch_loss: 0.01203\n",
      "Current_batch_loss: 0.00934\n",
      "Current_batch_loss: 8.24287\n",
      "Current_batch_loss: 0.01244\n",
      "Current_batch_loss: 0.01167\n",
      "Current_batch_loss: 9.41509\n",
      "Current_batch_loss: 0.01352\n",
      "Current_batch_loss: 0.01355\n",
      "Current_batch_loss: 19.78146\n",
      "Current_batch_loss: 21.50846\n",
      "Current_batch_loss: 0.00919\n",
      "Current_batch_loss: 0.01222\n",
      "Current_batch_loss: 12.57888\n",
      "Current_batch_loss: 23.73302\n",
      "Current_batch_loss: 0.01231\n",
      "Current_batch_loss: 11.94854\n",
      "Current_batch_loss: 18.72005\n",
      "Current_batch_loss: 0.0127\n",
      "Current_batch_loss: 24.42725\n",
      "Current_batch_loss: 10.78342\n",
      "Current_batch_loss: 18.70746\n",
      "Current_batch_loss: 0.01343\n",
      "Current_batch_loss: 0.00873\n",
      "Current_batch_loss: 0.01096\n",
      "Current_batch_loss: 14.9266\n",
      "Current_batch_loss: 0.00888\n",
      "Current_batch_loss: 10.06905\n",
      "Current_batch_loss: 0.01204\n",
      "Current_batch_loss: 0.01127\n",
      "Current_batch_loss: 11.33011\n",
      "Current_batch_loss: 8.41929\n",
      "Current_batch_loss: 9.02973\n",
      "Current_batch_loss: 16.88579\n",
      "Current_batch_loss: 0.01116\n",
      "Current_batch_loss: 14.45736\n",
      "Current_batch_loss: 20.13019\n",
      "Current_batch_loss: 9.1992\n",
      "Current_batch_loss: 11.53531\n",
      "Current_batch_loss: 0.01237\n",
      "Current_batch_loss: 8.90707\n",
      "Current_batch_loss: 0.01348\n",
      "Current_batch_loss: 0.01158\n",
      "Current_batch_loss: 9.99128\n",
      "Current_batch_loss: 21.49541\n",
      "Current_batch_loss: 0.01115\n",
      "Current_batch_loss: 10.49859\n",
      "Current_batch_loss: 0.0124\n",
      "Current_batch_loss: 0.00889\n",
      "Current_batch_loss: 0.00976\n",
      "Current_batch_loss: 0.00841\n",
      "Current_batch_loss: 0.0107\n",
      "Current_batch_loss: 0.00964\n",
      "Current_batch_loss: 0.00922\n",
      "Current_batch_loss: 47.74782\n",
      "Current_batch_loss: 0.00986\n",
      "Current_batch_loss: 0.01183\n",
      "Current_batch_loss: 0.01127\n",
      "Current_batch_loss: 0.01151\n",
      "Current_batch_loss: 7.75787\n",
      "Current_batch_loss: 18.66089\n",
      "Current_batch_loss: 8.1808\n",
      "Current_batch_loss: 0.01087\n",
      "Current_batch_loss: 24.70481\n",
      "Current_batch_loss: 0.00772\n",
      "Current_batch_loss: 12.41541\n",
      "Current_batch_loss: 16.57259\n",
      "Current_batch_loss: 11.05893\n",
      "Current_batch_loss: 0.01072\n",
      "Current_batch_loss: 0.0064\n",
      "Current_batch_loss: 0.00849\n",
      "Current_batch_loss: 0.01048\n",
      "Current_batch_loss: 0.01096\n",
      "Current_batch_loss: 17.06523\n",
      "Current_batch_loss: 0.01264\n",
      "Current_batch_loss: 31.1883\n",
      "Current_batch_loss: 0.0108\n",
      "Current_batch_loss: 11.07988\n",
      "Current_batch_loss: 6.61014\n",
      "Current_batch_loss: 0.01057\n",
      "Current_batch_loss: 0.00949\n",
      "Current_batch_loss: 11.77196\n",
      "Current_batch_loss: 8.26517\n",
      "Current_batch_loss: 0.00638\n",
      "Current_batch_loss: 8.00481\n",
      "Current_batch_loss: 12.06709\n",
      "Current_batch_loss: 0.00727\n",
      "Current_batch_loss: 8.71934\n",
      "Current_batch_loss: 0.00966\n",
      "Current_batch_loss: 0.00894\n",
      "Current_batch_loss: 0.01113\n",
      "Current_batch_loss: 8.6305\n",
      "Current_batch_loss: 6.90665\n",
      "Current_batch_loss: 0.01209\n",
      "Current_batch_loss: 0.01079\n",
      "Current_batch_loss: 15.65068\n",
      "Current_batch_loss: 0.01205\n",
      "Current_batch_loss: 12.22138\n",
      "Current_batch_loss: 0.01009\n",
      "Current_batch_loss: 0.00794\n",
      "Current_batch_loss: 0.01126\n",
      "Current_batch_loss: 12.96592\n",
      "Current_batch_loss: 9.04893\n",
      "Current_batch_loss: 0.00853\n",
      "Current_batch_loss: 0.0127\n",
      "Current_batch_loss: 0.01284\n",
      "Current_batch_loss: 0.01074\n",
      "Current_batch_loss: 10.43367\n",
      "Current_batch_loss: 9.9166\n",
      "Current_batch_loss: 0.00971\n",
      "Current_batch_loss: 0.00908\n",
      "Current_batch_loss: 0.01151\n",
      "Current_batch_loss: 0.00912\n",
      "Current_batch_loss: 0.00922\n",
      "Current_batch_loss: 21.9728\n",
      "Current_batch_loss: 10.87351\n",
      "Current_batch_loss: 11.09936\n",
      "Current_batch_loss: 0.00681\n",
      "Current_batch_loss: 0.00874\n",
      "Current_batch_loss: 0.01283\n",
      "Current_batch_loss: 9.0179\n",
      "Current_batch_loss: 0.00679\n",
      "Current_batch_loss: 0.00888\n",
      "Current_batch_loss: 11.97467\n",
      "Current_batch_loss: 0.008\n",
      "Current_batch_loss: 10.70025\n",
      "Current_batch_loss: 0.01615\n",
      "Current_batch_loss: 0.01148\n",
      "Current_batch_loss: 4.51419\n",
      "Current_batch_loss: 0.00809\n",
      "Current_batch_loss: 0.01312\n",
      "Current_batch_loss: 0.0118\n",
      "Current_batch_loss: 6.04087\n",
      "Current_batch_loss: 0.00961\n",
      "Current_batch_loss: 0.01325\n",
      "Current_batch_loss: 0.00659\n",
      "Current_batch_loss: 0.00648\n",
      "Current_batch_loss: 0.00674\n",
      "Current_batch_loss: 9.99733\n",
      "Current_batch_loss: 0.01933\n",
      "Current_batch_loss: 15.06217\n",
      "Current_batch_loss: 0.01218\n",
      "Current_batch_loss: 24.85631\n",
      "Current_batch_loss: 0.00611\n",
      "Current_batch_loss: 0.00976\n",
      "Current_batch_loss: 13.01277\n",
      "Current_batch_loss: 0.01573\n",
      "Current_batch_loss: 0.01365\n",
      "Current_batch_loss: 14.73745\n",
      "Current_batch_loss: 0.01027\n",
      "Current_batch_loss: 15.21537\n",
      "Current_batch_loss: 12.28116\n",
      "Current_batch_loss: 0.00865\n",
      "Current_batch_loss: 0.00756\n",
      "Current_batch_loss: 0.0083\n",
      "Current_batch_loss: 6.82807\n",
      "Current_batch_loss: 10.88775\n",
      "Current_batch_loss: 0.0083\n",
      "Current_batch_loss: 0.01011\n",
      "Current_batch_loss: 0.01411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [4:01:08<56:15:41, 7233.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_batch_loss: 0.00916\n",
      "Epoch #2 | Loss: 298.06526\n",
      "Current_batch_loss: 1041.82056\n",
      "Current_batch_loss: 982.36932\n",
      "Current_batch_loss: 903.89032\n",
      "Current_batch_loss: 998.6897\n",
      "Current_batch_loss: 1048.46838\n",
      "Current_batch_loss: 942.36688\n",
      "Current_batch_loss: 1004.98523\n",
      "Current_batch_loss: 948.94238\n",
      "Current_batch_loss: 897.20862\n",
      "Current_batch_loss: 807.19409\n",
      "Current_batch_loss: 1014.98145\n",
      "Current_batch_loss: 941.69696\n",
      "Current_batch_loss: 909.64447\n",
      "Current_batch_loss: 893.77643\n",
      "Current_batch_loss: 959.74585\n",
      "Current_batch_loss: 949.13867\n",
      "Current_batch_loss: 1032.26111\n",
      "Current_batch_loss: 975.0022\n",
      "Current_batch_loss: 908.79645\n",
      "Current_batch_loss: 942.68585\n",
      "Current_batch_loss: 995.14221\n",
      "Current_batch_loss: 1037.47351\n",
      "Current_batch_loss: 1041.38452\n",
      "Current_batch_loss: 986.63013\n",
      "Current_batch_loss: 1004.42773\n",
      "Current_batch_loss: 962.30731\n",
      "Current_batch_loss: 846.57275\n",
      "Current_batch_loss: 880.95752\n",
      "Current_batch_loss: 930.34045\n",
      "Current_batch_loss: 881.94586\n",
      "Current_batch_loss: 975.4408\n",
      "Current_batch_loss: 911.03455\n",
      "Current_batch_loss: 985.36414\n",
      "Current_batch_loss: 1039.53516\n",
      "Current_batch_loss: 979.315\n",
      "Current_batch_loss: 899.66461\n",
      "Current_batch_loss: 953.99457\n",
      "Current_batch_loss: 944.38092\n",
      "Current_batch_loss: 800.0705\n",
      "Current_batch_loss: 903.95819\n",
      "Current_batch_loss: 1008.03735\n",
      "Current_batch_loss: 865.04218\n",
      "Current_batch_loss: 879.14026\n",
      "Current_batch_loss: 900.65503\n",
      "Current_batch_loss: 835.88818\n",
      "Current_batch_loss: 793.72815\n",
      "Current_batch_loss: 733.52539\n",
      "Current_batch_loss: 812.15509\n",
      "Current_batch_loss: 981.38885\n",
      "Current_batch_loss: 921.48724\n",
      "Current_batch_loss: 883.44879\n",
      "Current_batch_loss: 821.02472\n",
      "Current_batch_loss: 981.43774\n",
      "Current_batch_loss: 905.0415\n",
      "Current_batch_loss: 899.49615\n",
      "Current_batch_loss: 1018.41895\n",
      "Current_batch_loss: 981.39844\n",
      "Current_batch_loss: 974.69562\n",
      "Current_batch_loss: 882.3858\n",
      "Current_batch_loss: 872.96655\n",
      "Current_batch_loss: 908.44366\n",
      "Current_batch_loss: 849.31555\n",
      "Current_batch_loss: 882.82312\n",
      "Current_batch_loss: 795.72406\n",
      "Current_batch_loss: 910.53082\n",
      "Current_batch_loss: 881.75653\n",
      "Current_batch_loss: 868.30762\n",
      "Current_batch_loss: 952.94281\n",
      "Current_batch_loss: 906.06934\n",
      "Current_batch_loss: 779.4978\n",
      "Current_batch_loss: 923.62164\n",
      "Current_batch_loss: 850.33331\n",
      "Current_batch_loss: 896.50928\n",
      "Current_batch_loss: 854.67712\n",
      "Current_batch_loss: 767.22406\n",
      "Current_batch_loss: 834.97559\n",
      "Current_batch_loss: 799.28333\n",
      "Current_batch_loss: 913.61481\n",
      "Current_batch_loss: 709.72913\n",
      "Current_batch_loss: 966.31506\n",
      "Current_batch_loss: 899.22192\n",
      "Current_batch_loss: 910.7511\n",
      "Current_batch_loss: 956.55078\n",
      "Current_batch_loss: 921.19952\n",
      "Current_batch_loss: 857.2171\n",
      "Current_batch_loss: 899.35553\n",
      "Current_batch_loss: 873.67114\n",
      "Current_batch_loss: 771.65326\n",
      "Current_batch_loss: 858.21893\n",
      "Current_batch_loss: 724.93976\n",
      "Current_batch_loss: 936.36194\n",
      "Current_batch_loss: 664.79962\n",
      "Current_batch_loss: 964.08148\n",
      "Current_batch_loss: 923.27985\n",
      "Current_batch_loss: 847.48743\n",
      "Current_batch_loss: 922.90271\n",
      "Current_batch_loss: 807.44214\n",
      "Current_batch_loss: 848.07074\n",
      "Current_batch_loss: 836.5224\n",
      "Current_batch_loss: 792.30847\n",
      "Current_batch_loss: 905.42365\n",
      "Current_batch_loss: 885.93951\n",
      "Current_batch_loss: 841.75116\n",
      "Current_batch_loss: 869.77948\n",
      "Current_batch_loss: 896.80994\n",
      "Current_batch_loss: 807.00214\n",
      "Current_batch_loss: 820.78851\n",
      "Current_batch_loss: 742.84668\n",
      "Current_batch_loss: 917.99286\n",
      "Current_batch_loss: 880.8172\n",
      "Current_batch_loss: 972.45355\n",
      "Current_batch_loss: 895.02216\n",
      "Current_batch_loss: 910.47461\n",
      "Current_batch_loss: 768.22614\n",
      "Current_batch_loss: 930.86298\n",
      "Current_batch_loss: 705.0144\n",
      "Current_batch_loss: 715.1817\n",
      "Current_batch_loss: 799.4201\n",
      "Current_batch_loss: 774.9306\n",
      "Current_batch_loss: 944.28784\n",
      "Current_batch_loss: 787.89801\n",
      "Current_batch_loss: 660.98035\n",
      "Current_batch_loss: 835.06805\n",
      "Current_batch_loss: 788.08197\n",
      "Current_batch_loss: 937.86139\n",
      "Current_batch_loss: 724.04773\n",
      "Current_batch_loss: 717.33856\n",
      "Current_batch_loss: 918.72375\n",
      "Current_batch_loss: 782.7514\n",
      "Current_batch_loss: 821.60913\n",
      "Current_batch_loss: 911.25775\n",
      "Current_batch_loss: 712.9859\n",
      "Current_batch_loss: 897.04584\n",
      "Current_batch_loss: 950.02454\n",
      "Current_batch_loss: 830.04346\n",
      "Current_batch_loss: 832.13098\n",
      "Current_batch_loss: 753.47107\n",
      "Current_batch_loss: 844.65375\n",
      "Current_batch_loss: 771.62573\n",
      "Current_batch_loss: 885.21545\n",
      "Current_batch_loss: 833.62885\n",
      "Current_batch_loss: 766.44739\n",
      "Current_batch_loss: 884.19678\n",
      "Current_batch_loss: 835.02319\n",
      "Current_batch_loss: 937.38184\n",
      "Current_batch_loss: 875.97986\n",
      "Current_batch_loss: 430.37967\n",
      "Current_batch_loss: 819.10944\n",
      "Current_batch_loss: 821.50708\n",
      "Current_batch_loss: 816.2807\n",
      "Current_batch_loss: 882.72308\n",
      "Current_batch_loss: 890.69879\n",
      "Current_batch_loss: 791.47589\n",
      "Current_batch_loss: 894.85266\n",
      "Current_batch_loss: 827.81268\n",
      "Current_batch_loss: 833.97833\n",
      "Current_batch_loss: 700.44946\n",
      "Current_batch_loss: 718.05426\n",
      "Current_batch_loss: 861.43372\n",
      "Current_batch_loss: 904.95093\n",
      "Current_batch_loss: 693.25024\n",
      "Current_batch_loss: 822.69965\n",
      "Current_batch_loss: 890.69733\n",
      "Current_batch_loss: 918.36133\n",
      "Current_batch_loss: 873.02167\n",
      "Current_batch_loss: 813.31281\n",
      "Current_batch_loss: 863.18958\n",
      "Current_batch_loss: 816.95099\n",
      "Current_batch_loss: 794.86206\n",
      "Current_batch_loss: 680.1936\n",
      "Current_batch_loss: 705.71387\n",
      "Current_batch_loss: 536.67285\n",
      "Current_batch_loss: 916.31366\n",
      "Current_batch_loss: 818.16876\n",
      "Current_batch_loss: 670.22742\n",
      "Current_batch_loss: 783.2403\n",
      "Current_batch_loss: 813.65436\n",
      "Current_batch_loss: 874.68817\n",
      "Current_batch_loss: 819.61145\n",
      "Current_batch_loss: 918.48834\n",
      "Current_batch_loss: 798.59845\n",
      "Current_batch_loss: 832.33691\n",
      "Current_batch_loss: 830.36505\n",
      "Current_batch_loss: 820.565\n",
      "Current_batch_loss: 813.78113\n",
      "Current_batch_loss: 775.83441\n",
      "Current_batch_loss: 904.52881\n",
      "Current_batch_loss: 786.29803\n",
      "Current_batch_loss: 791.54285\n",
      "Current_batch_loss: 809.49457\n",
      "Current_batch_loss: 744.47003\n",
      "Current_batch_loss: 712.31189\n",
      "Current_batch_loss: 690.76221\n",
      "Current_batch_loss: 762.87103\n",
      "Current_batch_loss: 821.63007\n",
      "Current_batch_loss: 780.40607\n",
      "Current_batch_loss: 792.25342\n",
      "Current_batch_loss: 571.86389\n",
      "Current_batch_loss: 783.84662\n",
      "Current_batch_loss: 933.09259\n",
      "Current_batch_loss: 830.75055\n",
      "Current_batch_loss: 841.383\n",
      "Current_batch_loss: 740.77716\n",
      "Current_batch_loss: 771.711\n",
      "Current_batch_loss: 633.09534\n",
      "Current_batch_loss: 873.32788\n",
      "Current_batch_loss: 741.20093\n",
      "Current_batch_loss: 808.41382\n",
      "Current_batch_loss: 875.84869\n",
      "Current_batch_loss: 693.99249\n",
      "Current_batch_loss: 799.81561\n",
      "Current_batch_loss: 687.7912\n",
      "Current_batch_loss: 585.66687\n",
      "Current_batch_loss: 700.84753\n",
      "Current_batch_loss: 793.2005\n",
      "Current_batch_loss: 746.46265\n",
      "Current_batch_loss: 701.67084\n",
      "Current_batch_loss: 692.85748\n",
      "Current_batch_loss: 735.96979\n",
      "Current_batch_loss: 803.78894\n",
      "Current_batch_loss: 620.11322\n",
      "Current_batch_loss: 660.18835\n",
      "Current_batch_loss: 850.1178\n",
      "Current_batch_loss: 783.3891\n",
      "Current_batch_loss: 744.4328\n",
      "Current_batch_loss: 559.60156\n",
      "Current_batch_loss: 753.45483\n",
      "Current_batch_loss: 787.36127\n",
      "Current_batch_loss: 616.22058\n",
      "Current_batch_loss: 778.32141\n",
      "Current_batch_loss: 713.81787\n",
      "Current_batch_loss: 654.59528\n",
      "Current_batch_loss: 689.37378\n",
      "Current_batch_loss: 789.59283\n",
      "Current_batch_loss: 608.68555\n",
      "Current_batch_loss: 876.80646\n",
      "Current_batch_loss: 712.70868\n",
      "Current_batch_loss: 821.61664\n",
      "Current_batch_loss: 773.45874\n",
      "Current_batch_loss: 645.78772\n",
      "Current_batch_loss: 693.98541\n",
      "Current_batch_loss: 679.63989\n",
      "Current_batch_loss: 890.24091\n",
      "Current_batch_loss: 813.73425\n",
      "Current_batch_loss: 727.73669\n",
      "Current_batch_loss: 695.96338\n",
      "Current_batch_loss: 667.4444\n",
      "Current_batch_loss: 738.75\n",
      "Current_batch_loss: 779.30829\n",
      "Current_batch_loss: 820.69568\n",
      "Current_batch_loss: 830.58856\n",
      "Current_batch_loss: 568.15948\n",
      "Current_batch_loss: 744.83783\n",
      "Current_batch_loss: 799.70294\n",
      "Current_batch_loss: 785.40411\n",
      "Current_batch_loss: 756.69543\n",
      "Current_batch_loss: 576.62524\n",
      "Current_batch_loss: 752.59644\n",
      "Current_batch_loss: 806.14789\n",
      "Current_batch_loss: 744.03839\n",
      "Current_batch_loss: 747.24854\n",
      "Current_batch_loss: 659.11237\n",
      "Current_batch_loss: 798.50623\n",
      "Current_batch_loss: 677.53925\n",
      "Current_batch_loss: 759.28101\n",
      "Current_batch_loss: 652.7171\n",
      "Current_batch_loss: 735.17407\n",
      "Current_batch_loss: 626.31207\n",
      "Current_batch_loss: 746.62714\n",
      "Current_batch_loss: 676.96069\n",
      "Current_batch_loss: 541.30847\n",
      "Current_batch_loss: 782.01697\n",
      "Current_batch_loss: 519.89465\n",
      "Current_batch_loss: 603.86792\n",
      "Current_batch_loss: 696.229\n",
      "Current_batch_loss: 923.12476\n",
      "Current_batch_loss: 765.60114\n",
      "Current_batch_loss: 637.7688\n",
      "Current_batch_loss: 785.7243\n",
      "Current_batch_loss: 748.7569\n",
      "Current_batch_loss: 789.7691\n",
      "Current_batch_loss: 855.72461\n",
      "Current_batch_loss: 651.85626\n",
      "Current_batch_loss: 740.3396\n",
      "Current_batch_loss: 773.91962\n",
      "Current_batch_loss: 698.1875\n",
      "Current_batch_loss: 671.79376\n",
      "Current_batch_loss: 962.5968\n",
      "Current_batch_loss: 704.07483\n",
      "Current_batch_loss: 754.81561\n",
      "Current_batch_loss: 629.19586\n",
      "Current_batch_loss: 753.6178\n",
      "Current_batch_loss: 605.0097\n",
      "Current_batch_loss: 741.43744\n",
      "Current_batch_loss: 656.04681\n",
      "Current_batch_loss: 702.93707\n",
      "Current_batch_loss: 690.09027\n",
      "Current_batch_loss: 706.12518\n",
      "Current_batch_loss: 722.71271\n",
      "Current_batch_loss: 716.1911\n",
      "Current_batch_loss: 756.40521\n",
      "Current_batch_loss: 745.81158\n",
      "Current_batch_loss: 713.70837\n",
      "Current_batch_loss: 703.53534\n",
      "Current_batch_loss: 714.73334\n",
      "Current_batch_loss: 814.98334\n",
      "Current_batch_loss: 801.98175\n",
      "Current_batch_loss: 745.24249\n",
      "Current_batch_loss: 637.02051\n",
      "Current_batch_loss: 658.22571\n",
      "Current_batch_loss: 721.61084\n",
      "Current_batch_loss: 568.30707\n",
      "Current_batch_loss: 761.02655\n",
      "Current_batch_loss: 575.02612\n",
      "Current_batch_loss: 689.12128\n",
      "Current_batch_loss: 729.30658\n",
      "Current_batch_loss: 646.42719\n",
      "Current_batch_loss: 747.18018\n",
      "Current_batch_loss: 805.09851\n",
      "Current_batch_loss: 636.75903\n",
      "Current_batch_loss: 798.54797\n",
      "Current_batch_loss: 511.84589\n",
      "Current_batch_loss: 678.86139\n",
      "Current_batch_loss: 668.11053\n",
      "Current_batch_loss: 636.66565\n",
      "Current_batch_loss: 570.85236\n",
      "Current_batch_loss: 571.97302\n",
      "Current_batch_loss: 673.80438\n",
      "Current_batch_loss: 689.41156\n",
      "Current_batch_loss: 702.40588\n",
      "Current_batch_loss: 561.65002\n",
      "Current_batch_loss: 795.89178\n",
      "Current_batch_loss: 565.09875\n",
      "Current_batch_loss: 644.50842\n",
      "Current_batch_loss: 583.64154\n",
      "Current_batch_loss: 673.28284\n",
      "Current_batch_loss: 654.97217\n",
      "Current_batch_loss: 821.76349\n",
      "Current_batch_loss: 765.04291\n",
      "Current_batch_loss: 691.08112\n",
      "Current_batch_loss: 594.98083\n",
      "Current_batch_loss: 764.67456\n",
      "Current_batch_loss: 670.42847\n",
      "Current_batch_loss: 700.75677\n",
      "Current_batch_loss: 507.74915\n",
      "Current_batch_loss: 652.14703\n",
      "Current_batch_loss: 607.21814\n",
      "Current_batch_loss: 692.42047\n",
      "Current_batch_loss: 695.11328\n",
      "Current_batch_loss: 635.04065\n",
      "Current_batch_loss: 671.39929\n",
      "Current_batch_loss: 630.43506\n",
      "Current_batch_loss: 591.44312\n",
      "Current_batch_loss: 535.64508\n",
      "Current_batch_loss: 702.01605\n",
      "Current_batch_loss: 592.86444\n",
      "Current_batch_loss: 547.08051\n",
      "Current_batch_loss: 612.3847\n",
      "Current_batch_loss: 728.82983\n",
      "Current_batch_loss: 713.94055\n",
      "Current_batch_loss: 588.03009\n",
      "Current_batch_loss: 571.69434\n",
      "Current_batch_loss: 625.42609\n",
      "Current_batch_loss: 729.82898\n",
      "Current_batch_loss: 710.75641\n",
      "Current_batch_loss: 644.79828\n",
      "Current_batch_loss: 582.98578\n",
      "Current_batch_loss: 645.67334\n",
      "Current_batch_loss: 579.862\n",
      "Current_batch_loss: 661.94629\n",
      "Current_batch_loss: 650.73816\n",
      "Current_batch_loss: 606.53955\n",
      "Current_batch_loss: 578.53571\n",
      "Current_batch_loss: 677.37427\n",
      "Current_batch_loss: 631.35095\n",
      "Current_batch_loss: 596.61841\n",
      "Current_batch_loss: 717.48071\n",
      "Current_batch_loss: 612.99457\n",
      "Current_batch_loss: 633.73969\n",
      "Current_batch_loss: 583.11365\n",
      "Current_batch_loss: 696.56116\n",
      "Current_batch_loss: 690.90942\n",
      "Current_batch_loss: 573.96753\n",
      "Current_batch_loss: 527.16803\n",
      "Current_batch_loss: 692.43542\n",
      "Current_batch_loss: 602.30249\n",
      "Current_batch_loss: 670.82404\n",
      "Current_batch_loss: 625.90967\n",
      "Current_batch_loss: 681.52087\n",
      "Current_batch_loss: 732.33575\n",
      "Current_batch_loss: 598.1217\n",
      "Current_batch_loss: 635.7207\n",
      "Current_batch_loss: 470.25244\n",
      "Current_batch_loss: 684.80701\n",
      "Current_batch_loss: 621.03918\n",
      "Current_batch_loss: 559.06604\n",
      "Current_batch_loss: 649.57471\n",
      "Current_batch_loss: 530.63391\n",
      "Current_batch_loss: 561.73907\n",
      "Current_batch_loss: 560.24304\n",
      "Current_batch_loss: 572.01794\n",
      "Current_batch_loss: 527.36108\n",
      "Current_batch_loss: 602.65601\n",
      "Current_batch_loss: 679.78705\n",
      "Current_batch_loss: 714.5705\n",
      "Current_batch_loss: 702.7702\n",
      "Current_batch_loss: 541.42816\n",
      "Current_batch_loss: 524.30963\n",
      "Current_batch_loss: 569.33459\n",
      "Current_batch_loss: 672.323\n",
      "Current_batch_loss: 577.95471\n",
      "Current_batch_loss: 590.34845\n",
      "Current_batch_loss: 558.71954\n",
      "Current_batch_loss: 734.61157\n",
      "Current_batch_loss: 487.82928\n",
      "Current_batch_loss: 581.1842\n",
      "Current_batch_loss: 744.82935\n",
      "Current_batch_loss: 626.50916\n",
      "Current_batch_loss: 650.88782\n",
      "Current_batch_loss: 737.47571\n",
      "Current_batch_loss: 578.78094\n",
      "Current_batch_loss: 659.18738\n",
      "Current_batch_loss: 465.70184\n",
      "Current_batch_loss: 535.39911\n",
      "Current_batch_loss: 451.93307\n",
      "Current_batch_loss: 592.16748\n",
      "Current_batch_loss: 735.46014\n",
      "Current_batch_loss: 719.15771\n",
      "Current_batch_loss: 594.8172\n",
      "Current_batch_loss: 571.60223\n",
      "Current_batch_loss: 700.09332\n",
      "Current_batch_loss: 548.03448\n",
      "Current_batch_loss: 727.8512\n",
      "Current_batch_loss: 704.07739\n",
      "Current_batch_loss: 631.80627\n",
      "Current_batch_loss: 634.20807\n",
      "Current_batch_loss: 517.48041\n",
      "Current_batch_loss: 556.93744\n",
      "Current_batch_loss: 681.41412\n",
      "Current_batch_loss: 634.5874\n",
      "Current_batch_loss: 659.89557\n",
      "Current_batch_loss: 642.66974\n",
      "Current_batch_loss: 520.32477\n",
      "Current_batch_loss: 791.87463\n",
      "Current_batch_loss: 505.99771\n",
      "Current_batch_loss: 573.08881\n",
      "Current_batch_loss: 570.849\n",
      "Current_batch_loss: 588.45477\n",
      "Current_batch_loss: 653.45685\n",
      "Current_batch_loss: 675.26941\n",
      "Current_batch_loss: 606.46045\n",
      "Current_batch_loss: 687.48425\n",
      "Current_batch_loss: 581.86774\n",
      "Current_batch_loss: 574.86615\n",
      "Current_batch_loss: 596.37421\n",
      "Current_batch_loss: 472.46756\n",
      "Current_batch_loss: 567.22589\n",
      "Current_batch_loss: 694.42529\n",
      "Current_batch_loss: 469.3447\n",
      "Current_batch_loss: 602.5199\n",
      "Current_batch_loss: 420.79529\n",
      "Current_batch_loss: 730.42566\n",
      "Current_batch_loss: 568.19574\n",
      "Current_batch_loss: 597.24359\n",
      "Current_batch_loss: 580.66418\n",
      "Current_batch_loss: 526.42609\n",
      "Current_batch_loss: 558.63739\n",
      "Current_batch_loss: 732.8902\n",
      "Current_batch_loss: 536.74719\n",
      "Current_batch_loss: 416.81921\n",
      "Current_batch_loss: 511.10327\n",
      "Current_batch_loss: 601.8255\n",
      "Current_batch_loss: 545.25421\n",
      "Current_batch_loss: 561.14154\n",
      "Current_batch_loss: 521.38989\n",
      "Current_batch_loss: 589.64209\n",
      "Current_batch_loss: 575.64502\n",
      "Current_batch_loss: 480.47937\n",
      "Current_batch_loss: 524.69794\n",
      "Current_batch_loss: 528.68512\n",
      "Current_batch_loss: 401.07932\n",
      "Current_batch_loss: 604.4024\n",
      "Current_batch_loss: 537.3772\n",
      "Current_batch_loss: 568.93756\n",
      "Current_batch_loss: 614.68201\n",
      "Current_batch_loss: 440.39746\n",
      "Current_batch_loss: 593.24506\n",
      "Current_batch_loss: 601.76129\n",
      "Current_batch_loss: 521.01727\n",
      "Current_batch_loss: 600.56024\n",
      "Current_batch_loss: 503.33832\n",
      "Current_batch_loss: 755.82062\n",
      "Current_batch_loss: 587.58643\n",
      "Current_batch_loss: 596.57684\n",
      "Current_batch_loss: 568.98529\n",
      "Current_batch_loss: 589.49829\n",
      "Current_batch_loss: 614.46844\n",
      "Current_batch_loss: 545.38226\n",
      "Current_batch_loss: 567.9209\n",
      "Current_batch_loss: 463.94229\n",
      "Current_batch_loss: 628.48438\n",
      "Current_batch_loss: 459.77087\n",
      "Current_batch_loss: 537.56836\n",
      "Current_batch_loss: 547.25781\n",
      "Current_batch_loss: 486.66232\n",
      "Current_batch_loss: 578.78485\n",
      "Current_batch_loss: 535.0744\n",
      "Current_batch_loss: 575.71338\n",
      "Current_batch_loss: 366.38651\n",
      "Current_batch_loss: 471.31067\n",
      "Current_batch_loss: 427.89249\n",
      "Current_batch_loss: 510.78424\n",
      "Current_batch_loss: 487.53699\n",
      "Current_batch_loss: 577.71118\n",
      "Current_batch_loss: 503.77567\n",
      "Current_batch_loss: 480.9194\n",
      "Current_batch_loss: 533.74347\n",
      "Current_batch_loss: 616.75946\n",
      "Current_batch_loss: 323.36032\n",
      "Current_batch_loss: 408.02634\n",
      "Current_batch_loss: 592.59741\n",
      "Current_batch_loss: 460.84366\n",
      "Current_batch_loss: 472.40756\n",
      "Current_batch_loss: 445.73499\n",
      "Current_batch_loss: 555.18268\n",
      "Current_batch_loss: 618.45135\n",
      "Current_batch_loss: 487.57156\n",
      "Current_batch_loss: 498.23102\n",
      "Current_batch_loss: 518.90448\n",
      "Current_batch_loss: 438.81821\n",
      "Current_batch_loss: 605.57739\n",
      "Current_batch_loss: 526.65063\n",
      "Current_batch_loss: 389.57065\n",
      "Current_batch_loss: 445.47653\n",
      "Current_batch_loss: 533.86865\n",
      "Current_batch_loss: 561.47797\n",
      "Current_batch_loss: 412.7579\n",
      "Current_batch_loss: 343.22327\n",
      "Current_batch_loss: 401.27527\n",
      "Current_batch_loss: 425.59402\n",
      "Current_batch_loss: 658.52917\n",
      "Current_batch_loss: 502.40475\n",
      "Current_batch_loss: 524.62512\n",
      "Current_batch_loss: 352.31036\n",
      "Current_batch_loss: 575.96906\n",
      "Current_batch_loss: 599.68427\n",
      "Current_batch_loss: 402.82562\n",
      "Current_batch_loss: 483.17865\n",
      "Current_batch_loss: 470.02469\n",
      "Current_batch_loss: 435.04556\n",
      "Current_batch_loss: 556.83997\n",
      "Current_batch_loss: 460.99268\n",
      "Current_batch_loss: 454.11948\n",
      "Current_batch_loss: 429.22382\n",
      "Current_batch_loss: 462.51425\n",
      "Current_batch_loss: 433.37848\n",
      "Current_batch_loss: 444.07553\n",
      "Current_batch_loss: 479.88208\n",
      "Current_batch_loss: 487.76135\n",
      "Current_batch_loss: 539.30579\n",
      "Current_batch_loss: 487.56271\n",
      "Current_batch_loss: 545.63263\n",
      "Current_batch_loss: 364.80893\n",
      "Current_batch_loss: 537.62463\n",
      "Current_batch_loss: 440.03577\n",
      "Current_batch_loss: 493.24792\n",
      "Current_batch_loss: 454.89822\n",
      "Current_batch_loss: 382.80527\n",
      "Current_batch_loss: 526.21954\n",
      "Current_batch_loss: 434.57172\n",
      "Current_batch_loss: 363.27853\n",
      "Current_batch_loss: 518.98767\n",
      "Current_batch_loss: 524.45142\n",
      "Current_batch_loss: 357.42648\n",
      "Current_batch_loss: 430.68457\n",
      "Current_batch_loss: 451.61899\n",
      "Current_batch_loss: 625.15222\n",
      "Current_batch_loss: 496.19992\n",
      "Current_batch_loss: 535.71802\n",
      "Current_batch_loss: 588.03308\n",
      "Current_batch_loss: 608.18793\n",
      "Current_batch_loss: 549.43652\n",
      "Current_batch_loss: 551.66846\n",
      "Current_batch_loss: 383.97305\n",
      "Current_batch_loss: 445.32599\n",
      "Current_batch_loss: 440.34427\n",
      "Current_batch_loss: 335.62799\n",
      "Current_batch_loss: 414.42484\n",
      "Current_batch_loss: 483.25772\n",
      "Current_batch_loss: 492.37363\n",
      "Current_batch_loss: 460.55899\n",
      "Current_batch_loss: 477.8215\n",
      "Current_batch_loss: 408.14331\n",
      "Current_batch_loss: 397.57288\n",
      "Current_batch_loss: 509.66434\n",
      "Current_batch_loss: 401.45358\n",
      "Current_batch_loss: 495.08176\n",
      "Current_batch_loss: 439.72189\n",
      "Current_batch_loss: 509.31775\n",
      "Current_batch_loss: 557.65521\n",
      "Current_batch_loss: 372.78873\n",
      "Current_batch_loss: 423.33255\n",
      "Current_batch_loss: 516.57635\n",
      "Current_batch_loss: 383.0072\n",
      "Current_batch_loss: 314.09717\n",
      "Current_batch_loss: 392.29456\n",
      "Current_batch_loss: 467.26385\n",
      "Current_batch_loss: 491.13406\n",
      "Current_batch_loss: 499.88245\n",
      "Current_batch_loss: 316.15646\n",
      "Current_batch_loss: 464.45953\n",
      "Current_batch_loss: 613.90881\n",
      "Current_batch_loss: 326.80734\n",
      "Current_batch_loss: 565.07574\n",
      "Current_batch_loss: 483.37265\n",
      "Current_batch_loss: 437.92255\n",
      "Current_batch_loss: 507.36737\n",
      "Current_batch_loss: 459.3205\n",
      "Current_batch_loss: 355.31805\n",
      "Current_batch_loss: 472.1954\n",
      "Current_batch_loss: 427.2941\n",
      "Current_batch_loss: 434.27835\n",
      "Current_batch_loss: 491.74881\n",
      "Current_batch_loss: 353.28787\n",
      "Current_batch_loss: 353.66342\n",
      "Current_batch_loss: 457.155\n",
      "Current_batch_loss: 470.29514\n",
      "Current_batch_loss: 446.29276\n",
      "Current_batch_loss: 408.71603\n",
      "Current_batch_loss: 425.83301\n",
      "Current_batch_loss: 419.75552\n",
      "Current_batch_loss: 457.2767\n",
      "Current_batch_loss: 589.66846\n",
      "Current_batch_loss: 396.69321\n",
      "Current_batch_loss: 360.46786\n",
      "Current_batch_loss: 490.51334\n",
      "Current_batch_loss: 336.75372\n",
      "Current_batch_loss: 491.95911\n",
      "Current_batch_loss: 293.37405\n",
      "Current_batch_loss: 441.90906\n",
      "Current_batch_loss: 485.26044\n",
      "Current_batch_loss: 480.72296\n",
      "Current_batch_loss: 325.30502\n",
      "Current_batch_loss: 329.02756\n",
      "Current_batch_loss: 291.15643\n",
      "Current_batch_loss: 406.74582\n",
      "Current_batch_loss: 507.5574\n",
      "Current_batch_loss: 428.88992\n",
      "Current_batch_loss: 385.19202\n",
      "Current_batch_loss: 533.98895\n",
      "Current_batch_loss: 443.21292\n",
      "Current_batch_loss: 347.06595\n",
      "Current_batch_loss: 437.29166\n",
      "Current_batch_loss: 389.05591\n",
      "Current_batch_loss: 432.38828\n",
      "Current_batch_loss: 337.41125\n",
      "Current_batch_loss: 357.20648\n",
      "Current_batch_loss: 489.12515\n",
      "Current_batch_loss: 313.25247\n",
      "Current_batch_loss: 535.51141\n",
      "Current_batch_loss: 462.29608\n",
      "Current_batch_loss: 336.96939\n",
      "Current_batch_loss: 522.54309\n",
      "Current_batch_loss: 424.89761\n",
      "Current_batch_loss: 296.07513\n",
      "Current_batch_loss: 491.35815\n",
      "Current_batch_loss: 508.30194\n",
      "Current_batch_loss: 438.28955\n",
      "Current_batch_loss: 234.28444\n",
      "Current_batch_loss: 445.18924\n",
      "Current_batch_loss: 323.29895\n",
      "Current_batch_loss: 382.47815\n",
      "Current_batch_loss: 355.29495\n",
      "Current_batch_loss: 373.25577\n",
      "Current_batch_loss: 368.88031\n",
      "Current_batch_loss: 399.95349\n",
      "Current_batch_loss: 371.25629\n",
      "Current_batch_loss: 385.29468\n",
      "Current_batch_loss: 252.05983\n",
      "Current_batch_loss: 467.31271\n",
      "Current_batch_loss: 164.11075\n",
      "Current_batch_loss: 312.67029\n",
      "Current_batch_loss: 545.0929\n",
      "Current_batch_loss: 392.34589\n",
      "Current_batch_loss: 365.91852\n",
      "Current_batch_loss: 311.38089\n",
      "Current_batch_loss: 293.22885\n",
      "Current_batch_loss: 191.31351\n",
      "Current_batch_loss: 441.66943\n",
      "Current_batch_loss: 417.18695\n",
      "Current_batch_loss: 427.43649\n",
      "Current_batch_loss: 488.52011\n",
      "Current_batch_loss: 306.26901\n",
      "Current_batch_loss: 306.80243\n",
      "Current_batch_loss: 340.57599\n",
      "Current_batch_loss: 360.974\n",
      "Current_batch_loss: 254.12909\n",
      "Current_batch_loss: 411.32339\n",
      "Current_batch_loss: 372.47809\n",
      "Current_batch_loss: 391.63043\n",
      "Current_batch_loss: 334.56342\n",
      "Current_batch_loss: 535.77997\n",
      "Current_batch_loss: 379.99069\n",
      "Current_batch_loss: 345.4426\n",
      "Current_batch_loss: 468.17792\n",
      "Current_batch_loss: 426.48962\n",
      "Current_batch_loss: 496.34842\n",
      "Current_batch_loss: 324.72162\n",
      "Current_batch_loss: 427.46994\n",
      "Current_batch_loss: 449.17117\n",
      "Current_batch_loss: 285.69077\n",
      "Current_batch_loss: 406.51477\n",
      "Current_batch_loss: 362.62515\n",
      "Current_batch_loss: 344.91528\n",
      "Current_batch_loss: 347.92041\n",
      "Current_batch_loss: 437.85043\n",
      "Current_batch_loss: 440.07663\n",
      "Current_batch_loss: 375.41364\n",
      "Current_batch_loss: 312.85205\n",
      "Current_batch_loss: 322.37567\n",
      "Current_batch_loss: 337.2507\n",
      "Current_batch_loss: 316.60513\n",
      "Current_batch_loss: 286.70068\n",
      "Current_batch_loss: 302.32254\n",
      "Current_batch_loss: 414.65814\n",
      "Current_batch_loss: 256.59232\n",
      "Current_batch_loss: 363.07318\n",
      "Current_batch_loss: 379.91626\n",
      "Current_batch_loss: 212.69026\n",
      "Current_batch_loss: 321.61575\n",
      "Current_batch_loss: 393.03946\n",
      "Current_batch_loss: 342.58176\n",
      "Current_batch_loss: 286.86731\n",
      "Current_batch_loss: 383.89856\n",
      "Current_batch_loss: 243.7719\n",
      "Current_batch_loss: 354.75748\n",
      "Current_batch_loss: 350.24255\n",
      "Current_batch_loss: 317.06076\n",
      "Current_batch_loss: 403.57938\n",
      "Current_batch_loss: 275.06198\n",
      "Current_batch_loss: 322.14688\n",
      "Current_batch_loss: 345.40738\n",
      "Current_batch_loss: 327.56702\n",
      "Current_batch_loss: 328.8941\n",
      "Current_batch_loss: 252.5464\n",
      "Current_batch_loss: 342.20865\n",
      "Current_batch_loss: 254.70845\n",
      "Current_batch_loss: 332.46841\n",
      "Current_batch_loss: 355.53116\n",
      "Current_batch_loss: 353.07455\n",
      "Current_batch_loss: 291.43143\n",
      "Current_batch_loss: 443.11838\n",
      "Current_batch_loss: 473.59872\n",
      "Current_batch_loss: 346.77997\n",
      "Current_batch_loss: 435.40991\n",
      "Current_batch_loss: 391.84509\n",
      "Current_batch_loss: 209.14017\n",
      "Current_batch_loss: 297.2634\n",
      "Current_batch_loss: 430.8421\n",
      "Current_batch_loss: 339.5676\n",
      "Current_batch_loss: 408.34119\n",
      "Current_batch_loss: 307.06195\n",
      "Current_batch_loss: 401.54083\n",
      "Current_batch_loss: 295.08118\n",
      "Current_batch_loss: 312.22943\n",
      "Current_batch_loss: 462.6814\n",
      "Current_batch_loss: 301.23315\n",
      "Current_batch_loss: 257.26294\n",
      "Current_batch_loss: 359.74585\n",
      "Current_batch_loss: 281.92438\n",
      "Current_batch_loss: 273.06952\n",
      "Current_batch_loss: 273.39828\n",
      "Current_batch_loss: 297.55106\n",
      "Current_batch_loss: 470.33618\n",
      "Current_batch_loss: 294.29221\n",
      "Current_batch_loss: 496.82288\n",
      "Current_batch_loss: 357.07272\n",
      "Current_batch_loss: 302.41772\n",
      "Current_batch_loss: 298.1792\n",
      "Current_batch_loss: 257.20865\n",
      "Current_batch_loss: 197.95601\n",
      "Current_batch_loss: 305.06104\n",
      "Current_batch_loss: 261.37457\n",
      "Current_batch_loss: 339.44949\n",
      "Current_batch_loss: 275.85431\n",
      "Current_batch_loss: 256.49792\n",
      "Current_batch_loss: 338.77875\n",
      "Current_batch_loss: 368.68088\n",
      "Current_batch_loss: 289.96628\n",
      "Current_batch_loss: 326.9921\n",
      "Current_batch_loss: 345.5997\n",
      "Current_batch_loss: 257.70044\n",
      "Current_batch_loss: 389.89728\n",
      "Current_batch_loss: 372.6734\n",
      "Current_batch_loss: 326.52597\n",
      "Current_batch_loss: 400.37546\n",
      "Current_batch_loss: 288.93448\n",
      "Current_batch_loss: 273.94171\n",
      "Current_batch_loss: 221.72507\n",
      "Current_batch_loss: 258.75439\n",
      "Current_batch_loss: 334.19766\n",
      "Current_batch_loss: 367.49814\n",
      "Current_batch_loss: 194.57812\n",
      "Current_batch_loss: 345.11929\n",
      "Current_batch_loss: 360.78546\n",
      "Current_batch_loss: 310.18784\n",
      "Current_batch_loss: 435.50696\n",
      "Current_batch_loss: 300.24628\n",
      "Current_batch_loss: 325.37598\n",
      "Current_batch_loss: 229.59787\n",
      "Current_batch_loss: 255.98817\n",
      "Current_batch_loss: 162.70341\n",
      "Current_batch_loss: 269.36227\n",
      "Current_batch_loss: 214.4994\n",
      "Current_batch_loss: 356.92664\n",
      "Current_batch_loss: 349.64038\n",
      "Current_batch_loss: 241.93365\n",
      "Current_batch_loss: 312.47043\n",
      "Current_batch_loss: 333.06171\n",
      "Current_batch_loss: 377.76053\n",
      "Current_batch_loss: 281.47134\n",
      "Current_batch_loss: 260.00732\n",
      "Current_batch_loss: 391.98651\n",
      "Current_batch_loss: 210.88829\n",
      "Current_batch_loss: 271.10153\n",
      "Current_batch_loss: 361.24152\n",
      "Current_batch_loss: 383.86224\n",
      "Current_batch_loss: 258.51859\n",
      "Current_batch_loss: 311.64023\n",
      "Current_batch_loss: 316.42621\n",
      "Current_batch_loss: 376.85651\n",
      "Current_batch_loss: 361.04657\n",
      "Current_batch_loss: 382.37329\n",
      "Current_batch_loss: 303.58533\n",
      "Current_batch_loss: 274.8577\n",
      "Current_batch_loss: 269.6828\n",
      "Current_batch_loss: 293.04721\n",
      "Current_batch_loss: 254.51988\n",
      "Current_batch_loss: 298.38367\n",
      "Current_batch_loss: 252.16536\n",
      "Current_batch_loss: 282.69568\n",
      "Current_batch_loss: 359.86475\n",
      "Current_batch_loss: 333.45544\n",
      "Current_batch_loss: 352.33087\n",
      "Current_batch_loss: 263.72073\n",
      "Current_batch_loss: 274.37177\n",
      "Current_batch_loss: 303.64401\n",
      "Current_batch_loss: 269.931\n",
      "Current_batch_loss: 316.06305\n",
      "Current_batch_loss: 233.15343\n",
      "Current_batch_loss: 282.87317\n",
      "Current_batch_loss: 379.69775\n",
      "Current_batch_loss: 263.02609\n",
      "Current_batch_loss: 222.29845\n",
      "Current_batch_loss: 462.71362\n",
      "Current_batch_loss: 328.66644\n",
      "Current_batch_loss: 247.81693\n",
      "Current_batch_loss: 281.84924\n",
      "Current_batch_loss: 249.62923\n",
      "Current_batch_loss: 203.73952\n",
      "Current_batch_loss: 317.72546\n",
      "Current_batch_loss: 287.67819\n",
      "Current_batch_loss: 184.70599\n",
      "Current_batch_loss: 307.68414\n",
      "Current_batch_loss: 257.32941\n",
      "Current_batch_loss: 283.50336\n",
      "Current_batch_loss: 276.29242\n",
      "Current_batch_loss: 243.33189\n",
      "Current_batch_loss: 257.42664\n",
      "Current_batch_loss: 245.20107\n",
      "Current_batch_loss: 347.3136\n",
      "Current_batch_loss: 274.22781\n",
      "Current_batch_loss: 336.12009\n",
      "Current_batch_loss: 261.31992\n",
      "Current_batch_loss: 263.25714\n",
      "Current_batch_loss: 103.68782\n",
      "Current_batch_loss: 364.49344\n",
      "Current_batch_loss: 299.54181\n",
      "Current_batch_loss: 173.35185\n",
      "Current_batch_loss: 232.23607\n",
      "Current_batch_loss: 259.10062\n",
      "Current_batch_loss: 360.33932\n",
      "Current_batch_loss: 315.57397\n",
      "Current_batch_loss: 173.05235\n",
      "Current_batch_loss: 198.81079\n",
      "Current_batch_loss: 215.30392\n",
      "Current_batch_loss: 427.34753\n",
      "Current_batch_loss: 246.44826\n",
      "Current_batch_loss: 206.41846\n",
      "Current_batch_loss: 215.18178\n",
      "Current_batch_loss: 154.83038\n",
      "Current_batch_loss: 238.20663\n",
      "Current_batch_loss: 255.38266\n",
      "Current_batch_loss: 303.0845\n",
      "Current_batch_loss: 309.72552\n",
      "Current_batch_loss: 249.88499\n",
      "Current_batch_loss: 235.81851\n",
      "Current_batch_loss: 245.04337\n",
      "Current_batch_loss: 247.38203\n",
      "Current_batch_loss: 269.41858\n",
      "Current_batch_loss: 304.55124\n",
      "Current_batch_loss: 218.60487\n",
      "Current_batch_loss: 262.11209\n",
      "Current_batch_loss: 226.05064\n",
      "Current_batch_loss: 189.93921\n",
      "Current_batch_loss: 306.78079\n",
      "Current_batch_loss: 387.55756\n",
      "Current_batch_loss: 234.66884\n",
      "Current_batch_loss: 206.09271\n",
      "Current_batch_loss: 234.55186\n",
      "Current_batch_loss: 231.54802\n",
      "Current_batch_loss: 255.32651\n",
      "Current_batch_loss: 236.39743\n",
      "Current_batch_loss: 208.60074\n",
      "Current_batch_loss: 260.99844\n",
      "Current_batch_loss: 280.93674\n",
      "Current_batch_loss: 205.21193\n",
      "Current_batch_loss: 284.01993\n",
      "Current_batch_loss: 253.50949\n",
      "Current_batch_loss: 245.18011\n",
      "Current_batch_loss: 199.41353\n",
      "Current_batch_loss: 203.10614\n",
      "Current_batch_loss: 329.12247\n",
      "Current_batch_loss: 172.43102\n",
      "Current_batch_loss: 268.25665\n",
      "Current_batch_loss: 253.96629\n",
      "Current_batch_loss: 307.94879\n",
      "Current_batch_loss: 201.04712\n",
      "Current_batch_loss: 163.60097\n",
      "Current_batch_loss: 281.51352\n",
      "Current_batch_loss: 220.78822\n",
      "Current_batch_loss: 245.41879\n",
      "Current_batch_loss: 381.20804\n",
      "Current_batch_loss: 207.43288\n",
      "Current_batch_loss: 244.62856\n",
      "Current_batch_loss: 242.03879\n",
      "Current_batch_loss: 197.04495\n",
      "Current_batch_loss: 198.04054\n",
      "Current_batch_loss: 225.39458\n",
      "Current_batch_loss: 259.24893\n",
      "Current_batch_loss: 171.23174\n",
      "Current_batch_loss: 255.51379\n",
      "Current_batch_loss: 142.02129\n",
      "Current_batch_loss: 189.49773\n",
      "Current_batch_loss: 312.45114\n",
      "Current_batch_loss: 309.47629\n",
      "Current_batch_loss: 189.66899\n",
      "Current_batch_loss: 234.22621\n",
      "Current_batch_loss: 167.5473\n",
      "Current_batch_loss: 243.50563\n",
      "Current_batch_loss: 300.39478\n",
      "Current_batch_loss: 222.40495\n",
      "Current_batch_loss: 292.63538\n",
      "Current_batch_loss: 171.09238\n",
      "Current_batch_loss: 179.09406\n",
      "Current_batch_loss: 265.06766\n",
      "Current_batch_loss: 232.85086\n",
      "Current_batch_loss: 236.22849\n",
      "Current_batch_loss: 269.46609\n",
      "Current_batch_loss: 172.81786\n",
      "Current_batch_loss: 317.76837\n",
      "Current_batch_loss: 226.95868\n",
      "Current_batch_loss: 273.34058\n",
      "Current_batch_loss: 192.1124\n",
      "Current_batch_loss: 226.7612\n",
      "Current_batch_loss: 266.37375\n",
      "Current_batch_loss: 280.89511\n",
      "Current_batch_loss: 236.11649\n",
      "Current_batch_loss: 211.75075\n",
      "Current_batch_loss: 237.0563\n",
      "Current_batch_loss: 194.91922\n",
      "Current_batch_loss: 275.04468\n",
      "Current_batch_loss: 181.27148\n",
      "Current_batch_loss: 168.81441\n",
      "Current_batch_loss: 180.50136\n",
      "Current_batch_loss: 95.60143\n",
      "Current_batch_loss: 209.70738\n",
      "Current_batch_loss: 247.34909\n",
      "Current_batch_loss: 166.75198\n",
      "Current_batch_loss: 191.84869\n",
      "Current_batch_loss: 144.51828\n",
      "Current_batch_loss: 269.84525\n",
      "Current_batch_loss: 175.43823\n",
      "Current_batch_loss: 163.45726\n",
      "Current_batch_loss: 252.09404\n",
      "Current_batch_loss: 274.08987\n",
      "Current_batch_loss: 134.06659\n",
      "Current_batch_loss: 169.15395\n",
      "Current_batch_loss: 250.54477\n",
      "Current_batch_loss: 305.62729\n",
      "Current_batch_loss: 241.89018\n",
      "Current_batch_loss: 216.89824\n",
      "Current_batch_loss: 225.58641\n",
      "Current_batch_loss: 227.02161\n",
      "Current_batch_loss: 222.62851\n",
      "Current_batch_loss: 211.57124\n",
      "Current_batch_loss: 147.02806\n",
      "Current_batch_loss: 202.25005\n",
      "Current_batch_loss: 292.14587\n",
      "Current_batch_loss: 189.99448\n",
      "Current_batch_loss: 262.86133\n",
      "Current_batch_loss: 141.30922\n",
      "Current_batch_loss: 232.01491\n",
      "Current_batch_loss: 203.86993\n",
      "Current_batch_loss: 184.49255\n",
      "Current_batch_loss: 317.88116\n",
      "Current_batch_loss: 196.92511\n",
      "Current_batch_loss: 166.29654\n",
      "Current_batch_loss: 177.57788\n",
      "Current_batch_loss: 197.12657\n",
      "Current_batch_loss: 183.75366\n",
      "Current_batch_loss: 167.61607\n",
      "Current_batch_loss: 224.77905\n",
      "Current_batch_loss: 129.02263\n",
      "Current_batch_loss: 256.20047\n",
      "Current_batch_loss: 150.56854\n",
      "Current_batch_loss: 238.51295\n",
      "Current_batch_loss: 169.39478\n",
      "Current_batch_loss: 177.92017\n",
      "Current_batch_loss: 193.56403\n",
      "Current_batch_loss: 180.84886\n",
      "Current_batch_loss: 232.43144\n",
      "Current_batch_loss: 193.71333\n",
      "Current_batch_loss: 212.45364\n",
      "Current_batch_loss: 159.12689\n",
      "Current_batch_loss: 197.58275\n",
      "Current_batch_loss: 193.6096\n",
      "Current_batch_loss: 196.02271\n",
      "Current_batch_loss: 188.24068\n",
      "Current_batch_loss: 145.02464\n",
      "Current_batch_loss: 139.81044\n",
      "Current_batch_loss: 193.75253\n",
      "Current_batch_loss: 206.79347\n",
      "Current_batch_loss: 209.47139\n",
      "Current_batch_loss: 193.55446\n",
      "Current_batch_loss: 156.44446\n",
      "Current_batch_loss: 204.20027\n",
      "Current_batch_loss: 155.86784\n",
      "Current_batch_loss: 179.64056\n",
      "Current_batch_loss: 112.01736\n",
      "Current_batch_loss: 141.21863\n",
      "Current_batch_loss: 201.88774\n",
      "Current_batch_loss: 227.08783\n",
      "Current_batch_loss: 130.76263\n",
      "Current_batch_loss: 274.68558\n",
      "Current_batch_loss: 190.51141\n",
      "Current_batch_loss: 122.36687\n",
      "Current_batch_loss: 192.65675\n",
      "Current_batch_loss: 184.91345\n",
      "Current_batch_loss: 186.3835\n",
      "Current_batch_loss: 149.11322\n",
      "Current_batch_loss: 177.56175\n",
      "Current_batch_loss: 176.90593\n",
      "Current_batch_loss: 119.46183\n",
      "Current_batch_loss: 131.22345\n",
      "Current_batch_loss: 150.10794\n",
      "Current_batch_loss: 198.24551\n",
      "Current_batch_loss: 172.2789\n",
      "Current_batch_loss: 115.08916\n",
      "Current_batch_loss: 168.3578\n",
      "Current_batch_loss: 230.31406\n",
      "Current_batch_loss: 126.7177\n",
      "Current_batch_loss: 136.99504\n",
      "Current_batch_loss: 144.08824\n",
      "Current_batch_loss: 167.21147\n",
      "Current_batch_loss: 131.64839\n",
      "Current_batch_loss: 165.50513\n",
      "Current_batch_loss: 128.5475\n",
      "Current_batch_loss: 139.26868\n",
      "Current_batch_loss: 174.91692\n",
      "Current_batch_loss: 154.50923\n",
      "Current_batch_loss: 207.80621\n",
      "Current_batch_loss: 195.77635\n",
      "Current_batch_loss: 178.61569\n",
      "Current_batch_loss: 101.58493\n",
      "Current_batch_loss: 133.09566\n",
      "Current_batch_loss: 173.13916\n",
      "Current_batch_loss: 227.8575\n",
      "Current_batch_loss: 82.32138\n",
      "Current_batch_loss: 145.85442\n",
      "Current_batch_loss: 143.06894\n",
      "Current_batch_loss: 142.26657\n",
      "Current_batch_loss: 254.10478\n",
      "Current_batch_loss: 250.4303\n",
      "Current_batch_loss: 111.60265\n",
      "Current_batch_loss: 180.6268\n",
      "Current_batch_loss: 145.51558\n",
      "Current_batch_loss: 195.37987\n",
      "Current_batch_loss: 135.36034\n",
      "Current_batch_loss: 175.59601\n",
      "Current_batch_loss: 97.1705\n",
      "Current_batch_loss: 131.70949\n",
      "Current_batch_loss: 161.24811\n",
      "Current_batch_loss: 222.41106\n",
      "Current_batch_loss: 160.99319\n",
      "Current_batch_loss: 236.04102\n",
      "Current_batch_loss: 123.17388\n",
      "Current_batch_loss: 101.37253\n",
      "Current_batch_loss: 93.39143\n",
      "Current_batch_loss: 153.34784\n",
      "Current_batch_loss: 106.79465\n",
      "Current_batch_loss: 74.18221\n",
      "Current_batch_loss: 169.10704\n",
      "Current_batch_loss: 186.76048\n",
      "Current_batch_loss: 157.89296\n",
      "Current_batch_loss: 178.36244\n",
      "Current_batch_loss: 140.0778\n",
      "Current_batch_loss: 91.05608\n",
      "Current_batch_loss: 140.29501\n",
      "Current_batch_loss: 115.3904\n",
      "Current_batch_loss: 175.07298\n",
      "Current_batch_loss: 184.77657\n",
      "Current_batch_loss: 121.05097\n",
      "Current_batch_loss: 204.54256\n",
      "Current_batch_loss: 196.59261\n",
      "Current_batch_loss: 108.03214\n",
      "Current_batch_loss: 144.02756\n",
      "Current_batch_loss: 218.07326\n",
      "Current_batch_loss: 97.8701\n",
      "Current_batch_loss: 139.26358\n",
      "Current_batch_loss: 150.78145\n",
      "Current_batch_loss: 119.1021\n",
      "Current_batch_loss: 193.68126\n",
      "Current_batch_loss: 154.87317\n",
      "Current_batch_loss: 137.88104\n",
      "Current_batch_loss: 134.74783\n",
      "Current_batch_loss: 146.48586\n",
      "Current_batch_loss: 162.44279\n",
      "Current_batch_loss: 184.72601\n",
      "Current_batch_loss: 147.22607\n",
      "Current_batch_loss: 60.71016\n",
      "Current_batch_loss: 146.57275\n",
      "Current_batch_loss: 149.92261\n",
      "Current_batch_loss: 165.16249\n",
      "Current_batch_loss: 153.82193\n",
      "Current_batch_loss: 140.6763\n",
      "Current_batch_loss: 70.01698\n",
      "Current_batch_loss: 167.70842\n",
      "Current_batch_loss: 102.61449\n",
      "Current_batch_loss: 163.00897\n",
      "Current_batch_loss: 101.51439\n",
      "Current_batch_loss: 128.45166\n",
      "Current_batch_loss: 81.81644\n",
      "Current_batch_loss: 119.25922\n",
      "Current_batch_loss: 154.50577\n",
      "Current_batch_loss: 104.07899\n",
      "Current_batch_loss: 139.54117\n",
      "Current_batch_loss: 150.51526\n",
      "Current_batch_loss: 123.80151\n",
      "Current_batch_loss: 181.59731\n",
      "Current_batch_loss: 103.71267\n",
      "Current_batch_loss: 97.72543\n",
      "Current_batch_loss: 156.70885\n",
      "Current_batch_loss: 124.11204\n",
      "Current_batch_loss: 172.03954\n",
      "Current_batch_loss: 121.56003\n",
      "Current_batch_loss: 99.06052\n",
      "Current_batch_loss: 158.36421\n",
      "Current_batch_loss: 102.61512\n",
      "Current_batch_loss: 85.69357\n",
      "Current_batch_loss: 140.14604\n",
      "Current_batch_loss: 161.58382\n",
      "Current_batch_loss: 140.81195\n",
      "Current_batch_loss: 35.92173\n",
      "Current_batch_loss: 158.77299\n",
      "Current_batch_loss: 143.1757\n",
      "Current_batch_loss: 145.95044\n",
      "Current_batch_loss: 103.88418\n",
      "Current_batch_loss: 101.27773\n",
      "Current_batch_loss: 85.40677\n",
      "Current_batch_loss: 125.72866\n",
      "Current_batch_loss: 111.08483\n",
      "Current_batch_loss: 101.22701\n",
      "Current_batch_loss: 114.57331\n",
      "Current_batch_loss: 169.22675\n",
      "Current_batch_loss: 108.88975\n",
      "Current_batch_loss: 126.05214\n",
      "Current_batch_loss: 124.23866\n",
      "Current_batch_loss: 131.17944\n",
      "Current_batch_loss: 174.08698\n",
      "Current_batch_loss: 81.55522\n",
      "Current_batch_loss: 187.91605\n",
      "Current_batch_loss: 106.24382\n",
      "Current_batch_loss: 153.36385\n",
      "Current_batch_loss: 108.48215\n",
      "Current_batch_loss: 53.70778\n",
      "Current_batch_loss: 110.57179\n",
      "Current_batch_loss: 153.69325\n",
      "Current_batch_loss: 100.5731\n",
      "Current_batch_loss: 105.13494\n",
      "Current_batch_loss: 132.38516\n",
      "Current_batch_loss: 140.44153\n",
      "Current_batch_loss: 120.04775\n",
      "Current_batch_loss: 154.45229\n",
      "Current_batch_loss: 108.01231\n",
      "Current_batch_loss: 111.2937\n",
      "Current_batch_loss: 155.71786\n",
      "Current_batch_loss: 161.70058\n",
      "Current_batch_loss: 74.30357\n",
      "Current_batch_loss: 147.23093\n",
      "Current_batch_loss: 126.07088\n",
      "Current_batch_loss: 149.43465\n",
      "Current_batch_loss: 166.10101\n",
      "Current_batch_loss: 157.42259\n",
      "Current_batch_loss: 84.8931\n",
      "Current_batch_loss: 75.57529\n",
      "Current_batch_loss: 162.12747\n",
      "Current_batch_loss: 155.63516\n",
      "Current_batch_loss: 166.2762\n",
      "Current_batch_loss: 104.63343\n",
      "Current_batch_loss: 123.67101\n",
      "Current_batch_loss: 72.48936\n",
      "Current_batch_loss: 145.62703\n",
      "Current_batch_loss: 61.48507\n",
      "Current_batch_loss: 156.53645\n",
      "Current_batch_loss: 77.40184\n",
      "Current_batch_loss: 138.74187\n",
      "Current_batch_loss: 135.52348\n",
      "Current_batch_loss: 103.61503\n",
      "Current_batch_loss: 106.67078\n",
      "Current_batch_loss: 126.13965\n",
      "Current_batch_loss: 69.0874\n",
      "Current_batch_loss: 106.20378\n",
      "Current_batch_loss: 99.69903\n",
      "Current_batch_loss: 94.54046\n",
      "Current_batch_loss: 97.11329\n",
      "Current_batch_loss: 66.46476\n",
      "Current_batch_loss: 102.29989\n",
      "Current_batch_loss: 92.09881\n",
      "Current_batch_loss: 164.18922\n",
      "Current_batch_loss: 130.38011\n",
      "Current_batch_loss: 161.36067\n",
      "Current_batch_loss: 59.26776\n",
      "Current_batch_loss: 165.02879\n",
      "Current_batch_loss: 103.17529\n",
      "Current_batch_loss: 148.95554\n",
      "Current_batch_loss: 41.09551\n",
      "Current_batch_loss: 87.99964\n",
      "Current_batch_loss: 80.01405\n",
      "Current_batch_loss: 81.82998\n",
      "Current_batch_loss: 97.97733\n",
      "Current_batch_loss: 101.59194\n",
      "Current_batch_loss: 63.33576\n",
      "Current_batch_loss: 119.31585\n",
      "Current_batch_loss: 81.49409\n",
      "Current_batch_loss: 132.8522\n",
      "Current_batch_loss: 114.76756\n",
      "Current_batch_loss: 100.95058\n",
      "Current_batch_loss: 84.16987\n",
      "Current_batch_loss: 111.05367\n",
      "Current_batch_loss: 86.8081\n",
      "Current_batch_loss: 87.20692\n",
      "Current_batch_loss: 95.96592\n",
      "Current_batch_loss: 99.81391\n",
      "Current_batch_loss: 39.51171\n",
      "Current_batch_loss: 130.47211\n",
      "Current_batch_loss: 49.38865\n",
      "Current_batch_loss: 89.82133\n",
      "Current_batch_loss: 59.34076\n",
      "Current_batch_loss: 91.64127\n",
      "Current_batch_loss: 138.47658\n",
      "Current_batch_loss: 44.51982\n",
      "Current_batch_loss: 141.86746\n",
      "Current_batch_loss: 74.95984\n",
      "Current_batch_loss: 53.13417\n",
      "Current_batch_loss: 138.50038\n",
      "Current_batch_loss: 108.68136\n",
      "Current_batch_loss: 81.48586\n",
      "Current_batch_loss: 95.88556\n",
      "Current_batch_loss: 95.35205\n",
      "Current_batch_loss: 84.30801\n",
      "Current_batch_loss: 146.90031\n",
      "Current_batch_loss: 171.12672\n",
      "Current_batch_loss: 64.99524\n",
      "Current_batch_loss: 78.14307\n",
      "Current_batch_loss: 81.93839\n",
      "Current_batch_loss: 146.46965\n",
      "Current_batch_loss: 60.93083\n",
      "Current_batch_loss: 109.52734\n",
      "Current_batch_loss: 79.78053\n",
      "Current_batch_loss: 84.36919\n",
      "Current_batch_loss: 101.78133\n",
      "Current_batch_loss: 159.65298\n",
      "Current_batch_loss: 71.09089\n",
      "Current_batch_loss: 166.02187\n",
      "Current_batch_loss: 99.90546\n",
      "Current_batch_loss: 100.92936\n",
      "Current_batch_loss: 96.73908\n",
      "Current_batch_loss: 42.80642\n",
      "Current_batch_loss: 174.67407\n",
      "Current_batch_loss: 144.19257\n",
      "Current_batch_loss: 158.17899\n",
      "Current_batch_loss: 89.01133\n",
      "Current_batch_loss: 27.13453\n",
      "Current_batch_loss: 74.91235\n",
      "Current_batch_loss: 100.64124\n",
      "Current_batch_loss: 73.41708\n",
      "Current_batch_loss: 117.84047\n",
      "Current_batch_loss: 40.67582\n",
      "Current_batch_loss: 87.33252\n",
      "Current_batch_loss: 108.59673\n",
      "Current_batch_loss: 62.13994\n",
      "Current_batch_loss: 25.48401\n",
      "Current_batch_loss: 40.37482\n",
      "Current_batch_loss: 121.09988\n",
      "Current_batch_loss: 68.68633\n",
      "Current_batch_loss: 104.172\n",
      "Current_batch_loss: 106.2379\n",
      "Current_batch_loss: 76.04241\n",
      "Current_batch_loss: 99.33324\n",
      "Current_batch_loss: 51.66471\n",
      "Current_batch_loss: 33.96448\n",
      "Current_batch_loss: 70.54459\n",
      "Current_batch_loss: 88.748\n",
      "Current_batch_loss: 49.48393\n",
      "Current_batch_loss: 91.16323\n",
      "Current_batch_loss: 80.97828\n",
      "Current_batch_loss: 73.80478\n",
      "Current_batch_loss: 31.73344\n",
      "Current_batch_loss: 65.54971\n",
      "Current_batch_loss: 93.68211\n",
      "Current_batch_loss: 106.52453\n",
      "Current_batch_loss: 82.22822\n",
      "Current_batch_loss: 109.80484\n",
      "Current_batch_loss: 52.50752\n",
      "Current_batch_loss: 80.08073\n",
      "Current_batch_loss: 44.78308\n",
      "Current_batch_loss: 86.95914\n",
      "Current_batch_loss: 177.07956\n",
      "Current_batch_loss: 45.33227\n",
      "Current_batch_loss: 91.21025\n",
      "Current_batch_loss: 46.46975\n",
      "Current_batch_loss: 56.35939\n",
      "Current_batch_loss: 58.21171\n",
      "Current_batch_loss: 25.76138\n",
      "Current_batch_loss: 58.46983\n",
      "Current_batch_loss: 74.04009\n",
      "Current_batch_loss: 91.58207\n",
      "Current_batch_loss: 52.29696\n",
      "Current_batch_loss: 123.04342\n",
      "Current_batch_loss: 35.73353\n",
      "Current_batch_loss: 82.55962\n",
      "Current_batch_loss: 51.51735\n",
      "Current_batch_loss: 78.47703\n",
      "Current_batch_loss: 76.62335\n",
      "Current_batch_loss: 50.36509\n",
      "Current_batch_loss: 95.5155\n",
      "Current_batch_loss: 62.39623\n",
      "Current_batch_loss: 68.06163\n",
      "Current_batch_loss: 103.07007\n",
      "Current_batch_loss: 37.33417\n",
      "Current_batch_loss: 48.76767\n",
      "Current_batch_loss: 31.23351\n",
      "Current_batch_loss: 90.76054\n",
      "Current_batch_loss: 147.61386\n",
      "Current_batch_loss: 60.5994\n",
      "Current_batch_loss: 46.56149\n",
      "Current_batch_loss: 33.47324\n",
      "Current_batch_loss: 29.81006\n",
      "Current_batch_loss: 31.69872\n",
      "Current_batch_loss: 69.03365\n",
      "Current_batch_loss: 65.0658\n",
      "Current_batch_loss: 96.39405\n",
      "Current_batch_loss: 262.16702\n",
      "Current_batch_loss: 27.76876\n",
      "Current_batch_loss: 48.81872\n",
      "Current_batch_loss: 42.62275\n",
      "Current_batch_loss: 66.09956\n",
      "Current_batch_loss: 75.0257\n",
      "Current_batch_loss: 58.33652\n",
      "Current_batch_loss: 36.18173\n",
      "Current_batch_loss: 70.01611\n",
      "Current_batch_loss: 61.60225\n",
      "Current_batch_loss: 81.04261\n",
      "Current_batch_loss: 72.34772\n",
      "Current_batch_loss: 55.66269\n",
      "Current_batch_loss: 85.0489\n",
      "Current_batch_loss: 28.18459\n",
      "Current_batch_loss: 104.37051\n",
      "Current_batch_loss: 62.37416\n",
      "Current_batch_loss: 53.41222\n",
      "Current_batch_loss: 137.48495\n",
      "Current_batch_loss: 77.31473\n",
      "Current_batch_loss: 75.2748\n",
      "Current_batch_loss: 62.43306\n",
      "Current_batch_loss: 83.82756\n",
      "Current_batch_loss: 127.93392\n",
      "Current_batch_loss: 78.83559\n",
      "Current_batch_loss: 63.58503\n",
      "Current_batch_loss: 33.92326\n",
      "Current_batch_loss: 94.95551\n",
      "Current_batch_loss: 28.79576\n",
      "Current_batch_loss: 62.34891\n",
      "Current_batch_loss: 130.3821\n",
      "Current_batch_loss: 19.07116\n",
      "Current_batch_loss: 83.82545\n",
      "Current_batch_loss: 38.81093\n",
      "Current_batch_loss: 86.36905\n",
      "Current_batch_loss: 38.38781\n",
      "Current_batch_loss: 47.14749\n",
      "Current_batch_loss: 46.81259\n",
      "Current_batch_loss: 64.03763\n",
      "Current_batch_loss: 51.98172\n",
      "Current_batch_loss: 43.77242\n",
      "Current_batch_loss: 71.84125\n",
      "Current_batch_loss: 29.35896\n",
      "Current_batch_loss: 71.53132\n",
      "Current_batch_loss: 88.5393\n",
      "Current_batch_loss: 70.73009\n",
      "Current_batch_loss: 46.2996\n",
      "Current_batch_loss: 112.08057\n",
      "Current_batch_loss: 34.58325\n",
      "Current_batch_loss: 60.44243\n",
      "Current_batch_loss: 43.42269\n",
      "Current_batch_loss: 37.02754\n",
      "Current_batch_loss: 51.94723\n",
      "Current_batch_loss: 63.02196\n",
      "Current_batch_loss: 50.54258\n",
      "Current_batch_loss: 69.43534\n",
      "Current_batch_loss: 32.80635\n",
      "Current_batch_loss: 27.19026\n",
      "Current_batch_loss: 65.27269\n",
      "Current_batch_loss: 23.02206\n",
      "Current_batch_loss: 74.90296\n",
      "Current_batch_loss: 37.43739\n",
      "Current_batch_loss: 43.52052\n",
      "Current_batch_loss: 42.21531\n",
      "Current_batch_loss: 50.66996\n",
      "Current_batch_loss: 78.95772\n",
      "Current_batch_loss: 78.22723\n",
      "Current_batch_loss: 18.24409\n",
      "Current_batch_loss: 31.03886\n",
      "Current_batch_loss: 71.67844\n",
      "Current_batch_loss: 8.56809\n",
      "Current_batch_loss: 36.56961\n",
      "Current_batch_loss: 26.69564\n",
      "Current_batch_loss: 67.38254\n",
      "Current_batch_loss: 38.87776\n",
      "Current_batch_loss: 51.32133\n",
      "Current_batch_loss: 90.42048\n",
      "Current_batch_loss: 49.67957\n",
      "Current_batch_loss: 60.32159\n",
      "Current_batch_loss: 46.13294\n",
      "Current_batch_loss: 37.22145\n",
      "Current_batch_loss: 59.70546\n",
      "Current_batch_loss: 25.00161\n",
      "Current_batch_loss: 56.8953\n",
      "Current_batch_loss: 51.47903\n",
      "Current_batch_loss: 23.48823\n",
      "Current_batch_loss: 38.3911\n",
      "Current_batch_loss: 50.42577\n",
      "Current_batch_loss: 16.10382\n",
      "Current_batch_loss: 29.19332\n",
      "Current_batch_loss: 47.10646\n",
      "Current_batch_loss: 34.80318\n",
      "Current_batch_loss: 103.1226\n",
      "Current_batch_loss: 16.37905\n",
      "Current_batch_loss: 85.67336\n",
      "Current_batch_loss: 46.35311\n",
      "Current_batch_loss: 66.8527\n",
      "Current_batch_loss: 32.21383\n",
      "Current_batch_loss: 54.83077\n",
      "Current_batch_loss: 19.75381\n",
      "Current_batch_loss: 28.41873\n",
      "Current_batch_loss: 54.33902\n",
      "Current_batch_loss: 28.22919\n",
      "Current_batch_loss: 44.15574\n",
      "Current_batch_loss: 18.31776\n",
      "Current_batch_loss: 49.2023\n",
      "Current_batch_loss: 34.18584\n",
      "Current_batch_loss: 23.71851\n",
      "Current_batch_loss: 47.67785\n",
      "Current_batch_loss: 57.8396\n",
      "Current_batch_loss: 78.96623\n",
      "Current_batch_loss: 36.67804\n",
      "Current_batch_loss: 47.07933\n",
      "Current_batch_loss: 16.19666\n",
      "Current_batch_loss: 19.57323\n",
      "Current_batch_loss: 51.42155\n",
      "Current_batch_loss: 34.1547\n",
      "Current_batch_loss: 56.47859\n",
      "Current_batch_loss: 48.41627\n",
      "Current_batch_loss: 32.1138\n",
      "Current_batch_loss: 70.55961\n",
      "Current_batch_loss: 64.3772\n",
      "Current_batch_loss: 32.86086\n",
      "Current_batch_loss: 27.1528\n",
      "Current_batch_loss: 40.33729\n",
      "Current_batch_loss: 52.92081\n",
      "Current_batch_loss: 32.51606\n",
      "Current_batch_loss: 40.30386\n",
      "Current_batch_loss: 35.77411\n",
      "Current_batch_loss: 65.1564\n",
      "Current_batch_loss: 30.29472\n",
      "Current_batch_loss: 39.41436\n",
      "Current_batch_loss: 36.11425\n",
      "Current_batch_loss: 40.00667\n",
      "Current_batch_loss: 22.48624\n",
      "Current_batch_loss: 63.74617\n",
      "Current_batch_loss: 53.26333\n",
      "Current_batch_loss: 23.95148\n",
      "Current_batch_loss: 8.38392\n",
      "Current_batch_loss: 38.05934\n",
      "Current_batch_loss: 15.04058\n",
      "Current_batch_loss: 20.30728\n",
      "Current_batch_loss: 26.15503\n",
      "Current_batch_loss: 56.5342\n",
      "Current_batch_loss: 45.42706\n",
      "Current_batch_loss: 25.31787\n",
      "Current_batch_loss: 10.48508\n",
      "Current_batch_loss: 65.16016\n",
      "Current_batch_loss: 7.85872\n",
      "Current_batch_loss: 29.90632\n",
      "Current_batch_loss: 48.98808\n",
      "Current_batch_loss: 28.15411\n",
      "Current_batch_loss: 27.73841\n",
      "Current_batch_loss: 23.65081\n",
      "Current_batch_loss: 46.95898\n",
      "Current_batch_loss: 11.24307\n",
      "Current_batch_loss: 81.44931\n",
      "Current_batch_loss: 42.55926\n",
      "Current_batch_loss: 0.01279\n",
      "Current_batch_loss: 17.92188\n",
      "Current_batch_loss: 44.19125\n",
      "Current_batch_loss: 31.27579\n",
      "Current_batch_loss: 10.81676\n",
      "Current_batch_loss: 9.92558\n",
      "Current_batch_loss: 11.4583\n",
      "Current_batch_loss: 68.21899\n",
      "Current_batch_loss: 40.00667\n",
      "Current_batch_loss: 26.8797\n",
      "Current_batch_loss: 20.00245\n",
      "Current_batch_loss: 30.67766\n",
      "Current_batch_loss: 22.48407\n",
      "Current_batch_loss: 33.3939\n",
      "Current_batch_loss: 33.82997\n",
      "Current_batch_loss: 15.73638\n",
      "Current_batch_loss: 89.32404\n",
      "Current_batch_loss: 61.28839\n",
      "Current_batch_loss: 35.3601\n",
      "Current_batch_loss: 21.73347\n",
      "Current_batch_loss: 0.01074\n",
      "Current_batch_loss: 49.18824\n",
      "Current_batch_loss: 43.73102\n",
      "Current_batch_loss: 54.21379\n",
      "Current_batch_loss: 0.01174\n",
      "Current_batch_loss: 5.95718\n",
      "Current_batch_loss: 47.67791\n",
      "Current_batch_loss: 19.65723\n",
      "Current_batch_loss: 0.00993\n",
      "Current_batch_loss: 52.00455\n",
      "Current_batch_loss: 72.23927\n",
      "Current_batch_loss: 45.07299\n",
      "Current_batch_loss: 33.45762\n",
      "Current_batch_loss: 29.55383\n",
      "Current_batch_loss: 76.49191\n",
      "Current_batch_loss: 18.8363\n",
      "Current_batch_loss: 21.18843\n",
      "Current_batch_loss: 21.34601\n",
      "Current_batch_loss: 32.19888\n",
      "Current_batch_loss: 11.15923\n",
      "Current_batch_loss: 50.80989\n",
      "Current_batch_loss: 26.76629\n",
      "Current_batch_loss: 31.30472\n",
      "Current_batch_loss: 7.69736\n",
      "Current_batch_loss: 7.69173\n",
      "Current_batch_loss: 17.84119\n",
      "Current_batch_loss: 81.58904\n",
      "Current_batch_loss: 19.99742\n",
      "Current_batch_loss: 14.88439\n",
      "Current_batch_loss: 12.18678\n",
      "Current_batch_loss: 30.30513\n",
      "Current_batch_loss: 37.17847\n",
      "Current_batch_loss: 33.32202\n",
      "Current_batch_loss: 12.20503\n",
      "Current_batch_loss: 43.26427\n",
      "Current_batch_loss: 63.62429\n",
      "Current_batch_loss: 23.49639\n",
      "Current_batch_loss: 17.64052\n",
      "Current_batch_loss: 43.93714\n",
      "Current_batch_loss: 32.01651\n",
      "Current_batch_loss: 6.95731\n",
      "Current_batch_loss: 58.78783\n",
      "Current_batch_loss: 22.0654\n",
      "Current_batch_loss: 35.57528\n",
      "Current_batch_loss: 30.26008\n",
      "Current_batch_loss: 51.73753\n",
      "Current_batch_loss: 36.08942\n",
      "Current_batch_loss: 47.88936\n",
      "Current_batch_loss: 8.2382\n",
      "Current_batch_loss: 0.01353\n",
      "Current_batch_loss: 54.3301\n",
      "Current_batch_loss: 15.167\n",
      "Current_batch_loss: 16.60171\n",
      "Current_batch_loss: 36.00971\n",
      "Current_batch_loss: 28.53847\n",
      "Current_batch_loss: 27.53759\n",
      "Current_batch_loss: 40.26015\n",
      "Current_batch_loss: 0.0095\n",
      "Current_batch_loss: 17.97533\n",
      "Current_batch_loss: 8.35435\n",
      "Current_batch_loss: 28.55549\n",
      "Current_batch_loss: 35.96924\n",
      "Current_batch_loss: 17.24448\n",
      "Current_batch_loss: 9.72067\n",
      "Current_batch_loss: 22.76058\n",
      "Current_batch_loss: 9.15744\n",
      "Current_batch_loss: 35.0969\n",
      "Current_batch_loss: 20.48912\n",
      "Current_batch_loss: 28.30219\n",
      "Current_batch_loss: 28.33262\n",
      "Current_batch_loss: 17.17991\n",
      "Current_batch_loss: 38.44346\n",
      "Current_batch_loss: 22.76847\n",
      "Current_batch_loss: 34.35187\n",
      "Current_batch_loss: 37.84117\n",
      "Current_batch_loss: 34.68007\n",
      "Current_batch_loss: 7.93962\n",
      "Current_batch_loss: 19.53879\n",
      "Current_batch_loss: 17.53589\n",
      "Current_batch_loss: 21.75281\n",
      "Current_batch_loss: 0.01254\n",
      "Current_batch_loss: 26.32359\n",
      "Current_batch_loss: 23.44341\n",
      "Current_batch_loss: 25.89514\n",
      "Current_batch_loss: 25.5645\n",
      "Current_batch_loss: 0.01527\n",
      "Current_batch_loss: 50.62587\n",
      "Current_batch_loss: 32.70767\n",
      "Current_batch_loss: 22.89012\n",
      "Current_batch_loss: 39.76943\n",
      "Current_batch_loss: 19.64116\n",
      "Current_batch_loss: 27.98148\n",
      "Current_batch_loss: 28.12505\n",
      "Current_batch_loss: 36.73541\n",
      "Current_batch_loss: 31.02397\n",
      "Current_batch_loss: 11.94201\n",
      "Current_batch_loss: 8.18082\n",
      "Current_batch_loss: 43.8208\n",
      "Current_batch_loss: 47.01591\n",
      "Current_batch_loss: 65.84317\n",
      "Current_batch_loss: 22.04829\n",
      "Current_batch_loss: 8.25235\n",
      "Current_batch_loss: 19.43125\n",
      "Current_batch_loss: 25.87689\n",
      "Current_batch_loss: 42.83678\n",
      "Current_batch_loss: 27.09634\n",
      "Current_batch_loss: 0.01029\n",
      "Current_batch_loss: 36.09361\n",
      "Current_batch_loss: 9.36957\n",
      "Current_batch_loss: 47.66979\n",
      "Current_batch_loss: 31.29552\n",
      "Current_batch_loss: 11.0008\n",
      "Current_batch_loss: 47.93618\n",
      "Current_batch_loss: 18.75407\n",
      "Current_batch_loss: 24.16722\n",
      "Current_batch_loss: 5.88107\n",
      "Current_batch_loss: 33.37263\n",
      "Current_batch_loss: 11.40809\n",
      "Current_batch_loss: 21.35018\n",
      "Current_batch_loss: 8.45298\n",
      "Current_batch_loss: 0.01463\n",
      "Current_batch_loss: 7.57118\n",
      "Current_batch_loss: 7.92392\n",
      "Current_batch_loss: 36.59681\n",
      "Current_batch_loss: 32.91354\n",
      "Current_batch_loss: 16.29522\n",
      "Current_batch_loss: 30.58529\n",
      "Current_batch_loss: 6.77352\n",
      "Current_batch_loss: 19.63732\n",
      "Current_batch_loss: 0.01164\n",
      "Current_batch_loss: 9.461\n",
      "Current_batch_loss: 0.00742\n",
      "Current_batch_loss: 0.012\n",
      "Current_batch_loss: 24.86594\n",
      "Current_batch_loss: 26.27543\n",
      "Current_batch_loss: 6.44164\n",
      "Current_batch_loss: 9.6167\n",
      "Current_batch_loss: 17.44357\n",
      "Current_batch_loss: 13.25403\n",
      "Current_batch_loss: 10.85182\n",
      "Current_batch_loss: 9.47384\n",
      "Current_batch_loss: 14.18257\n",
      "Current_batch_loss: 9.0538\n",
      "Current_batch_loss: 22.3848\n",
      "Current_batch_loss: 6.67999\n",
      "Current_batch_loss: 0.01197\n",
      "Current_batch_loss: 0.0121\n",
      "Current_batch_loss: 7.93126\n",
      "Current_batch_loss: 21.5627\n",
      "Current_batch_loss: 25.65804\n",
      "Current_batch_loss: 12.84074\n",
      "Current_batch_loss: 24.04187\n",
      "Current_batch_loss: 19.16989\n",
      "Current_batch_loss: 7.15792\n",
      "Current_batch_loss: 19.06244\n",
      "Current_batch_loss: 49.3704\n",
      "Current_batch_loss: 8.51463\n",
      "Current_batch_loss: 35.20577\n",
      "Current_batch_loss: 0.0115\n",
      "Current_batch_loss: 0.01051\n",
      "Current_batch_loss: 0.01015\n",
      "Current_batch_loss: 21.86594\n",
      "Current_batch_loss: 9.55023\n",
      "Current_batch_loss: 44.50072\n",
      "Current_batch_loss: 31.44569\n",
      "Current_batch_loss: 0.01267\n",
      "Current_batch_loss: 30.51315\n",
      "Current_batch_loss: 26.23408\n",
      "Current_batch_loss: 10.72845\n",
      "Current_batch_loss: 22.24745\n",
      "Current_batch_loss: 35.13535\n",
      "Current_batch_loss: 24.43901\n",
      "Current_batch_loss: 24.96971\n",
      "Current_batch_loss: 54.18837\n",
      "Current_batch_loss: 6.44172\n",
      "Current_batch_loss: 0.01169\n",
      "Current_batch_loss: 60.70714\n",
      "Current_batch_loss: 12.69177\n",
      "Current_batch_loss: 17.00302\n",
      "Current_batch_loss: 8.07661\n",
      "Current_batch_loss: 10.78944\n",
      "Current_batch_loss: 14.33955\n",
      "Current_batch_loss: 24.68533\n",
      "Current_batch_loss: 0.01111\n",
      "Current_batch_loss: 39.63235\n",
      "Current_batch_loss: 16.98939\n",
      "Current_batch_loss: 9.37347\n",
      "Current_batch_loss: 12.80266\n",
      "Current_batch_loss: 7.94807\n",
      "Current_batch_loss: 8.49787\n",
      "Current_batch_loss: 19.65232\n",
      "Current_batch_loss: 19.47117\n",
      "Current_batch_loss: 7.99408\n",
      "Current_batch_loss: 9.21297\n",
      "Current_batch_loss: 24.09205\n",
      "Current_batch_loss: 10.46957\n",
      "Current_batch_loss: 7.17644\n",
      "Current_batch_loss: 0.0102\n",
      "Current_batch_loss: 18.62431\n",
      "Current_batch_loss: 14.79574\n",
      "Current_batch_loss: 0.01289\n",
      "Current_batch_loss: 0.00873\n",
      "Current_batch_loss: 16.3257\n",
      "Current_batch_loss: 7.1374\n",
      "Current_batch_loss: 25.50665\n",
      "Current_batch_loss: 24.37709\n",
      "Current_batch_loss: 8.20945\n",
      "Current_batch_loss: 0.0084\n",
      "Current_batch_loss: 0.00952\n",
      "Current_batch_loss: 0.0071\n",
      "Current_batch_loss: 6.83269\n",
      "Current_batch_loss: 0.00868\n",
      "Current_batch_loss: 8.0508\n",
      "Current_batch_loss: 0.0119\n",
      "Current_batch_loss: 12.02955\n",
      "Current_batch_loss: 9.41425\n",
      "Current_batch_loss: 0.01046\n",
      "Current_batch_loss: 0.01807\n",
      "Current_batch_loss: 11.48014\n",
      "Current_batch_loss: 9.15486\n",
      "Current_batch_loss: 10.54458\n",
      "Current_batch_loss: 13.31736\n",
      "Current_batch_loss: 0.01567\n",
      "Current_batch_loss: 8.81468\n",
      "Current_batch_loss: 0.0111\n",
      "Current_batch_loss: 12.60432\n",
      "Current_batch_loss: 12.46301\n",
      "Current_batch_loss: 0.00834\n",
      "Current_batch_loss: 0.00845\n",
      "Current_batch_loss: 22.775\n",
      "Current_batch_loss: 13.99385\n",
      "Current_batch_loss: 26.67924\n",
      "Current_batch_loss: 52.07365\n",
      "Current_batch_loss: 22.27716\n",
      "Current_batch_loss: 15.52767\n",
      "Current_batch_loss: 11.39838\n",
      "Current_batch_loss: 0.01009\n",
      "Current_batch_loss: 7.57083\n",
      "Current_batch_loss: 29.08708\n",
      "Current_batch_loss: 18.61042\n",
      "Current_batch_loss: 0.01014\n",
      "Current_batch_loss: 9.57765\n",
      "Current_batch_loss: 16.76018\n",
      "Current_batch_loss: 10.20487\n",
      "Current_batch_loss: 15.42522\n",
      "Current_batch_loss: 11.92049\n",
      "Current_batch_loss: 0.00725\n",
      "Current_batch_loss: 0.01036\n",
      "Current_batch_loss: 8.47025\n",
      "Current_batch_loss: 9.72067\n",
      "Current_batch_loss: 28.09926\n",
      "Current_batch_loss: 0.00965\n",
      "Current_batch_loss: 0.00701\n",
      "Current_batch_loss: 0.00886\n",
      "Current_batch_loss: 7.14249\n",
      "Current_batch_loss: 8.95759\n",
      "Current_batch_loss: 0.01512\n",
      "Current_batch_loss: 0.00907\n",
      "Current_batch_loss: 0.00908\n",
      "Current_batch_loss: 21.7837\n",
      "Current_batch_loss: 21.15214\n",
      "Current_batch_loss: 25.27686\n",
      "Current_batch_loss: 0.00794\n",
      "Current_batch_loss: 0.00662\n",
      "Current_batch_loss: 0.00848\n",
      "Current_batch_loss: 15.48282\n",
      "Current_batch_loss: 0.01133\n",
      "Current_batch_loss: 0.00955\n",
      "Current_batch_loss: 5.42338\n",
      "Current_batch_loss: 28.65848\n",
      "Current_batch_loss: 0.00968\n",
      "Current_batch_loss: 9.69961\n",
      "Current_batch_loss: 0.0076\n",
      "Current_batch_loss: 0.01257\n",
      "Current_batch_loss: 0.01026\n",
      "Current_batch_loss: 5.98465\n",
      "Current_batch_loss: 25.4032\n",
      "Current_batch_loss: 0.0076\n",
      "Current_batch_loss: 16.38726\n",
      "Current_batch_loss: 0.01169\n",
      "Current_batch_loss: 10.87589\n",
      "Current_batch_loss: 9.78131\n",
      "Current_batch_loss: 13.9384\n",
      "Current_batch_loss: 0.01272\n",
      "Current_batch_loss: 0.00699\n",
      "Current_batch_loss: 0.00887\n",
      "Current_batch_loss: 12.75393\n",
      "Current_batch_loss: 6.37135\n",
      "Current_batch_loss: 11.92668\n",
      "Current_batch_loss: 0.00961\n",
      "Current_batch_loss: 20.85815\n",
      "Current_batch_loss: 21.42739\n",
      "Current_batch_loss: 17.82779\n",
      "Current_batch_loss: 13.8762\n",
      "Current_batch_loss: 0.01198\n",
      "Current_batch_loss: 0.00882\n",
      "Current_batch_loss: 11.56864\n",
      "Current_batch_loss: 10.81347\n",
      "Current_batch_loss: 0.01159\n",
      "Current_batch_loss: 15.22989\n",
      "Current_batch_loss: 14.17434\n",
      "Current_batch_loss: 11.99386\n",
      "Current_batch_loss: 34.93633\n",
      "Current_batch_loss: 0.01044\n",
      "Current_batch_loss: 7.71152\n",
      "Current_batch_loss: 0.00869\n",
      "Current_batch_loss: 8.83967\n",
      "Current_batch_loss: 15.48972\n",
      "Current_batch_loss: 5.64803\n",
      "Current_batch_loss: 22.05368\n",
      "Current_batch_loss: 11.2515\n",
      "Current_batch_loss: 0.00803\n",
      "Current_batch_loss: 24.67582\n",
      "Current_batch_loss: 6.02182\n",
      "Current_batch_loss: 33.69741\n",
      "Current_batch_loss: 14.99073\n",
      "Current_batch_loss: 23.22833\n",
      "Current_batch_loss: 12.21572\n",
      "Current_batch_loss: 17.97453\n",
      "Current_batch_loss: 16.64055\n",
      "Current_batch_loss: 0.01192\n",
      "Current_batch_loss: 13.12454\n",
      "Current_batch_loss: 7.56967\n",
      "Current_batch_loss: 0.00768\n",
      "Current_batch_loss: 0.00673\n",
      "Current_batch_loss: 0.0081\n",
      "Current_batch_loss: 10.77494\n",
      "Current_batch_loss: 10.16459\n",
      "Current_batch_loss: 11.95095\n",
      "Current_batch_loss: 0.00949\n",
      "Current_batch_loss: 11.42486\n",
      "Current_batch_loss: 0.01304\n",
      "Current_batch_loss: 0.00983\n",
      "Current_batch_loss: 0.0072\n",
      "Current_batch_loss: 8.22913\n",
      "Current_batch_loss: 0.01055\n",
      "Current_batch_loss: 0.00916\n",
      "Current_batch_loss: 9.39912\n",
      "Current_batch_loss: 0.01205\n",
      "Current_batch_loss: 0.01166\n",
      "Current_batch_loss: 19.74778\n",
      "Current_batch_loss: 21.47431\n",
      "Current_batch_loss: 0.00697\n",
      "Current_batch_loss: 0.01006\n",
      "Current_batch_loss: 12.55579\n",
      "Current_batch_loss: 23.69186\n",
      "Current_batch_loss: 0.01007\n",
      "Current_batch_loss: 11.927\n",
      "Current_batch_loss: 18.69098\n",
      "Current_batch_loss: 0.00915\n",
      "Current_batch_loss: 24.38203\n",
      "Current_batch_loss: 10.76486\n",
      "Current_batch_loss: 18.67704\n",
      "Current_batch_loss: 0.0114\n",
      "Current_batch_loss: 0.00763\n",
      "Current_batch_loss: 0.00936\n",
      "Current_batch_loss: 14.89604\n",
      "Current_batch_loss: 0.00686\n",
      "Current_batch_loss: 10.05136\n",
      "Current_batch_loss: 0.01078\n",
      "Current_batch_loss: 0.00917\n",
      "Current_batch_loss: 11.3115\n",
      "Current_batch_loss: 8.4058\n",
      "Current_batch_loss: 9.01565\n",
      "Current_batch_loss: 16.84965\n",
      "Current_batch_loss: 0.00954\n",
      "Current_batch_loss: 14.42976\n",
      "Current_batch_loss: 20.09769\n",
      "Current_batch_loss: 9.18275\n",
      "Current_batch_loss: 11.51581\n",
      "Current_batch_loss: 0.01078\n",
      "Current_batch_loss: 8.89044\n",
      "Current_batch_loss: 0.01005\n",
      "Current_batch_loss: 0.00893\n",
      "Current_batch_loss: 9.97361\n",
      "Current_batch_loss: 21.46042\n",
      "Current_batch_loss: 0.01004\n",
      "Current_batch_loss: 10.48079\n",
      "Current_batch_loss: 0.01127\n",
      "Current_batch_loss: 0.00738\n",
      "Current_batch_loss: 0.00763\n",
      "Current_batch_loss: 0.00736\n",
      "Current_batch_loss: 0.00831\n",
      "Current_batch_loss: 0.00765\n",
      "Current_batch_loss: 0.00775\n",
      "Current_batch_loss: 47.6633\n",
      "Current_batch_loss: 0.00807\n",
      "Current_batch_loss: 0.00912\n",
      "Current_batch_loss: 0.01047\n",
      "Current_batch_loss: 0.00914\n",
      "Current_batch_loss: 7.74602\n",
      "Current_batch_loss: 18.63064\n",
      "Current_batch_loss: 8.16783\n",
      "Current_batch_loss: 0.00833\n",
      "Current_batch_loss: 24.65919\n",
      "Current_batch_loss: 0.00659\n",
      "Current_batch_loss: 12.39182\n",
      "Current_batch_loss: 16.53852\n",
      "Current_batch_loss: 11.04036\n",
      "Current_batch_loss: 0.00891\n",
      "Current_batch_loss: 0.00493\n",
      "Current_batch_loss: 0.00641\n",
      "Current_batch_loss: 0.00853\n",
      "Current_batch_loss: 0.00878\n",
      "Current_batch_loss: 17.03071\n",
      "Current_batch_loss: 0.01075\n",
      "Current_batch_loss: 31.12682\n",
      "Current_batch_loss: 0.00884\n",
      "Current_batch_loss: 11.06025\n",
      "Current_batch_loss: 6.60029\n",
      "Current_batch_loss: 0.00788\n",
      "Current_batch_loss: 0.00805\n",
      "Current_batch_loss: 11.75058\n",
      "Current_batch_loss: 8.2512\n",
      "Current_batch_loss: 0.00561\n",
      "Current_batch_loss: 7.99234\n",
      "Current_batch_loss: 12.04438\n",
      "Current_batch_loss: 0.00501\n",
      "Current_batch_loss: 8.70598\n",
      "Current_batch_loss: 0.00757\n",
      "Current_batch_loss: 0.00807\n",
      "Current_batch_loss: 0.00809\n",
      "Current_batch_loss: 8.61892\n",
      "Current_batch_loss: 6.89685\n",
      "Current_batch_loss: 0.00976\n",
      "Current_batch_loss: 0.0081\n",
      "Current_batch_loss: 15.62705\n",
      "Current_batch_loss: 0.00973\n",
      "Current_batch_loss: 12.19904\n",
      "Current_batch_loss: 0.00902\n",
      "Current_batch_loss: 0.00642\n",
      "Current_batch_loss: 0.0084\n",
      "Current_batch_loss: 12.9377\n",
      "Current_batch_loss: 9.03345\n",
      "Current_batch_loss: 0.00646\n",
      "Current_batch_loss: 0.00958\n",
      "Current_batch_loss: 0.01182\n",
      "Current_batch_loss: 0.00816\n",
      "Current_batch_loss: 10.41515\n",
      "Current_batch_loss: 9.89946\n",
      "Current_batch_loss: 0.00817\n",
      "Current_batch_loss: 0.00808\n",
      "Current_batch_loss: 0.00897\n",
      "Current_batch_loss: 0.00768\n",
      "Current_batch_loss: 0.00766\n",
      "Current_batch_loss: 21.93501\n",
      "Current_batch_loss: 10.85485\n",
      "Current_batch_loss: 11.0796\n",
      "Current_batch_loss: 0.00547\n",
      "Current_batch_loss: 0.00654\n",
      "Current_batch_loss: 0.00994\n",
      "Current_batch_loss: 9.00208\n",
      "Current_batch_loss: 0.00574\n",
      "Current_batch_loss: 0.00723\n",
      "Current_batch_loss: 11.95238\n",
      "Current_batch_loss: 0.0063\n",
      "Current_batch_loss: 10.67981\n",
      "Current_batch_loss: 0.01249\n",
      "Current_batch_loss: 0.0099\n",
      "Current_batch_loss: 4.50691\n",
      "Current_batch_loss: 0.00679\n",
      "Current_batch_loss: 0.01065\n",
      "Current_batch_loss: 0.00922\n",
      "Current_batch_loss: 6.03352\n",
      "Current_batch_loss: 0.00885\n",
      "Current_batch_loss: 0.01065\n",
      "Current_batch_loss: 0.00471\n",
      "Current_batch_loss: 0.0052\n",
      "Current_batch_loss: 0.00484\n",
      "Current_batch_loss: 9.98074\n",
      "Current_batch_loss: 0.01636\n",
      "Current_batch_loss: 15.02829\n",
      "Current_batch_loss: 0.00929\n",
      "Current_batch_loss: 24.81196\n",
      "Current_batch_loss: 0.00458\n",
      "Current_batch_loss: 0.00834\n",
      "Current_batch_loss: 12.98753\n",
      "Current_batch_loss: 0.01239\n",
      "Current_batch_loss: 0.01292\n",
      "Current_batch_loss: 14.70978\n",
      "Current_batch_loss: 0.00826\n",
      "Current_batch_loss: 15.18548\n",
      "Current_batch_loss: 12.25844\n",
      "Current_batch_loss: 0.00678\n",
      "Current_batch_loss: 0.00609\n",
      "Current_batch_loss: 0.00752\n",
      "Current_batch_loss: 6.81696\n",
      "Current_batch_loss: 10.86932\n",
      "Current_batch_loss: 0.00643\n",
      "Current_batch_loss: 0.00872\n",
      "Current_batch_loss: 0.01201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [6:01:40<54:14:49, 7232.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_batch_loss: 0.00748\n",
      "Epoch #3 | Loss: 297.57196\n",
      "Current_batch_loss: 1040.00598\n",
      "Current_batch_loss: 980.67041\n",
      "Current_batch_loss: 902.35211\n",
      "Current_batch_loss: 996.96838\n",
      "Current_batch_loss: 1046.65271\n",
      "Current_batch_loss: 940.74744\n",
      "Current_batch_loss: 1003.23389\n",
      "Current_batch_loss: 947.36249\n",
      "Current_batch_loss: 895.70532\n",
      "Current_batch_loss: 805.82617\n",
      "Current_batch_loss: 1013.22186\n",
      "Current_batch_loss: 940.10199\n",
      "Current_batch_loss: 908.09497\n",
      "Current_batch_loss: 892.25006\n",
      "Current_batch_loss: 958.10992\n",
      "Current_batch_loss: 947.53131\n",
      "Current_batch_loss: 1030.47852\n",
      "Current_batch_loss: 973.31982\n",
      "Current_batch_loss: 907.24078\n",
      "Current_batch_loss: 941.13812\n",
      "Current_batch_loss: 993.43665\n",
      "Current_batch_loss: 1035.67456\n",
      "Current_batch_loss: 1039.6012\n",
      "Current_batch_loss: 984.9826\n",
      "Current_batch_loss: 1002.70642\n",
      "Current_batch_loss: 960.70184\n",
      "Current_batch_loss: 845.1731\n",
      "Current_batch_loss: 879.52563\n",
      "Current_batch_loss: 928.80267\n",
      "Current_batch_loss: 880.49731\n",
      "Current_batch_loss: 973.80243\n",
      "Current_batch_loss: 909.51941\n",
      "Current_batch_loss: 983.69806\n",
      "Current_batch_loss: 1037.76978\n",
      "Current_batch_loss: 977.66431\n",
      "Current_batch_loss: 898.14679\n",
      "Current_batch_loss: 952.41193\n",
      "Current_batch_loss: 942.82422\n",
      "Current_batch_loss: 798.76434\n",
      "Current_batch_loss: 902.4679\n",
      "Current_batch_loss: 1006.30591\n",
      "Current_batch_loss: 863.60681\n",
      "Current_batch_loss: 877.68732\n",
      "Current_batch_loss: 899.15723\n",
      "Current_batch_loss: 834.52472\n",
      "Current_batch_loss: 792.42896\n",
      "Current_batch_loss: 732.27893\n",
      "Current_batch_loss: 810.78064\n",
      "Current_batch_loss: 979.7417\n",
      "Current_batch_loss: 919.95007\n",
      "Current_batch_loss: 882.00262\n",
      "Current_batch_loss: 819.64117\n",
      "Current_batch_loss: 979.75543\n",
      "Current_batch_loss: 903.53638\n",
      "Current_batch_loss: 897.99359\n",
      "Current_batch_loss: 1016.67523\n",
      "Current_batch_loss: 979.75653\n",
      "Current_batch_loss: 973.03339\n",
      "Current_batch_loss: 880.94629\n",
      "Current_batch_loss: 871.51807\n",
      "Current_batch_loss: 906.92126\n",
      "Current_batch_loss: 847.90112\n",
      "Current_batch_loss: 881.36444\n",
      "Current_batch_loss: 794.39191\n",
      "Current_batch_loss: 909.00427\n",
      "Current_batch_loss: 880.28766\n",
      "Current_batch_loss: 866.87579\n",
      "Current_batch_loss: 951.32947\n",
      "Current_batch_loss: 904.56067\n",
      "Current_batch_loss: 778.2002\n",
      "Current_batch_loss: 922.07642\n",
      "Current_batch_loss: 848.94049\n",
      "Current_batch_loss: 894.99359\n",
      "Current_batch_loss: 853.29004\n",
      "Current_batch_loss: 765.95508\n",
      "Current_batch_loss: 833.58917\n",
      "Current_batch_loss: 797.97284\n",
      "Current_batch_loss: 912.07867\n",
      "Current_batch_loss: 708.591\n",
      "Current_batch_loss: 964.69153\n",
      "Current_batch_loss: 897.71063\n",
      "Current_batch_loss: 909.26508\n",
      "Current_batch_loss: 954.91022\n",
      "Current_batch_loss: 919.69165\n",
      "Current_batch_loss: 855.82581\n",
      "Current_batch_loss: 897.83734\n",
      "Current_batch_loss: 872.2428\n",
      "Current_batch_loss: 770.37878\n",
      "Current_batch_loss: 856.80255\n",
      "Current_batch_loss: 723.75891\n",
      "Current_batch_loss: 934.79169\n",
      "Current_batch_loss: 663.6925\n",
      "Current_batch_loss: 962.44202\n",
      "Current_batch_loss: 921.73718\n",
      "Current_batch_loss: 846.0744\n",
      "Current_batch_loss: 921.39081\n",
      "Current_batch_loss: 806.078\n",
      "Current_batch_loss: 846.67755\n",
      "Current_batch_loss: 835.1236\n",
      "Current_batch_loss: 790.97839\n",
      "Current_batch_loss: 903.92267\n",
      "Current_batch_loss: 884.46344\n",
      "Current_batch_loss: 840.36023\n",
      "Current_batch_loss: 868.34546\n",
      "Current_batch_loss: 895.31226\n",
      "Current_batch_loss: 805.67059\n",
      "Current_batch_loss: 819.41821\n",
      "Current_batch_loss: 741.61407\n",
      "Current_batch_loss: 916.45941\n",
      "Current_batch_loss: 879.32477\n",
      "Current_batch_loss: 970.82117\n",
      "Current_batch_loss: 893.54932\n",
      "Current_batch_loss: 908.9444\n",
      "Current_batch_loss: 766.96997\n",
      "Current_batch_loss: 929.31525\n",
      "Current_batch_loss: 703.83527\n",
      "Current_batch_loss: 714.00378\n",
      "Current_batch_loss: 798.09003\n",
      "Current_batch_loss: 773.66565\n",
      "Current_batch_loss: 942.73407\n",
      "Current_batch_loss: 786.59149\n",
      "Current_batch_loss: 659.87799\n",
      "Current_batch_loss: 833.69604\n",
      "Current_batch_loss: 786.78217\n",
      "Current_batch_loss: 936.28778\n",
      "Current_batch_loss: 722.87549\n",
      "Current_batch_loss: 716.16663\n",
      "Current_batch_loss: 917.20416\n",
      "Current_batch_loss: 781.46063\n",
      "Current_batch_loss: 820.22675\n",
      "Current_batch_loss: 909.72412\n",
      "Current_batch_loss: 711.76599\n",
      "Current_batch_loss: 895.54279\n",
      "Current_batch_loss: 948.45422\n",
      "Current_batch_loss: 828.64093\n",
      "Current_batch_loss: 830.75421\n",
      "Current_batch_loss: 752.20972\n",
      "Current_batch_loss: 843.26532\n",
      "Current_batch_loss: 770.3736\n",
      "Current_batch_loss: 883.77844\n",
      "Current_batch_loss: 832.24323\n",
      "Current_batch_loss: 765.18585\n",
      "Current_batch_loss: 882.72522\n",
      "Current_batch_loss: 833.66052\n",
      "Current_batch_loss: 935.82477\n",
      "Current_batch_loss: 874.52332\n",
      "Current_batch_loss: 429.67029\n",
      "Current_batch_loss: 817.76062\n",
      "Current_batch_loss: 820.14166\n",
      "Current_batch_loss: 814.93616\n",
      "Current_batch_loss: 881.2804\n",
      "Current_batch_loss: 889.18762\n",
      "Current_batch_loss: 790.18042\n",
      "Current_batch_loss: 893.38501\n",
      "Current_batch_loss: 826.43036\n",
      "Current_batch_loss: 832.6394\n",
      "Current_batch_loss: 699.28406\n",
      "Current_batch_loss: 716.8855\n",
      "Current_batch_loss: 860.02124\n",
      "Current_batch_loss: 903.4425\n",
      "Current_batch_loss: 692.10962\n",
      "Current_batch_loss: 821.34509\n",
      "Current_batch_loss: 889.22302\n",
      "Current_batch_loss: 916.81195\n",
      "Current_batch_loss: 871.58423\n",
      "Current_batch_loss: 812.00067\n",
      "Current_batch_loss: 861.77106\n",
      "Current_batch_loss: 815.60565\n",
      "Current_batch_loss: 793.54095\n",
      "Current_batch_loss: 679.07745\n",
      "Current_batch_loss: 704.57635\n",
      "Current_batch_loss: 535.77588\n",
      "Current_batch_loss: 914.80603\n",
      "Current_batch_loss: 816.81927\n",
      "Current_batch_loss: 669.1272\n",
      "Current_batch_loss: 781.92328\n",
      "Current_batch_loss: 812.30237\n",
      "Current_batch_loss: 873.21045\n",
      "Current_batch_loss: 818.24097\n",
      "Current_batch_loss: 916.99481\n",
      "Current_batch_loss: 797.26703\n",
      "Current_batch_loss: 830.95081\n",
      "Current_batch_loss: 828.98584\n",
      "Current_batch_loss: 819.20691\n",
      "Current_batch_loss: 812.40057\n",
      "Current_batch_loss: 774.56885\n",
      "Current_batch_loss: 903.07092\n",
      "Current_batch_loss: 785.01569\n",
      "Current_batch_loss: 790.22644\n",
      "Current_batch_loss: 808.16693\n",
      "Current_batch_loss: 743.26532\n",
      "Current_batch_loss: 711.13904\n",
      "Current_batch_loss: 689.62329\n",
      "Current_batch_loss: 761.62512\n",
      "Current_batch_loss: 820.28833\n",
      "Current_batch_loss: 779.11975\n",
      "Current_batch_loss: 790.93677\n",
      "Current_batch_loss: 570.93005\n",
      "Current_batch_loss: 782.58887\n",
      "Current_batch_loss: 931.57196\n",
      "Current_batch_loss: 829.36255\n",
      "Current_batch_loss: 839.98962\n",
      "Current_batch_loss: 739.56671\n",
      "Current_batch_loss: 770.44653\n",
      "Current_batch_loss: 632.07275\n",
      "Current_batch_loss: 871.89294\n",
      "Current_batch_loss: 739.99567\n",
      "Current_batch_loss: 807.08636\n",
      "Current_batch_loss: 874.41187\n",
      "Current_batch_loss: 692.85876\n",
      "Current_batch_loss: 798.49799\n",
      "Current_batch_loss: 686.64331\n",
      "Current_batch_loss: 584.73395\n",
      "Current_batch_loss: 699.69427\n",
      "Current_batch_loss: 791.87762\n",
      "Current_batch_loss: 745.2525\n",
      "Current_batch_loss: 700.56146\n",
      "Current_batch_loss: 691.70728\n",
      "Current_batch_loss: 734.71539\n",
      "Current_batch_loss: 802.42902\n",
      "Current_batch_loss: 619.10699\n",
      "Current_batch_loss: 659.13153\n",
      "Current_batch_loss: 848.72736\n",
      "Current_batch_loss: 782.08893\n",
      "Current_batch_loss: 743.22168\n",
      "Current_batch_loss: 558.69159\n",
      "Current_batch_loss: 752.19745\n",
      "Current_batch_loss: 786.05884\n",
      "Current_batch_loss: 615.2417\n",
      "Current_batch_loss: 777.0368\n",
      "Current_batch_loss: 712.63434\n",
      "Current_batch_loss: 653.51294\n",
      "Current_batch_loss: 688.28864\n",
      "Current_batch_loss: 788.2677\n",
      "Current_batch_loss: 607.65759\n",
      "Current_batch_loss: 875.37701\n",
      "Current_batch_loss: 711.55273\n",
      "Current_batch_loss: 820.26978\n",
      "Current_batch_loss: 772.21814\n",
      "Current_batch_loss: 644.73236\n",
      "Current_batch_loss: 692.8399\n",
      "Current_batch_loss: 678.51062\n",
      "Current_batch_loss: 888.75287\n",
      "Current_batch_loss: 812.4046\n",
      "Current_batch_loss: 726.51208\n",
      "Current_batch_loss: 694.84918\n",
      "Current_batch_loss: 666.34863\n",
      "Current_batch_loss: 737.57056\n",
      "Current_batch_loss: 778.02191\n",
      "Current_batch_loss: 819.36218\n",
      "Current_batch_loss: 829.19781\n",
      "Current_batch_loss: 567.22809\n",
      "Current_batch_loss: 743.58649\n",
      "Current_batch_loss: 798.39331\n",
      "Current_batch_loss: 784.0907\n",
      "Current_batch_loss: 755.45618\n",
      "Current_batch_loss: 575.65759\n",
      "Current_batch_loss: 751.3913\n",
      "Current_batch_loss: 804.82819\n",
      "Current_batch_loss: 742.80371\n",
      "Current_batch_loss: 745.99902\n",
      "Current_batch_loss: 658.03961\n",
      "Current_batch_loss: 797.21033\n",
      "Current_batch_loss: 676.44977\n",
      "Current_batch_loss: 758.04333\n",
      "Current_batch_loss: 651.66107\n",
      "Current_batch_loss: 733.94482\n",
      "Current_batch_loss: 625.27502\n",
      "Current_batch_loss: 745.39172\n",
      "Current_batch_loss: 675.8399\n",
      "Current_batch_loss: 540.42682\n",
      "Current_batch_loss: 780.74133\n",
      "Current_batch_loss: 519.05139\n",
      "Current_batch_loss: 602.90247\n",
      "Current_batch_loss: 695.10455\n",
      "Current_batch_loss: 921.57013\n",
      "Current_batch_loss: 764.36401\n",
      "Current_batch_loss: 636.72333\n",
      "Current_batch_loss: 784.4502\n",
      "Current_batch_loss: 747.54645\n",
      "Current_batch_loss: 788.44635\n",
      "Current_batch_loss: 854.32111\n",
      "Current_batch_loss: 650.78235\n",
      "Current_batch_loss: 739.13354\n",
      "Current_batch_loss: 772.6651\n",
      "Current_batch_loss: 697.06628\n",
      "Current_batch_loss: 670.69666\n",
      "Current_batch_loss: 960.95746\n",
      "Current_batch_loss: 702.93201\n",
      "Current_batch_loss: 753.5498\n",
      "Current_batch_loss: 628.16388\n",
      "Current_batch_loss: 752.34528\n",
      "Current_batch_loss: 604.01624\n",
      "Current_batch_loss: 740.21387\n",
      "Current_batch_loss: 654.9729\n",
      "Current_batch_loss: 701.74786\n",
      "Current_batch_loss: 688.97107\n",
      "Current_batch_loss: 704.98236\n",
      "Current_batch_loss: 721.49591\n",
      "Current_batch_loss: 715.02271\n",
      "Current_batch_loss: 755.16516\n",
      "Current_batch_loss: 744.57831\n",
      "Current_batch_loss: 712.54462\n",
      "Current_batch_loss: 702.40204\n",
      "Current_batch_loss: 713.55695\n",
      "Current_batch_loss: 813.65454\n",
      "Current_batch_loss: 800.64697\n",
      "Current_batch_loss: 744.00958\n",
      "Current_batch_loss: 636.00983\n",
      "Current_batch_loss: 657.12518\n",
      "Current_batch_loss: 720.46106\n",
      "Current_batch_loss: 567.37762\n",
      "Current_batch_loss: 759.79657\n",
      "Current_batch_loss: 574.09491\n",
      "Current_batch_loss: 688.00153\n",
      "Current_batch_loss: 728.0849\n",
      "Current_batch_loss: 645.3703\n",
      "Current_batch_loss: 745.94476\n",
      "Current_batch_loss: 803.80603\n",
      "Current_batch_loss: 635.73627\n",
      "Current_batch_loss: 797.23181\n",
      "Current_batch_loss: 511.01443\n",
      "Current_batch_loss: 677.755\n",
      "Current_batch_loss: 667.03265\n",
      "Current_batch_loss: 635.60864\n",
      "Current_batch_loss: 569.90845\n",
      "Current_batch_loss: 571.02625\n",
      "Current_batch_loss: 672.68115\n",
      "Current_batch_loss: 688.30249\n",
      "Current_batch_loss: 701.25458\n",
      "Current_batch_loss: 560.72552\n",
      "Current_batch_loss: 794.59393\n",
      "Current_batch_loss: 564.18073\n",
      "Current_batch_loss: 643.46698\n",
      "Current_batch_loss: 582.68188\n",
      "Current_batch_loss: 672.17151\n",
      "Current_batch_loss: 653.90637\n",
      "Current_batch_loss: 820.41565\n",
      "Current_batch_loss: 763.78949\n",
      "Current_batch_loss: 689.92554\n",
      "Current_batch_loss: 594.01685\n",
      "Current_batch_loss: 763.44061\n",
      "Current_batch_loss: 669.29382\n",
      "Current_batch_loss: 699.64374\n",
      "Current_batch_loss: 506.91043\n",
      "Current_batch_loss: 651.08643\n",
      "Current_batch_loss: 606.21277\n",
      "Current_batch_loss: 691.32892\n",
      "Current_batch_loss: 693.97314\n",
      "Current_batch_loss: 634.0177\n",
      "Current_batch_loss: 670.31238\n",
      "Current_batch_loss: 629.40851\n",
      "Current_batch_loss: 590.48883\n",
      "Current_batch_loss: 534.75531\n",
      "Current_batch_loss: 700.84711\n",
      "Current_batch_loss: 591.87897\n",
      "Current_batch_loss: 546.2002\n",
      "Current_batch_loss: 611.37347\n",
      "Current_batch_loss: 727.61115\n",
      "Current_batch_loss: 712.76886\n",
      "Current_batch_loss: 587.07611\n",
      "Current_batch_loss: 570.74347\n",
      "Current_batch_loss: 624.43738\n",
      "Current_batch_loss: 728.65521\n",
      "Current_batch_loss: 709.57806\n",
      "Current_batch_loss: 643.76331\n",
      "Current_batch_loss: 582.02081\n",
      "Current_batch_loss: 644.64368\n",
      "Current_batch_loss: 578.9256\n",
      "Current_batch_loss: 660.849\n",
      "Current_batch_loss: 649.6853\n",
      "Current_batch_loss: 605.5368\n",
      "Current_batch_loss: 577.6004\n",
      "Current_batch_loss: 676.27655\n",
      "Current_batch_loss: 630.2962\n",
      "Current_batch_loss: 595.61755\n",
      "Current_batch_loss: 716.28815\n",
      "Current_batch_loss: 611.97522\n",
      "Current_batch_loss: 632.7251\n",
      "Current_batch_loss: 582.18726\n",
      "Current_batch_loss: 695.42047\n",
      "Current_batch_loss: 689.77582\n",
      "Current_batch_loss: 573.05042\n",
      "Current_batch_loss: 526.32111\n",
      "Current_batch_loss: 691.29645\n",
      "Current_batch_loss: 601.30157\n",
      "Current_batch_loss: 669.72162\n",
      "Current_batch_loss: 624.88214\n",
      "Current_batch_loss: 680.38733\n",
      "Current_batch_loss: 731.14166\n",
      "Current_batch_loss: 597.1507\n",
      "Current_batch_loss: 634.66577\n",
      "Current_batch_loss: 469.48407\n",
      "Current_batch_loss: 683.68616\n",
      "Current_batch_loss: 620.02081\n",
      "Current_batch_loss: 558.15088\n",
      "Current_batch_loss: 648.5152\n",
      "Current_batch_loss: 529.79364\n",
      "Current_batch_loss: 560.82483\n",
      "Current_batch_loss: 559.32745\n",
      "Current_batch_loss: 571.09113\n",
      "Current_batch_loss: 526.49438\n",
      "Current_batch_loss: 601.69171\n",
      "Current_batch_loss: 678.66638\n",
      "Current_batch_loss: 713.4046\n",
      "Current_batch_loss: 701.60626\n",
      "Current_batch_loss: 540.5766\n",
      "Current_batch_loss: 523.46326\n",
      "Current_batch_loss: 568.42743\n",
      "Current_batch_loss: 671.21063\n",
      "Current_batch_loss: 577.00983\n",
      "Current_batch_loss: 589.41577\n",
      "Current_batch_loss: 557.81354\n",
      "Current_batch_loss: 733.4101\n",
      "Current_batch_loss: 487.04633\n",
      "Current_batch_loss: 580.22461\n",
      "Current_batch_loss: 743.61053\n",
      "Current_batch_loss: 625.48663\n",
      "Current_batch_loss: 649.80096\n",
      "Current_batch_loss: 736.25293\n",
      "Current_batch_loss: 577.84198\n",
      "Current_batch_loss: 658.09924\n",
      "Current_batch_loss: 464.94238\n",
      "Current_batch_loss: 534.52063\n",
      "Current_batch_loss: 451.20462\n",
      "Current_batch_loss: 591.22388\n",
      "Current_batch_loss: 734.25623\n",
      "Current_batch_loss: 717.97864\n",
      "Current_batch_loss: 593.85284\n",
      "Current_batch_loss: 570.677\n",
      "Current_batch_loss: 698.91107\n",
      "Current_batch_loss: 547.16125\n",
      "Current_batch_loss: 726.63971\n",
      "Current_batch_loss: 702.95709\n",
      "Current_batch_loss: 630.7608\n",
      "Current_batch_loss: 633.15881\n",
      "Current_batch_loss: 516.63409\n",
      "Current_batch_loss: 556.03064\n",
      "Current_batch_loss: 680.2901\n",
      "Current_batch_loss: 633.57062\n",
      "Current_batch_loss: 658.80853\n",
      "Current_batch_loss: 641.58643\n",
      "Current_batch_loss: 519.47308\n",
      "Current_batch_loss: 790.59125\n",
      "Current_batch_loss: 505.16766\n",
      "Current_batch_loss: 572.14368\n",
      "Current_batch_loss: 569.89392\n",
      "Current_batch_loss: 587.5094\n",
      "Current_batch_loss: 652.38763\n",
      "Current_batch_loss: 674.16266\n",
      "Current_batch_loss: 605.4632\n",
      "Current_batch_loss: 686.35529\n",
      "Current_batch_loss: 580.87415\n",
      "Current_batch_loss: 573.91907\n",
      "Current_batch_loss: 595.39514\n",
      "Current_batch_loss: 471.69937\n",
      "Current_batch_loss: 566.30103\n",
      "Current_batch_loss: 693.31128\n",
      "Current_batch_loss: 468.56491\n",
      "Current_batch_loss: 601.54297\n",
      "Current_batch_loss: 420.10983\n",
      "Current_batch_loss: 729.23712\n",
      "Current_batch_loss: 567.26874\n",
      "Current_batch_loss: 596.25092\n",
      "Current_batch_loss: 579.70667\n",
      "Current_batch_loss: 525.57129\n",
      "Current_batch_loss: 557.73468\n",
      "Current_batch_loss: 731.68835\n",
      "Current_batch_loss: 535.88251\n",
      "Current_batch_loss: 416.16141\n",
      "Current_batch_loss: 510.29825\n",
      "Current_batch_loss: 600.81537\n",
      "Current_batch_loss: 544.37622\n",
      "Current_batch_loss: 560.23224\n",
      "Current_batch_loss: 520.52661\n",
      "Current_batch_loss: 588.67047\n",
      "Current_batch_loss: 574.7027\n",
      "Current_batch_loss: 479.70105\n",
      "Current_batch_loss: 523.8653\n",
      "Current_batch_loss: 527.83948\n",
      "Current_batch_loss: 400.42368\n",
      "Current_batch_loss: 603.42297\n",
      "Current_batch_loss: 536.49945\n",
      "Current_batch_loss: 568.04303\n",
      "Current_batch_loss: 613.67834\n",
      "Current_batch_loss: 439.66248\n",
      "Current_batch_loss: 592.26154\n",
      "Current_batch_loss: 600.77393\n",
      "Current_batch_loss: 520.15198\n",
      "Current_batch_loss: 599.56824\n",
      "Current_batch_loss: 502.49252\n",
      "Current_batch_loss: 754.58881\n",
      "Current_batch_loss: 586.65253\n",
      "Current_batch_loss: 595.6178\n",
      "Current_batch_loss: 568.04388\n",
      "Current_batch_loss: 588.54773\n",
      "Current_batch_loss: 613.4906\n",
      "Current_batch_loss: 544.48016\n",
      "Current_batch_loss: 567.00446\n",
      "Current_batch_loss: 463.19678\n",
      "Current_batch_loss: 627.46454\n",
      "Current_batch_loss: 459.00946\n",
      "Current_batch_loss: 536.69775\n",
      "Current_batch_loss: 546.38739\n",
      "Current_batch_loss: 485.88559\n",
      "Current_batch_loss: 577.85889\n",
      "Current_batch_loss: 534.18604\n",
      "Current_batch_loss: 574.75696\n",
      "Current_batch_loss: 365.78964\n",
      "Current_batch_loss: 470.55695\n",
      "Current_batch_loss: 427.20663\n",
      "Current_batch_loss: 509.94583\n",
      "Current_batch_loss: 486.73505\n",
      "Current_batch_loss: 576.77173\n",
      "Current_batch_loss: 502.95169\n",
      "Current_batch_loss: 480.10776\n",
      "Current_batch_loss: 532.8772\n",
      "Current_batch_loss: 615.77551\n",
      "Current_batch_loss: 322.82773\n",
      "Current_batch_loss: 407.35434\n",
      "Current_batch_loss: 591.63263\n",
      "Current_batch_loss: 460.05319\n",
      "Current_batch_loss: 471.63571\n",
      "Current_batch_loss: 445.01385\n",
      "Current_batch_loss: 554.3064\n",
      "Current_batch_loss: 617.41882\n",
      "Current_batch_loss: 486.7897\n",
      "Current_batch_loss: 497.39468\n",
      "Current_batch_loss: 518.06317\n",
      "Current_batch_loss: 438.12909\n",
      "Current_batch_loss: 604.59613\n",
      "Current_batch_loss: 525.77216\n",
      "Current_batch_loss: 388.9238\n",
      "Current_batch_loss: 444.74826\n",
      "Current_batch_loss: 532.96198\n",
      "Current_batch_loss: 560.54608\n",
      "Current_batch_loss: 412.08682\n",
      "Current_batch_loss: 342.67596\n",
      "Current_batch_loss: 400.61548\n",
      "Current_batch_loss: 424.89886\n",
      "Current_batch_loss: 657.45477\n",
      "Current_batch_loss: 501.57047\n",
      "Current_batch_loss: 523.76465\n",
      "Current_batch_loss: 351.74271\n",
      "Current_batch_loss: 575.02539\n",
      "Current_batch_loss: 598.71478\n",
      "Current_batch_loss: 402.18118\n",
      "Current_batch_loss: 482.36493\n",
      "Current_batch_loss: 469.27267\n",
      "Current_batch_loss: 434.32025\n",
      "Current_batch_loss: 555.95251\n",
      "Current_batch_loss: 460.23779\n",
      "Current_batch_loss: 453.39413\n",
      "Current_batch_loss: 428.54266\n",
      "Current_batch_loss: 461.74905\n",
      "Current_batch_loss: 432.68961\n",
      "Current_batch_loss: 443.36859\n",
      "Current_batch_loss: 479.11249\n",
      "Current_batch_loss: 486.97354\n",
      "Current_batch_loss: 538.43616\n",
      "Current_batch_loss: 486.78003\n",
      "Current_batch_loss: 544.73077\n",
      "Current_batch_loss: 364.24591\n",
      "Current_batch_loss: 536.74982\n",
      "Current_batch_loss: 439.32529\n",
      "Current_batch_loss: 492.41995\n",
      "Current_batch_loss: 454.18231\n",
      "Current_batch_loss: 382.18964\n",
      "Current_batch_loss: 525.33856\n",
      "Current_batch_loss: 433.86505\n",
      "Current_batch_loss: 362.70901\n",
      "Current_batch_loss: 518.14673\n",
      "Current_batch_loss: 523.62415\n",
      "Current_batch_loss: 356.8244\n",
      "Current_batch_loss: 430.00119\n",
      "Current_batch_loss: 450.88437\n",
      "Current_batch_loss: 624.12024\n",
      "Current_batch_loss: 495.39011\n",
      "Current_batch_loss: 534.86353\n",
      "Current_batch_loss: 587.07892\n",
      "Current_batch_loss: 607.19318\n",
      "Current_batch_loss: 548.51331\n",
      "Current_batch_loss: 550.77856\n",
      "Current_batch_loss: 383.37772\n",
      "Current_batch_loss: 444.62915\n",
      "Current_batch_loss: 439.63272\n",
      "Current_batch_loss: 335.08777\n",
      "Current_batch_loss: 413.76343\n",
      "Current_batch_loss: 482.46381\n",
      "Current_batch_loss: 491.5766\n",
      "Current_batch_loss: 459.81305\n",
      "Current_batch_loss: 477.04462\n",
      "Current_batch_loss: 407.48709\n",
      "Current_batch_loss: 396.9444\n",
      "Current_batch_loss: 508.84326\n",
      "Current_batch_loss: 400.79678\n",
      "Current_batch_loss: 494.27271\n",
      "Current_batch_loss: 439.02762\n",
      "Current_batch_loss: 508.5025\n",
      "Current_batch_loss: 556.72711\n",
      "Current_batch_loss: 372.19861\n",
      "Current_batch_loss: 422.64459\n",
      "Current_batch_loss: 515.72913\n",
      "Current_batch_loss: 382.3833\n",
      "Current_batch_loss: 313.60385\n",
      "Current_batch_loss: 391.68259\n",
      "Current_batch_loss: 466.51016\n",
      "Current_batch_loss: 490.34079\n",
      "Current_batch_loss: 499.05637\n",
      "Current_batch_loss: 315.65875\n",
      "Current_batch_loss: 463.7164\n",
      "Current_batch_loss: 612.8916\n",
      "Current_batch_loss: 326.27612\n",
      "Current_batch_loss: 564.13647\n",
      "Current_batch_loss: 482.58875\n",
      "Current_batch_loss: 437.23044\n",
      "Current_batch_loss: 506.53555\n",
      "Current_batch_loss: 458.57092\n",
      "Current_batch_loss: 354.75558\n",
      "Current_batch_loss: 471.45865\n",
      "Current_batch_loss: 426.62714\n",
      "Current_batch_loss: 433.58374\n",
      "Current_batch_loss: 490.96164\n",
      "Current_batch_loss: 352.74011\n",
      "Current_batch_loss: 353.07721\n",
      "Current_batch_loss: 456.41806\n",
      "Current_batch_loss: 469.52399\n",
      "Current_batch_loss: 445.57895\n",
      "Current_batch_loss: 408.06387\n",
      "Current_batch_loss: 425.14551\n",
      "Current_batch_loss: 419.07602\n",
      "Current_batch_loss: 456.55624\n",
      "Current_batch_loss: 588.71375\n",
      "Current_batch_loss: 396.03336\n",
      "Current_batch_loss: 359.88803\n",
      "Current_batch_loss: 489.6965\n",
      "Current_batch_loss: 336.20465\n",
      "Current_batch_loss: 491.17038\n",
      "Current_batch_loss: 292.92102\n",
      "Current_batch_loss: 441.20374\n",
      "Current_batch_loss: 484.47592\n",
      "Current_batch_loss: 479.93979\n",
      "Current_batch_loss: 324.76935\n",
      "Current_batch_loss: 328.49127\n",
      "Current_batch_loss: 290.69254\n",
      "Current_batch_loss: 406.0993\n",
      "Current_batch_loss: 506.73322\n",
      "Current_batch_loss: 428.20615\n",
      "Current_batch_loss: 384.595\n",
      "Current_batch_loss: 533.12714\n",
      "Current_batch_loss: 442.48721\n",
      "Current_batch_loss: 346.50946\n",
      "Current_batch_loss: 436.57803\n",
      "Current_batch_loss: 388.41693\n",
      "Current_batch_loss: 431.68066\n",
      "Current_batch_loss: 336.87125\n",
      "Current_batch_loss: 356.65326\n",
      "Current_batch_loss: 488.30563\n",
      "Current_batch_loss: 312.74893\n",
      "Current_batch_loss: 534.63464\n",
      "Current_batch_loss: 461.55951\n",
      "Current_batch_loss: 336.40796\n",
      "Current_batch_loss: 521.67566\n",
      "Current_batch_loss: 424.22772\n",
      "Current_batch_loss: 295.58514\n",
      "Current_batch_loss: 490.54227\n",
      "Current_batch_loss: 507.46051\n",
      "Current_batch_loss: 437.60211\n",
      "Current_batch_loss: 233.91319\n",
      "Current_batch_loss: 444.44943\n",
      "Current_batch_loss: 322.79572\n",
      "Current_batch_loss: 381.88861\n",
      "Current_batch_loss: 354.73111\n",
      "Current_batch_loss: 372.65802\n",
      "Current_batch_loss: 368.28146\n",
      "Current_batch_loss: 399.29849\n",
      "Current_batch_loss: 370.67395\n",
      "Current_batch_loss: 384.67422\n",
      "Current_batch_loss: 251.65497\n",
      "Current_batch_loss: 466.52811\n",
      "Current_batch_loss: 163.8268\n",
      "Current_batch_loss: 312.17966\n",
      "Current_batch_loss: 544.19421\n",
      "Current_batch_loss: 391.72449\n",
      "Current_batch_loss: 365.31683\n",
      "Current_batch_loss: 310.88559\n",
      "Current_batch_loss: 292.77167\n",
      "Current_batch_loss: 190.9921\n",
      "Current_batch_loss: 440.96881\n",
      "Current_batch_loss: 416.47293\n",
      "Current_batch_loss: 426.75198\n",
      "Current_batch_loss: 487.7258\n",
      "Current_batch_loss: 305.78137\n",
      "Current_batch_loss: 306.32681\n",
      "Current_batch_loss: 340.05359\n",
      "Current_batch_loss: 360.41046\n",
      "Current_batch_loss: 253.69431\n",
      "Current_batch_loss: 410.67365\n",
      "Current_batch_loss: 371.87802\n",
      "Current_batch_loss: 390.99673\n",
      "Current_batch_loss: 334.05051\n",
      "Current_batch_loss: 534.89661\n",
      "Current_batch_loss: 379.38162\n",
      "Current_batch_loss: 344.89764\n",
      "Current_batch_loss: 467.41214\n",
      "Current_batch_loss: 425.79712\n",
      "Current_batch_loss: 495.55096\n",
      "Current_batch_loss: 324.20602\n",
      "Current_batch_loss: 426.77945\n",
      "Current_batch_loss: 448.45441\n",
      "Current_batch_loss: 285.21918\n",
      "Current_batch_loss: 405.85855\n",
      "Current_batch_loss: 362.02869\n",
      "Current_batch_loss: 344.34576\n",
      "Current_batch_loss: 347.3602\n",
      "Current_batch_loss: 437.13837\n",
      "Current_batch_loss: 439.37604\n",
      "Current_batch_loss: 374.81601\n",
      "Current_batch_loss: 312.36337\n",
      "Current_batch_loss: 321.84549\n",
      "Current_batch_loss: 336.68964\n",
      "Current_batch_loss: 316.10626\n",
      "Current_batch_loss: 286.23694\n",
      "Current_batch_loss: 301.86154\n",
      "Current_batch_loss: 413.98395\n",
      "Current_batch_loss: 256.17252\n",
      "Current_batch_loss: 362.46896\n",
      "Current_batch_loss: 379.31555\n",
      "Current_batch_loss: 212.34332\n",
      "Current_batch_loss: 321.11859\n",
      "Current_batch_loss: 392.42068\n",
      "Current_batch_loss: 342.04623\n",
      "Current_batch_loss: 286.4032\n",
      "Current_batch_loss: 383.26746\n",
      "Current_batch_loss: 243.38763\n",
      "Current_batch_loss: 354.17731\n",
      "Current_batch_loss: 349.68604\n",
      "Current_batch_loss: 316.53366\n",
      "Current_batch_loss: 402.93716\n",
      "Current_batch_loss: 274.63077\n",
      "Current_batch_loss: 321.63031\n",
      "Current_batch_loss: 344.85172\n",
      "Current_batch_loss: 327.05951\n",
      "Current_batch_loss: 328.35202\n",
      "Current_batch_loss: 252.15639\n",
      "Current_batch_loss: 341.6777\n",
      "Current_batch_loss: 254.30902\n",
      "Current_batch_loss: 331.95013\n",
      "Current_batch_loss: 354.95029\n",
      "Current_batch_loss: 352.50232\n",
      "Current_batch_loss: 290.9718\n",
      "Current_batch_loss: 442.38977\n",
      "Current_batch_loss: 472.85596\n",
      "Current_batch_loss: 346.23456\n",
      "Current_batch_loss: 434.68085\n",
      "Current_batch_loss: 391.22678\n",
      "Current_batch_loss: 208.80403\n",
      "Current_batch_loss: 296.78915\n",
      "Current_batch_loss: 430.13095\n",
      "Current_batch_loss: 339.00668\n",
      "Current_batch_loss: 407.69772\n",
      "Current_batch_loss: 306.54666\n",
      "Current_batch_loss: 400.9024\n",
      "Current_batch_loss: 294.59692\n",
      "Current_batch_loss: 311.71048\n",
      "Current_batch_loss: 461.94421\n",
      "Current_batch_loss: 300.75797\n",
      "Current_batch_loss: 256.83386\n",
      "Current_batch_loss: 359.15997\n",
      "Current_batch_loss: 281.47333\n",
      "Current_batch_loss: 272.6153\n",
      "Current_batch_loss: 272.96176\n",
      "Current_batch_loss: 297.0751\n",
      "Current_batch_loss: 469.55692\n",
      "Current_batch_loss: 293.81943\n",
      "Current_batch_loss: 496.02594\n",
      "Current_batch_loss: 356.50104\n",
      "Current_batch_loss: 301.93518\n",
      "Current_batch_loss: 297.68372\n",
      "Current_batch_loss: 256.81393\n",
      "Current_batch_loss: 197.64969\n",
      "Current_batch_loss: 304.58609\n",
      "Current_batch_loss: 260.98569\n",
      "Current_batch_loss: 338.91095\n",
      "Current_batch_loss: 275.41534\n",
      "Current_batch_loss: 256.07135\n",
      "Current_batch_loss: 338.24191\n",
      "Current_batch_loss: 368.08517\n",
      "Current_batch_loss: 289.50574\n",
      "Current_batch_loss: 326.4455\n",
      "Current_batch_loss: 345.05338\n",
      "Current_batch_loss: 257.28119\n",
      "Current_batch_loss: 389.29785\n",
      "Current_batch_loss: 372.08228\n",
      "Current_batch_loss: 326.00748\n",
      "Current_batch_loss: 399.72427\n",
      "Current_batch_loss: 288.49857\n",
      "Current_batch_loss: 273.50473\n",
      "Current_batch_loss: 221.37718\n",
      "Current_batch_loss: 258.33719\n",
      "Current_batch_loss: 333.66089\n",
      "Current_batch_loss: 366.9263\n",
      "Current_batch_loss: 194.26294\n",
      "Current_batch_loss: 344.56628\n",
      "Current_batch_loss: 360.21301\n",
      "Current_batch_loss: 309.68759\n",
      "Current_batch_loss: 434.8006\n",
      "Current_batch_loss: 299.75891\n",
      "Current_batch_loss: 324.85968\n",
      "Current_batch_loss: 229.22806\n",
      "Current_batch_loss: 255.58444\n",
      "Current_batch_loss: 162.44521\n",
      "Current_batch_loss: 268.94199\n",
      "Current_batch_loss: 214.17142\n",
      "Current_batch_loss: 356.35626\n",
      "Current_batch_loss: 349.06888\n",
      "Current_batch_loss: 241.54469\n",
      "Current_batch_loss: 311.97989\n",
      "Current_batch_loss: 332.53055\n",
      "Current_batch_loss: 377.13818\n",
      "Current_batch_loss: 281.01358\n",
      "Current_batch_loss: 259.58597\n",
      "Current_batch_loss: 391.33618\n",
      "Current_batch_loss: 210.56238\n",
      "Current_batch_loss: 270.67789\n",
      "Current_batch_loss: 360.66138\n",
      "Current_batch_loss: 383.22751\n",
      "Current_batch_loss: 258.11652\n",
      "Current_batch_loss: 311.16257\n",
      "Current_batch_loss: 315.91589\n",
      "Current_batch_loss: 376.22095\n",
      "Current_batch_loss: 360.44852\n",
      "Current_batch_loss: 381.7583\n",
      "Current_batch_loss: 303.10574\n",
      "Current_batch_loss: 274.40649\n",
      "Current_batch_loss: 269.26605\n",
      "Current_batch_loss: 292.56821\n",
      "Current_batch_loss: 254.12146\n",
      "Current_batch_loss: 297.91397\n",
      "Current_batch_loss: 251.74995\n",
      "Current_batch_loss: 282.22952\n",
      "Current_batch_loss: 359.29199\n",
      "Current_batch_loss: 332.90356\n",
      "Current_batch_loss: 351.78082\n",
      "Current_batch_loss: 263.32574\n",
      "Current_batch_loss: 273.93954\n",
      "Current_batch_loss: 303.1619\n",
      "Current_batch_loss: 269.49857\n",
      "Current_batch_loss: 315.5701\n",
      "Current_batch_loss: 232.79829\n",
      "Current_batch_loss: 282.41522\n",
      "Current_batch_loss: 379.05762\n",
      "Current_batch_loss: 262.61942\n",
      "Current_batch_loss: 221.94537\n",
      "Current_batch_loss: 461.95621\n",
      "Current_batch_loss: 328.14343\n",
      "Current_batch_loss: 247.41513\n",
      "Current_batch_loss: 281.39789\n",
      "Current_batch_loss: 249.23491\n",
      "Current_batch_loss: 203.4174\n",
      "Current_batch_loss: 317.21783\n",
      "Current_batch_loss: 287.20294\n",
      "Current_batch_loss: 184.40744\n",
      "Current_batch_loss: 307.18112\n",
      "Current_batch_loss: 256.93536\n",
      "Current_batch_loss: 283.06448\n",
      "Current_batch_loss: 275.84259\n",
      "Current_batch_loss: 242.94589\n",
      "Current_batch_loss: 257.01581\n",
      "Current_batch_loss: 244.80496\n",
      "Current_batch_loss: 346.76154\n",
      "Current_batch_loss: 273.77945\n",
      "Current_batch_loss: 335.565\n",
      "Current_batch_loss: 260.90799\n",
      "Current_batch_loss: 262.82986\n",
      "Current_batch_loss: 103.52398\n",
      "Current_batch_loss: 363.89258\n",
      "Current_batch_loss: 299.06656\n",
      "Current_batch_loss: 173.08313\n",
      "Current_batch_loss: 231.87051\n",
      "Current_batch_loss: 258.69504\n",
      "Current_batch_loss: 359.76962\n",
      "Current_batch_loss: 315.04758\n",
      "Current_batch_loss: 172.77158\n",
      "Current_batch_loss: 198.50021\n",
      "Current_batch_loss: 214.95499\n",
      "Current_batch_loss: 426.63498\n",
      "Current_batch_loss: 246.05869\n",
      "Current_batch_loss: 206.09639\n",
      "Current_batch_loss: 214.83252\n",
      "Current_batch_loss: 154.58269\n",
      "Current_batch_loss: 237.83133\n",
      "Current_batch_loss: 254.97768\n",
      "Current_batch_loss: 302.58868\n",
      "Current_batch_loss: 309.23596\n",
      "Current_batch_loss: 249.4781\n",
      "Current_batch_loss: 235.46161\n",
      "Current_batch_loss: 244.64752\n",
      "Current_batch_loss: 246.97887\n",
      "Current_batch_loss: 268.98126\n",
      "Current_batch_loss: 304.07617\n",
      "Current_batch_loss: 218.25342\n",
      "Current_batch_loss: 261.70978\n",
      "Current_batch_loss: 225.67786\n",
      "Current_batch_loss: 189.62053\n",
      "Current_batch_loss: 306.29175\n",
      "Current_batch_loss: 386.91208\n",
      "Current_batch_loss: 234.28438\n",
      "Current_batch_loss: 205.78435\n",
      "Current_batch_loss: 234.17964\n",
      "Current_batch_loss: 231.19489\n",
      "Current_batch_loss: 254.92728\n",
      "Current_batch_loss: 236.01468\n",
      "Current_batch_loss: 208.26973\n",
      "Current_batch_loss: 260.59717\n",
      "Current_batch_loss: 280.50955\n",
      "Current_batch_loss: 204.88464\n",
      "Current_batch_loss: 283.55008\n",
      "Current_batch_loss: 253.08826\n",
      "Current_batch_loss: 244.79564\n",
      "Current_batch_loss: 199.07881\n",
      "Current_batch_loss: 202.78755\n",
      "Current_batch_loss: 328.55981\n",
      "Current_batch_loss: 172.16368\n",
      "Current_batch_loss: 267.84857\n",
      "Current_batch_loss: 253.5692\n",
      "Current_batch_loss: 307.4631\n",
      "Current_batch_loss: 200.7363\n",
      "Current_batch_loss: 163.33385\n",
      "Current_batch_loss: 281.0654\n",
      "Current_batch_loss: 220.44188\n",
      "Current_batch_loss: 245.03357\n",
      "Current_batch_loss: 380.59351\n",
      "Current_batch_loss: 207.09892\n",
      "Current_batch_loss: 244.25735\n",
      "Current_batch_loss: 241.64703\n",
      "Current_batch_loss: 196.73119\n",
      "Current_batch_loss: 197.7253\n",
      "Current_batch_loss: 225.0163\n",
      "Current_batch_loss: 258.8472\n",
      "Current_batch_loss: 170.96965\n",
      "Current_batch_loss: 255.09868\n",
      "Current_batch_loss: 141.7988\n",
      "Current_batch_loss: 189.20995\n",
      "Current_batch_loss: 311.95322\n",
      "Current_batch_loss: 308.95981\n",
      "Current_batch_loss: 189.35193\n",
      "Current_batch_loss: 233.85739\n",
      "Current_batch_loss: 167.28679\n",
      "Current_batch_loss: 243.11575\n",
      "Current_batch_loss: 299.93054\n",
      "Current_batch_loss: 222.04942\n",
      "Current_batch_loss: 292.16553\n",
      "Current_batch_loss: 170.83188\n",
      "Current_batch_loss: 178.79468\n",
      "Current_batch_loss: 264.6347\n",
      "Current_batch_loss: 232.4774\n",
      "Current_batch_loss: 235.84409\n",
      "Current_batch_loss: 269.04019\n",
      "Current_batch_loss: 172.5451\n",
      "Current_batch_loss: 317.28162\n",
      "Current_batch_loss: 226.61166\n",
      "Current_batch_loss: 272.90533\n",
      "Current_batch_loss: 191.78171\n",
      "Current_batch_loss: 226.41342\n",
      "Current_batch_loss: 265.93839\n",
      "Current_batch_loss: 280.43845\n",
      "Current_batch_loss: 235.73894\n",
      "Current_batch_loss: 211.42024\n",
      "Current_batch_loss: 236.66808\n",
      "Current_batch_loss: 194.62189\n",
      "Current_batch_loss: 274.5947\n",
      "Current_batch_loss: 180.97153\n",
      "Current_batch_loss: 168.55182\n",
      "Current_batch_loss: 180.19862\n",
      "Current_batch_loss: 95.44773\n",
      "Current_batch_loss: 209.3683\n",
      "Current_batch_loss: 246.94574\n",
      "Current_batch_loss: 166.49165\n",
      "Current_batch_loss: 191.55276\n",
      "Current_batch_loss: 144.29776\n",
      "Current_batch_loss: 269.42096\n",
      "Current_batch_loss: 175.15869\n",
      "Current_batch_loss: 163.19048\n",
      "Current_batch_loss: 251.68201\n",
      "Current_batch_loss: 273.63406\n",
      "Current_batch_loss: 133.85638\n",
      "Current_batch_loss: 168.88795\n",
      "Current_batch_loss: 250.13338\n",
      "Current_batch_loss: 305.1185\n",
      "Current_batch_loss: 241.48697\n",
      "Current_batch_loss: 216.55058\n",
      "Current_batch_loss: 225.21014\n",
      "Current_batch_loss: 226.66721\n",
      "Current_batch_loss: 222.2805\n",
      "Current_batch_loss: 211.22226\n",
      "Current_batch_loss: 146.79683\n",
      "Current_batch_loss: 201.93176\n",
      "Current_batch_loss: 291.70193\n",
      "Current_batch_loss: 189.69742\n",
      "Current_batch_loss: 262.45053\n",
      "Current_batch_loss: 141.09537\n",
      "Current_batch_loss: 231.63416\n",
      "Current_batch_loss: 203.56114\n",
      "Current_batch_loss: 184.20416\n",
      "Current_batch_loss: 317.36465\n",
      "Current_batch_loss: 196.6207\n",
      "Current_batch_loss: 166.03012\n",
      "Current_batch_loss: 177.2957\n",
      "Current_batch_loss: 196.82152\n",
      "Current_batch_loss: 183.45451\n",
      "Current_batch_loss: 167.35464\n",
      "Current_batch_loss: 224.41331\n",
      "Current_batch_loss: 128.82275\n",
      "Current_batch_loss: 255.79579\n",
      "Current_batch_loss: 150.32957\n",
      "Current_batch_loss: 238.12549\n",
      "Current_batch_loss: 169.11244\n",
      "Current_batch_loss: 177.63643\n",
      "Current_batch_loss: 193.26152\n",
      "Current_batch_loss: 180.571\n",
      "Current_batch_loss: 232.06238\n",
      "Current_batch_loss: 193.41057\n",
      "Current_batch_loss: 212.10945\n",
      "Current_batch_loss: 158.88632\n",
      "Current_batch_loss: 197.27188\n",
      "Current_batch_loss: 193.29971\n",
      "Current_batch_loss: 195.70506\n",
      "Current_batch_loss: 187.95175\n",
      "Current_batch_loss: 144.80286\n",
      "Current_batch_loss: 139.57967\n",
      "Current_batch_loss: 193.43588\n",
      "Current_batch_loss: 206.47672\n",
      "Current_batch_loss: 209.15109\n",
      "Current_batch_loss: 193.25162\n",
      "Current_batch_loss: 156.1973\n",
      "Current_batch_loss: 203.86525\n",
      "Current_batch_loss: 155.60895\n",
      "Current_batch_loss: 179.37105\n",
      "Current_batch_loss: 111.83323\n",
      "Current_batch_loss: 141.0076\n",
      "Current_batch_loss: 201.56589\n",
      "Current_batch_loss: 226.71252\n",
      "Current_batch_loss: 130.56001\n",
      "Current_batch_loss: 274.23294\n",
      "Current_batch_loss: 190.1942\n",
      "Current_batch_loss: 122.17843\n",
      "Current_batch_loss: 192.35988\n",
      "Current_batch_loss: 184.62994\n",
      "Current_batch_loss: 186.09547\n",
      "Current_batch_loss: 148.87038\n",
      "Current_batch_loss: 177.27515\n",
      "Current_batch_loss: 176.58566\n",
      "Current_batch_loss: 119.26505\n",
      "Current_batch_loss: 131.0052\n",
      "Current_batch_loss: 149.88159\n",
      "Current_batch_loss: 197.92026\n",
      "Current_batch_loss: 171.98915\n",
      "Current_batch_loss: 114.90458\n",
      "Current_batch_loss: 168.08099\n",
      "Current_batch_loss: 229.94989\n",
      "Current_batch_loss: 126.51334\n",
      "Current_batch_loss: 136.79279\n",
      "Current_batch_loss: 143.84908\n",
      "Current_batch_loss: 166.94792\n",
      "Current_batch_loss: 131.44186\n",
      "Current_batch_loss: 165.24792\n",
      "Current_batch_loss: 128.3488\n",
      "Current_batch_loss: 139.04446\n",
      "Current_batch_loss: 174.65282\n",
      "Current_batch_loss: 154.26413\n",
      "Current_batch_loss: 207.46317\n",
      "Current_batch_loss: 195.45781\n",
      "Current_batch_loss: 178.32492\n",
      "Current_batch_loss: 101.43381\n",
      "Current_batch_loss: 132.88826\n",
      "Current_batch_loss: 172.86967\n",
      "Current_batch_loss: 227.48161\n",
      "Current_batch_loss: 82.19061\n",
      "Current_batch_loss: 145.62932\n",
      "Current_batch_loss: 142.83401\n",
      "Current_batch_loss: 142.03708\n",
      "Current_batch_loss: 253.70038\n",
      "Current_batch_loss: 250.0417\n",
      "Current_batch_loss: 111.42609\n",
      "Current_batch_loss: 180.35811\n",
      "Current_batch_loss: 145.29555\n",
      "Current_batch_loss: 195.07372\n",
      "Current_batch_loss: 135.15746\n",
      "Current_batch_loss: 175.31712\n",
      "Current_batch_loss: 97.01967\n",
      "Current_batch_loss: 131.49695\n",
      "Current_batch_loss: 160.97078\n",
      "Current_batch_loss: 222.06339\n",
      "Current_batch_loss: 160.74693\n",
      "Current_batch_loss: 235.6833\n",
      "Current_batch_loss: 122.97956\n",
      "Current_batch_loss: 101.22213\n",
      "Current_batch_loss: 93.24503\n",
      "Current_batch_loss: 153.09558\n",
      "Current_batch_loss: 106.6148\n",
      "Current_batch_loss: 74.05946\n",
      "Current_batch_loss: 168.84579\n",
      "Current_batch_loss: 186.47473\n",
      "Current_batch_loss: 157.64705\n",
      "Current_batch_loss: 178.08809\n",
      "Current_batch_loss: 139.87888\n",
      "Current_batch_loss: 90.92242\n",
      "Current_batch_loss: 140.08305\n",
      "Current_batch_loss: 115.20678\n",
      "Current_batch_loss: 174.78607\n",
      "Current_batch_loss: 184.46233\n",
      "Current_batch_loss: 120.85327\n",
      "Current_batch_loss: 204.22723\n",
      "Current_batch_loss: 196.26314\n",
      "Current_batch_loss: 107.85658\n",
      "Current_batch_loss: 143.80859\n",
      "Current_batch_loss: 217.72063\n",
      "Current_batch_loss: 97.72408\n",
      "Current_batch_loss: 139.0367\n",
      "Current_batch_loss: 150.54552\n",
      "Current_batch_loss: 118.90762\n",
      "Current_batch_loss: 193.36981\n",
      "Current_batch_loss: 154.63924\n",
      "Current_batch_loss: 137.67908\n",
      "Current_batch_loss: 134.53436\n",
      "Current_batch_loss: 146.2524\n",
      "Current_batch_loss: 162.18013\n",
      "Current_batch_loss: 184.4328\n",
      "Current_batch_loss: 146.99118\n",
      "Current_batch_loss: 60.6199\n",
      "Current_batch_loss: 146.34\n",
      "Current_batch_loss: 149.68382\n",
      "Current_batch_loss: 164.91911\n",
      "Current_batch_loss: 153.57018\n",
      "Current_batch_loss: 140.46613\n",
      "Current_batch_loss: 69.9082\n",
      "Current_batch_loss: 167.43263\n",
      "Current_batch_loss: 102.46294\n",
      "Current_batch_loss: 162.74725\n",
      "Current_batch_loss: 101.35449\n",
      "Current_batch_loss: 128.25313\n",
      "Current_batch_loss: 81.69349\n",
      "Current_batch_loss: 119.07249\n",
      "Current_batch_loss: 154.27882\n",
      "Current_batch_loss: 103.9097\n",
      "Current_batch_loss: 139.33594\n",
      "Current_batch_loss: 150.27219\n",
      "Current_batch_loss: 123.60944\n",
      "Current_batch_loss: 181.30371\n",
      "Current_batch_loss: 103.54736\n",
      "Current_batch_loss: 97.57542\n",
      "Current_batch_loss: 156.45609\n",
      "Current_batch_loss: 123.91741\n",
      "Current_batch_loss: 171.77773\n",
      "Current_batch_loss: 121.37061\n",
      "Current_batch_loss: 98.90655\n",
      "Current_batch_loss: 158.12581\n",
      "Current_batch_loss: 102.4515\n",
      "Current_batch_loss: 85.55529\n",
      "Current_batch_loss: 139.92639\n",
      "Current_batch_loss: 161.33777\n",
      "Current_batch_loss: 140.58226\n",
      "Current_batch_loss: 35.87017\n",
      "Current_batch_loss: 158.52414\n",
      "Current_batch_loss: 142.94635\n",
      "Current_batch_loss: 145.71016\n",
      "Current_batch_loss: 103.72046\n",
      "Current_batch_loss: 101.11517\n",
      "Current_batch_loss: 85.28098\n",
      "Current_batch_loss: 125.53206\n",
      "Current_batch_loss: 110.91822\n",
      "Current_batch_loss: 101.0671\n",
      "Current_batch_loss: 114.38917\n",
      "Current_batch_loss: 168.95538\n",
      "Current_batch_loss: 108.72012\n",
      "Current_batch_loss: 125.83992\n",
      "Current_batch_loss: 124.04108\n",
      "Current_batch_loss: 130.97758\n",
      "Current_batch_loss: 173.80911\n",
      "Current_batch_loss: 81.42607\n",
      "Current_batch_loss: 187.62585\n",
      "Current_batch_loss: 106.09395\n",
      "Current_batch_loss: 153.10797\n",
      "Current_batch_loss: 108.30424\n",
      "Current_batch_loss: 53.6296\n",
      "Current_batch_loss: 110.38057\n",
      "Current_batch_loss: 153.4454\n",
      "Current_batch_loss: 100.41573\n",
      "Current_batch_loss: 104.97007\n",
      "Current_batch_loss: 132.16988\n",
      "Current_batch_loss: 140.21674\n",
      "Current_batch_loss: 119.84232\n",
      "Current_batch_loss: 154.20151\n",
      "Current_batch_loss: 107.85448\n",
      "Current_batch_loss: 111.12568\n",
      "Current_batch_loss: 155.46745\n",
      "Current_batch_loss: 161.43648\n",
      "Current_batch_loss: 74.18671\n",
      "Current_batch_loss: 146.97765\n",
      "Current_batch_loss: 125.86205\n",
      "Current_batch_loss: 149.20261\n",
      "Current_batch_loss: 165.82838\n",
      "Current_batch_loss: 157.16467\n",
      "Current_batch_loss: 84.77477\n",
      "Current_batch_loss: 75.46323\n",
      "Current_batch_loss: 161.85689\n",
      "Current_batch_loss: 155.39702\n",
      "Current_batch_loss: 166.01874\n",
      "Current_batch_loss: 104.47885\n",
      "Current_batch_loss: 123.47382\n",
      "Current_batch_loss: 72.37507\n",
      "Current_batch_loss: 145.38782\n",
      "Current_batch_loss: 61.38082\n",
      "Current_batch_loss: 156.28204\n",
      "Current_batch_loss: 77.28594\n",
      "Current_batch_loss: 138.52888\n",
      "Current_batch_loss: 135.29967\n",
      "Current_batch_loss: 103.46044\n",
      "Current_batch_loss: 106.49831\n",
      "Current_batch_loss: 125.94838\n",
      "Current_batch_loss: 68.97897\n",
      "Current_batch_loss: 106.02262\n",
      "Current_batch_loss: 99.54417\n",
      "Current_batch_loss: 94.40131\n",
      "Current_batch_loss: 96.95551\n",
      "Current_batch_loss: 66.3648\n",
      "Current_batch_loss: 102.13086\n",
      "Current_batch_loss: 91.94801\n",
      "Current_batch_loss: 163.92081\n",
      "Current_batch_loss: 130.16693\n",
      "Current_batch_loss: 161.10129\n",
      "Current_batch_loss: 59.17659\n",
      "Current_batch_loss: 164.77367\n",
      "Current_batch_loss: 103.02981\n",
      "Current_batch_loss: 148.70323\n",
      "Current_batch_loss: 41.03338\n",
      "Current_batch_loss: 87.86441\n",
      "Current_batch_loss: 79.89929\n",
      "Current_batch_loss: 81.70796\n",
      "Current_batch_loss: 97.82687\n",
      "Current_batch_loss: 101.42195\n",
      "Current_batch_loss: 63.23624\n",
      "Current_batch_loss: 119.13483\n",
      "Current_batch_loss: 81.37746\n",
      "Current_batch_loss: 132.64426\n",
      "Current_batch_loss: 114.57648\n",
      "Current_batch_loss: 100.79335\n",
      "Current_batch_loss: 84.03835\n",
      "Current_batch_loss: 110.87957\n",
      "Current_batch_loss: 86.66805\n",
      "Current_batch_loss: 87.06757\n",
      "Current_batch_loss: 95.8247\n",
      "Current_batch_loss: 99.6544\n",
      "Current_batch_loss: 39.44381\n",
      "Current_batch_loss: 130.26967\n",
      "Current_batch_loss: 49.31149\n",
      "Current_batch_loss: 89.6819\n",
      "Current_batch_loss: 59.25492\n",
      "Current_batch_loss: 91.50049\n",
      "Current_batch_loss: 138.23993\n",
      "Current_batch_loss: 44.45374\n",
      "Current_batch_loss: 141.62444\n",
      "Current_batch_loss: 74.8431\n",
      "Current_batch_loss: 53.05431\n",
      "Current_batch_loss: 138.28622\n",
      "Current_batch_loss: 108.49339\n",
      "Current_batch_loss: 81.36762\n",
      "Current_batch_loss: 95.73676\n",
      "Current_batch_loss: 95.20515\n",
      "Current_batch_loss: 84.18065\n",
      "Current_batch_loss: 146.67786\n",
      "Current_batch_loss: 170.84073\n",
      "Current_batch_loss: 64.89493\n",
      "Current_batch_loss: 78.01488\n",
      "Current_batch_loss: 81.80791\n",
      "Current_batch_loss: 146.21346\n",
      "Current_batch_loss: 60.85023\n",
      "Current_batch_loss: 109.34912\n",
      "Current_batch_loss: 79.66441\n",
      "Current_batch_loss: 84.23746\n",
      "Current_batch_loss: 101.62215\n",
      "Current_batch_loss: 159.39577\n",
      "Current_batch_loss: 70.97899\n",
      "Current_batch_loss: 165.75677\n",
      "Current_batch_loss: 99.74528\n",
      "Current_batch_loss: 100.77171\n",
      "Current_batch_loss: 96.58389\n",
      "Current_batch_loss: 42.74566\n",
      "Current_batch_loss: 174.38875\n",
      "Current_batch_loss: 143.96587\n",
      "Current_batch_loss: 157.9209\n",
      "Current_batch_loss: 88.86954\n",
      "Current_batch_loss: 27.09529\n",
      "Current_batch_loss: 74.79233\n",
      "Current_batch_loss: 100.45438\n",
      "Current_batch_loss: 73.3012\n",
      "Current_batch_loss: 117.66089\n",
      "Current_batch_loss: 40.61255\n",
      "Current_batch_loss: 87.2008\n",
      "Current_batch_loss: 108.43086\n",
      "Current_batch_loss: 62.04235\n",
      "Current_batch_loss: 25.44044\n",
      "Current_batch_loss: 40.31117\n",
      "Current_batch_loss: 120.90495\n",
      "Current_batch_loss: 68.58067\n",
      "Current_batch_loss: 104.00727\n",
      "Current_batch_loss: 106.0748\n",
      "Current_batch_loss: 75.93018\n",
      "Current_batch_loss: 99.17198\n",
      "Current_batch_loss: 51.58338\n",
      "Current_batch_loss: 33.92042\n",
      "Current_batch_loss: 70.43582\n",
      "Current_batch_loss: 88.60847\n",
      "Current_batch_loss: 49.4122\n",
      "Current_batch_loss: 91.00967\n",
      "Current_batch_loss: 80.85162\n",
      "Current_batch_loss: 73.68579\n",
      "Current_batch_loss: 31.68527\n",
      "Current_batch_loss: 65.45507\n",
      "Current_batch_loss: 93.52431\n",
      "Current_batch_loss: 106.35403\n",
      "Current_batch_loss: 82.09769\n",
      "Current_batch_loss: 109.62895\n",
      "Current_batch_loss: 52.42444\n",
      "Current_batch_loss: 79.95792\n",
      "Current_batch_loss: 44.71079\n",
      "Current_batch_loss: 86.81884\n",
      "Current_batch_loss: 176.78714\n",
      "Current_batch_loss: 45.25869\n",
      "Current_batch_loss: 91.06025\n",
      "Current_batch_loss: 46.39358\n",
      "Current_batch_loss: 56.27551\n",
      "Current_batch_loss: 58.11406\n",
      "Current_batch_loss: 25.71637\n",
      "Current_batch_loss: 58.38389\n",
      "Current_batch_loss: 73.92294\n",
      "Current_batch_loss: 91.43565\n",
      "Current_batch_loss: 52.21676\n",
      "Current_batch_loss: 122.85297\n",
      "Current_batch_loss: 35.67303\n",
      "Current_batch_loss: 82.4253\n",
      "Current_batch_loss: 51.43758\n",
      "Current_batch_loss: 78.36001\n",
      "Current_batch_loss: 76.50115\n",
      "Current_batch_loss: 50.28798\n",
      "Current_batch_loss: 95.3728\n",
      "Current_batch_loss: 62.29872\n",
      "Current_batch_loss: 67.95845\n",
      "Current_batch_loss: 102.90993\n",
      "Current_batch_loss: 37.27145\n",
      "Current_batch_loss: 48.69278\n",
      "Current_batch_loss: 31.18453\n",
      "Current_batch_loss: 90.61868\n",
      "Current_batch_loss: 147.36595\n",
      "Current_batch_loss: 60.50352\n",
      "Current_batch_loss: 46.48245\n",
      "Current_batch_loss: 33.41859\n",
      "Current_batch_loss: 29.76317\n",
      "Current_batch_loss: 31.64858\n",
      "Current_batch_loss: 68.91939\n",
      "Current_batch_loss: 64.96537\n",
      "Current_batch_loss: 96.24213\n",
      "Current_batch_loss: 261.72946\n",
      "Current_batch_loss: 27.72883\n",
      "Current_batch_loss: 48.7501\n",
      "Current_batch_loss: 42.55639\n",
      "Current_batch_loss: 65.99066\n",
      "Current_batch_loss: 74.9054\n",
      "Current_batch_loss: 58.24783\n",
      "Current_batch_loss: 36.12824\n",
      "Current_batch_loss: 69.9112\n",
      "Current_batch_loss: 61.51355\n",
      "Current_batch_loss: 80.90805\n",
      "Current_batch_loss: 72.23662\n",
      "Current_batch_loss: 55.58191\n",
      "Current_batch_loss: 84.92365\n",
      "Current_batch_loss: 28.13067\n",
      "Current_batch_loss: 104.19611\n",
      "Current_batch_loss: 62.27726\n",
      "Current_batch_loss: 53.33033\n",
      "Current_batch_loss: 137.26994\n",
      "Current_batch_loss: 77.19063\n",
      "Current_batch_loss: 75.16172\n",
      "Current_batch_loss: 62.3414\n",
      "Current_batch_loss: 83.70227\n",
      "Current_batch_loss: 127.72378\n",
      "Current_batch_loss: 78.71539\n",
      "Current_batch_loss: 63.48297\n",
      "Current_batch_loss: 33.86828\n",
      "Current_batch_loss: 94.80514\n",
      "Current_batch_loss: 28.75121\n",
      "Current_batch_loss: 62.25555\n",
      "Current_batch_loss: 130.15604\n",
      "Current_batch_loss: 19.04111\n",
      "Current_batch_loss: 83.6944\n",
      "Current_batch_loss: 38.75191\n",
      "Current_batch_loss: 86.23746\n",
      "Current_batch_loss: 38.336\n",
      "Current_batch_loss: 47.07997\n",
      "Current_batch_loss: 46.74321\n",
      "Current_batch_loss: 63.94236\n",
      "Current_batch_loss: 51.91504\n",
      "Current_batch_loss: 43.70839\n",
      "Current_batch_loss: 71.73198\n",
      "Current_batch_loss: 29.32047\n",
      "Current_batch_loss: 71.42632\n",
      "Current_batch_loss: 88.38123\n",
      "Current_batch_loss: 70.60991\n",
      "Current_batch_loss: 46.2307\n",
      "Current_batch_loss: 111.90074\n",
      "Current_batch_loss: 34.5341\n",
      "Current_batch_loss: 60.34091\n",
      "Current_batch_loss: 43.35248\n",
      "Current_batch_loss: 36.96352\n",
      "Current_batch_loss: 51.86631\n",
      "Current_batch_loss: 62.92335\n",
      "Current_batch_loss: 50.4536\n",
      "Current_batch_loss: 69.32941\n",
      "Current_batch_loss: 32.75348\n",
      "Current_batch_loss: 27.15059\n",
      "Current_batch_loss: 65.17772\n",
      "Current_batch_loss: 22.9833\n",
      "Current_batch_loss: 74.78304\n",
      "Current_batch_loss: 37.38073\n",
      "Current_batch_loss: 43.44723\n",
      "Current_batch_loss: 42.14597\n",
      "Current_batch_loss: 50.5885\n",
      "Current_batch_loss: 78.82531\n",
      "Current_batch_loss: 78.10606\n",
      "Current_batch_loss: 18.21853\n",
      "Current_batch_loss: 30.98923\n",
      "Current_batch_loss: 71.55984\n",
      "Current_batch_loss: 8.55465\n",
      "Current_batch_loss: 36.50781\n",
      "Current_batch_loss: 26.6591\n",
      "Current_batch_loss: 67.27006\n",
      "Current_batch_loss: 38.81907\n",
      "Current_batch_loss: 51.2483\n",
      "Current_batch_loss: 90.27315\n",
      "Current_batch_loss: 49.59167\n",
      "Current_batch_loss: 60.22918\n",
      "Current_batch_loss: 46.06113\n",
      "Current_batch_loss: 37.16064\n",
      "Current_batch_loss: 59.61432\n",
      "Current_batch_loss: 24.96704\n",
      "Current_batch_loss: 56.79981\n",
      "Current_batch_loss: 51.39809\n",
      "Current_batch_loss: 23.44972\n",
      "Current_batch_loss: 38.33266\n",
      "Current_batch_loss: 50.34626\n",
      "Current_batch_loss: 16.08108\n",
      "Current_batch_loss: 29.14927\n",
      "Current_batch_loss: 47.03175\n",
      "Current_batch_loss: 34.74727\n",
      "Current_batch_loss: 102.95476\n",
      "Current_batch_loss: 16.35724\n",
      "Current_batch_loss: 85.54764\n",
      "Current_batch_loss: 46.28459\n",
      "Current_batch_loss: 66.75237\n",
      "Current_batch_loss: 32.16345\n",
      "Current_batch_loss: 54.74051\n",
      "Current_batch_loss: 19.72335\n",
      "Current_batch_loss: 28.37643\n",
      "Current_batch_loss: 54.25799\n",
      "Current_batch_loss: 28.18801\n",
      "Current_batch_loss: 44.08376\n",
      "Current_batch_loss: 18.29051\n",
      "Current_batch_loss: 49.12701\n",
      "Current_batch_loss: 34.12891\n",
      "Current_batch_loss: 23.67601\n",
      "Current_batch_loss: 47.59749\n",
      "Current_batch_loss: 57.74856\n",
      "Current_batch_loss: 78.83549\n",
      "Current_batch_loss: 36.61437\n",
      "Current_batch_loss: 46.99555\n",
      "Current_batch_loss: 16.17575\n",
      "Current_batch_loss: 19.54408\n",
      "Current_batch_loss: 51.32935\n",
      "Current_batch_loss: 34.09955\n",
      "Current_batch_loss: 56.39617\n",
      "Current_batch_loss: 48.34742\n",
      "Current_batch_loss: 32.06502\n",
      "Current_batch_loss: 70.4482\n",
      "Current_batch_loss: 64.28016\n",
      "Current_batch_loss: 32.81093\n",
      "Current_batch_loss: 27.11901\n",
      "Current_batch_loss: 40.2723\n",
      "Current_batch_loss: 52.83748\n",
      "Current_batch_loss: 32.46322\n",
      "Current_batch_loss: 40.24316\n",
      "Current_batch_loss: 35.7208\n",
      "Current_batch_loss: 65.04813\n",
      "Current_batch_loss: 30.2508\n",
      "Current_batch_loss: 39.35241\n",
      "Current_batch_loss: 36.05362\n",
      "Current_batch_loss: 39.9457\n",
      "Current_batch_loss: 22.45012\n",
      "Current_batch_loss: 63.64323\n",
      "Current_batch_loss: 53.17911\n",
      "Current_batch_loss: 23.91093\n",
      "Current_batch_loss: 8.37182\n",
      "Current_batch_loss: 38.00393\n",
      "Current_batch_loss: 15.02071\n",
      "Current_batch_loss: 20.27654\n",
      "Current_batch_loss: 26.10765\n",
      "Current_batch_loss: 56.4404\n",
      "Current_batch_loss: 45.35077\n",
      "Current_batch_loss: 25.27393\n",
      "Current_batch_loss: 10.46837\n",
      "Current_batch_loss: 65.05435\n",
      "Current_batch_loss: 7.84809\n",
      "Current_batch_loss: 29.85884\n",
      "Current_batch_loss: 48.91637\n",
      "Current_batch_loss: 28.11263\n",
      "Current_batch_loss: 27.68744\n",
      "Current_batch_loss: 23.6103\n",
      "Current_batch_loss: 46.88189\n",
      "Current_batch_loss: 11.22423\n",
      "Current_batch_loss: 81.32205\n",
      "Current_batch_loss: 42.49707\n",
      "Current_batch_loss: 0.0133\n",
      "Current_batch_loss: 17.89542\n",
      "Current_batch_loss: 44.11888\n",
      "Current_batch_loss: 31.22603\n",
      "Current_batch_loss: 10.80032\n",
      "Current_batch_loss: 9.90968\n",
      "Current_batch_loss: 11.43927\n",
      "Current_batch_loss: 68.11028\n",
      "Current_batch_loss: 39.94542\n",
      "Current_batch_loss: 26.83988\n",
      "Current_batch_loss: 19.97016\n",
      "Current_batch_loss: 30.63037\n",
      "Current_batch_loss: 22.44813\n",
      "Current_batch_loss: 33.34953\n",
      "Current_batch_loss: 33.77445\n",
      "Current_batch_loss: 15.71484\n",
      "Current_batch_loss: 89.18503\n",
      "Current_batch_loss: 61.19192\n",
      "Current_batch_loss: 35.30921\n",
      "Current_batch_loss: 21.70205\n",
      "Current_batch_loss: 0.01092\n",
      "Current_batch_loss: 49.1134\n",
      "Current_batch_loss: 43.66253\n",
      "Current_batch_loss: 54.12637\n",
      "Current_batch_loss: 0.0118\n",
      "Current_batch_loss: 5.9501\n",
      "Current_batch_loss: 47.6054\n",
      "Current_batch_loss: 19.62651\n",
      "Current_batch_loss: 0.0098\n",
      "Current_batch_loss: 51.92441\n",
      "Current_batch_loss: 72.1219\n",
      "Current_batch_loss: 45.00735\n",
      "Current_batch_loss: 33.40225\n",
      "Current_batch_loss: 29.50953\n",
      "Current_batch_loss: 76.35918\n",
      "Current_batch_loss: 18.81008\n",
      "Current_batch_loss: 21.15457\n",
      "Current_batch_loss: 21.31201\n",
      "Current_batch_loss: 32.14738\n",
      "Current_batch_loss: 11.14137\n",
      "Current_batch_loss: 50.74141\n",
      "Current_batch_loss: 26.72803\n",
      "Current_batch_loss: 31.25589\n",
      "Current_batch_loss: 7.68681\n",
      "Current_batch_loss: 7.6822\n",
      "Current_batch_loss: 17.81456\n",
      "Current_batch_loss: 81.45257\n",
      "Current_batch_loss: 19.96786\n",
      "Current_batch_loss: 14.85787\n",
      "Current_batch_loss: 12.16596\n",
      "Current_batch_loss: 30.26157\n",
      "Current_batch_loss: 37.12363\n",
      "Current_batch_loss: 33.26719\n",
      "Current_batch_loss: 12.18424\n",
      "Current_batch_loss: 43.19507\n",
      "Current_batch_loss: 63.52568\n",
      "Current_batch_loss: 23.45693\n",
      "Current_batch_loss: 17.61377\n",
      "Current_batch_loss: 43.87308\n",
      "Current_batch_loss: 31.97159\n",
      "Current_batch_loss: 6.94824\n",
      "Current_batch_loss: 58.68829\n",
      "Current_batch_loss: 22.02975\n",
      "Current_batch_loss: 35.5149\n",
      "Current_batch_loss: 30.2133\n",
      "Current_batch_loss: 51.65498\n",
      "Current_batch_loss: 36.02683\n",
      "Current_batch_loss: 47.8072\n",
      "Current_batch_loss: 8.22685\n",
      "Current_batch_loss: 0.01412\n",
      "Current_batch_loss: 54.23079\n",
      "Current_batch_loss: 15.13769\n",
      "Current_batch_loss: 16.57866\n",
      "Current_batch_loss: 35.95748\n",
      "Current_batch_loss: 28.49595\n",
      "Current_batch_loss: 27.49701\n",
      "Current_batch_loss: 40.19726\n",
      "Current_batch_loss: 0.00896\n",
      "Current_batch_loss: 17.94822\n",
      "Current_batch_loss: 8.3428\n",
      "Current_batch_loss: 28.5144\n",
      "Current_batch_loss: 35.90741\n",
      "Current_batch_loss: 17.21045\n",
      "Current_batch_loss: 9.70592\n",
      "Current_batch_loss: 22.72271\n",
      "Current_batch_loss: 9.14536\n",
      "Current_batch_loss: 35.0433\n",
      "Current_batch_loss: 20.45873\n",
      "Current_batch_loss: 28.25834\n",
      "Current_batch_loss: 28.2912\n",
      "Current_batch_loss: 17.1547\n",
      "Current_batch_loss: 38.37769\n",
      "Current_batch_loss: 22.73156\n",
      "Current_batch_loss: 34.29538\n",
      "Current_batch_loss: 37.77488\n",
      "Current_batch_loss: 34.6215\n",
      "Current_batch_loss: 7.92851\n",
      "Current_batch_loss: 19.50829\n",
      "Current_batch_loss: 17.50996\n",
      "Current_batch_loss: 21.72684\n",
      "Current_batch_loss: 0.01279\n",
      "Current_batch_loss: 26.27503\n",
      "Current_batch_loss: 23.40386\n",
      "Current_batch_loss: 25.85853\n",
      "Current_batch_loss: 25.51935\n",
      "Current_batch_loss: 0.01537\n",
      "Current_batch_loss: 50.54505\n",
      "Current_batch_loss: 32.65575\n",
      "Current_batch_loss: 22.85344\n",
      "Current_batch_loss: 39.70651\n",
      "Current_batch_loss: 19.61063\n",
      "Current_batch_loss: 27.94066\n",
      "Current_batch_loss: 28.07372\n",
      "Current_batch_loss: 36.6748\n",
      "Current_batch_loss: 30.98188\n",
      "Current_batch_loss: 11.92192\n",
      "Current_batch_loss: 8.1687\n",
      "Current_batch_loss: 43.75575\n",
      "Current_batch_loss: 46.93771\n",
      "Current_batch_loss: 65.73519\n",
      "Current_batch_loss: 22.0098\n",
      "Current_batch_loss: 8.24201\n",
      "Current_batch_loss: 19.40277\n",
      "Current_batch_loss: 25.83086\n",
      "Current_batch_loss: 42.77576\n",
      "Current_batch_loss: 27.05319\n",
      "Current_batch_loss: 0.01022\n",
      "Current_batch_loss: 36.04054\n",
      "Current_batch_loss: 9.35485\n",
      "Current_batch_loss: 47.59006\n",
      "Current_batch_loss: 31.23789\n",
      "Current_batch_loss: 10.98342\n",
      "Current_batch_loss: 47.85004\n",
      "Current_batch_loss: 18.72571\n",
      "Current_batch_loss: 24.13383\n",
      "Current_batch_loss: 5.87484\n",
      "Current_batch_loss: 33.32454\n",
      "Current_batch_loss: 11.38928\n",
      "Current_batch_loss: 21.315\n",
      "Current_batch_loss: 8.44116\n",
      "Current_batch_loss: 0.01442\n",
      "Current_batch_loss: 7.56055\n",
      "Current_batch_loss: 7.91358\n",
      "Current_batch_loss: 36.54258\n",
      "Current_batch_loss: 32.85942\n",
      "Current_batch_loss: 16.27357\n",
      "Current_batch_loss: 30.537\n",
      "Current_batch_loss: 6.76471\n",
      "Current_batch_loss: 19.60497\n",
      "Current_batch_loss: 0.01156\n",
      "Current_batch_loss: 9.45001\n",
      "Current_batch_loss: 0.00727\n",
      "Current_batch_loss: 0.01203\n",
      "Current_batch_loss: 24.82287\n",
      "Current_batch_loss: 26.23871\n",
      "Current_batch_loss: 6.43344\n",
      "Current_batch_loss: 9.60283\n",
      "Current_batch_loss: 17.41789\n",
      "Current_batch_loss: 13.23003\n",
      "Current_batch_loss: 10.83427\n",
      "Current_batch_loss: 9.45954\n",
      "Current_batch_loss: 14.16503\n",
      "Current_batch_loss: 9.03987\n",
      "Current_batch_loss: 22.34697\n",
      "Current_batch_loss: 6.67194\n",
      "Current_batch_loss: 0.01203\n",
      "Current_batch_loss: 0.01227\n",
      "Current_batch_loss: 7.92037\n",
      "Current_batch_loss: 21.52647\n",
      "Current_batch_loss: 25.62148\n",
      "Current_batch_loss: 12.81779\n",
      "Current_batch_loss: 24.00141\n",
      "Current_batch_loss: 19.1403\n",
      "Current_batch_loss: 7.14888\n",
      "Current_batch_loss: 19.03436\n",
      "Current_batch_loss: 49.2942\n",
      "Current_batch_loss: 8.50285\n",
      "Current_batch_loss: 35.14588\n",
      "Current_batch_loss: 0.01101\n",
      "Current_batch_loss: 0.0105\n",
      "Current_batch_loss: 0.01008\n",
      "Current_batch_loss: 21.83102\n",
      "Current_batch_loss: 9.53553\n",
      "Current_batch_loss: 44.43559\n",
      "Current_batch_loss: 31.39283\n",
      "Current_batch_loss: 0.01252\n",
      "Current_batch_loss: 30.46464\n",
      "Current_batch_loss: 26.18699\n",
      "Current_batch_loss: 10.7108\n",
      "Current_batch_loss: 22.21098\n",
      "Current_batch_loss: 35.07554\n",
      "Current_batch_loss: 24.4043\n",
      "Current_batch_loss: 24.93517\n",
      "Current_batch_loss: 54.09966\n",
      "Current_batch_loss: 6.4339\n",
      "Current_batch_loss: 0.01183\n",
      "Current_batch_loss: 60.61195\n",
      "Current_batch_loss: 12.66894\n",
      "Current_batch_loss: 16.97947\n",
      "Current_batch_loss: 8.06656\n",
      "Current_batch_loss: 10.77172\n",
      "Current_batch_loss: 14.31288\n",
      "Current_batch_loss: 24.64469\n",
      "Current_batch_loss: 0.01065\n",
      "Current_batch_loss: 39.57053\n",
      "Current_batch_loss: 16.96479\n",
      "Current_batch_loss: 9.35965\n",
      "Current_batch_loss: 12.77986\n",
      "Current_batch_loss: 7.93665\n",
      "Current_batch_loss: 8.48525\n",
      "Current_batch_loss: 19.62399\n",
      "Current_batch_loss: 19.44082\n",
      "Current_batch_loss: 7.98288\n",
      "Current_batch_loss: 9.19954\n",
      "Current_batch_loss: 24.05083\n",
      "Current_batch_loss: 10.45411\n",
      "Current_batch_loss: 7.16673\n",
      "Current_batch_loss: 0.01012\n",
      "Current_batch_loss: 18.597\n",
      "Current_batch_loss: 14.7769\n",
      "Current_batch_loss: 0.01294\n",
      "Current_batch_loss: 0.00881\n",
      "Current_batch_loss: 16.30357\n",
      "Current_batch_loss: 7.12883\n",
      "Current_batch_loss: 25.462\n",
      "Current_batch_loss: 24.33606\n",
      "Current_batch_loss: 8.19734\n",
      "Current_batch_loss: 0.00807\n",
      "Current_batch_loss: 0.00926\n",
      "Current_batch_loss: 0.00692\n",
      "Current_batch_loss: 6.82411\n",
      "Current_batch_loss: 0.00868\n",
      "Current_batch_loss: 8.03964\n",
      "Current_batch_loss: 0.01162\n",
      "Current_batch_loss: 12.00976\n",
      "Current_batch_loss: 9.39988\n",
      "Current_batch_loss: 0.01052\n",
      "Current_batch_loss: 0.01804\n",
      "Current_batch_loss: 11.46145\n",
      "Current_batch_loss: 9.14041\n",
      "Current_batch_loss: 10.5281\n",
      "Current_batch_loss: 13.29315\n",
      "Current_batch_loss: 0.01547\n",
      "Current_batch_loss: 8.80173\n",
      "Current_batch_loss: 0.01095\n",
      "Current_batch_loss: 12.58168\n",
      "Current_batch_loss: 12.44138\n",
      "Current_batch_loss: 0.00808\n",
      "Current_batch_loss: 0.00848\n",
      "Current_batch_loss: 22.7366\n",
      "Current_batch_loss: 13.97619\n",
      "Current_batch_loss: 26.63143\n",
      "Current_batch_loss: 51.99031\n",
      "Current_batch_loss: 22.2432\n",
      "Current_batch_loss: 15.50619\n",
      "Current_batch_loss: 11.37964\n",
      "Current_batch_loss: 0.00992\n",
      "Current_batch_loss: 7.56289\n",
      "Current_batch_loss: 29.04199\n",
      "Current_batch_loss: 18.58293\n",
      "Current_batch_loss: 0.00983\n",
      "Current_batch_loss: 9.56303\n",
      "Current_batch_loss: 16.73649\n",
      "Current_batch_loss: 10.19025\n",
      "Current_batch_loss: 15.40436\n",
      "Current_batch_loss: 11.90107\n",
      "Current_batch_loss: 0.00726\n",
      "Current_batch_loss: 0.01032\n",
      "Current_batch_loss: 8.45778\n",
      "Current_batch_loss: 9.70594\n",
      "Current_batch_loss: 28.05778\n",
      "Current_batch_loss: 0.00959\n",
      "Current_batch_loss: 0.00662\n",
      "Current_batch_loss: 0.00871\n",
      "Current_batch_loss: 7.13391\n",
      "Current_batch_loss: 8.94433\n",
      "Current_batch_loss: 0.0146\n",
      "Current_batch_loss: 0.00871\n",
      "Current_batch_loss: 0.00902\n",
      "Current_batch_loss: 21.74817\n",
      "Current_batch_loss: 21.12016\n",
      "Current_batch_loss: 25.24244\n",
      "Current_batch_loss: 0.00781\n",
      "Current_batch_loss: 0.00638\n",
      "Current_batch_loss: 0.00823\n",
      "Current_batch_loss: 15.46171\n",
      "Current_batch_loss: 0.01115\n",
      "Current_batch_loss: 0.00928\n",
      "Current_batch_loss: 5.41741\n",
      "Current_batch_loss: 28.60611\n",
      "Current_batch_loss: 0.00948\n",
      "Current_batch_loss: 9.68446\n",
      "Current_batch_loss: 0.00743\n",
      "Current_batch_loss: 0.0125\n",
      "Current_batch_loss: 0.0099\n",
      "Current_batch_loss: 5.97873\n",
      "Current_batch_loss: 25.3678\n",
      "Current_batch_loss: 0.00739\n",
      "Current_batch_loss: 16.36354\n",
      "Current_batch_loss: 0.01134\n",
      "Current_batch_loss: 10.85842\n",
      "Current_batch_loss: 9.76606\n",
      "Current_batch_loss: 13.913\n",
      "Current_batch_loss: 0.01254\n",
      "Current_batch_loss: 0.00677\n",
      "Current_batch_loss: 0.00883\n",
      "Current_batch_loss: 12.7319\n",
      "Current_batch_loss: 6.36541\n",
      "Current_batch_loss: 11.90608\n",
      "Current_batch_loss: 0.00903\n",
      "Current_batch_loss: 20.82482\n",
      "Current_batch_loss: 21.39311\n",
      "Current_batch_loss: 17.80151\n",
      "Current_batch_loss: 13.85268\n",
      "Current_batch_loss: 0.01128\n",
      "Current_batch_loss: 0.00867\n",
      "Current_batch_loss: 11.54909\n",
      "Current_batch_loss: 10.79546\n",
      "Current_batch_loss: 0.01103\n",
      "Current_batch_loss: 15.20082\n",
      "Current_batch_loss: 14.15056\n",
      "Current_batch_loss: 11.97337\n",
      "Current_batch_loss: 34.88004\n",
      "Current_batch_loss: 0.01019\n",
      "Current_batch_loss: 7.70181\n",
      "Current_batch_loss: 0.00865\n",
      "Current_batch_loss: 8.82677\n",
      "Current_batch_loss: 15.45993\n",
      "Current_batch_loss: 5.64123\n",
      "Current_batch_loss: 22.01686\n",
      "Current_batch_loss: 11.23283\n",
      "Current_batch_loss: 0.00793\n",
      "Current_batch_loss: 24.63348\n",
      "Current_batch_loss: 6.01432\n",
      "Current_batch_loss: 33.64109\n",
      "Current_batch_loss: 14.96231\n",
      "Current_batch_loss: 23.18934\n",
      "Current_batch_loss: 12.19474\n",
      "Current_batch_loss: 17.94699\n",
      "Current_batch_loss: 16.60693\n",
      "Current_batch_loss: 0.01147\n",
      "Current_batch_loss: 13.10093\n",
      "Current_batch_loss: 7.55949\n",
      "Current_batch_loss: 0.00745\n",
      "Current_batch_loss: 0.00672\n",
      "Current_batch_loss: 0.00784\n",
      "Current_batch_loss: 10.75728\n",
      "Current_batch_loss: 10.14823\n",
      "Current_batch_loss: 11.9304\n",
      "Current_batch_loss: 0.00954\n",
      "Current_batch_loss: 11.40618\n",
      "Current_batch_loss: 0.01278\n",
      "Current_batch_loss: 0.00956\n",
      "Current_batch_loss: 0.00691\n",
      "Current_batch_loss: 8.21718\n",
      "Current_batch_loss: 0.01034\n",
      "Current_batch_loss: 0.00879\n",
      "Current_batch_loss: 9.38487\n",
      "Current_batch_loss: 0.01196\n",
      "Current_batch_loss: 0.01146\n",
      "Current_batch_loss: 19.71635\n",
      "Current_batch_loss: 21.442\n",
      "Current_batch_loss: 0.00667\n",
      "Current_batch_loss: 0.00979\n",
      "Current_batch_loss: 12.53391\n",
      "Current_batch_loss: 23.65217\n",
      "Current_batch_loss: 0.00974\n",
      "Current_batch_loss: 11.90658\n",
      "Current_batch_loss: 18.6629\n",
      "Current_batch_loss: 0.00856\n",
      "Current_batch_loss: 24.33965\n",
      "Current_batch_loss: 10.74755\n",
      "Current_batch_loss: 18.64855\n",
      "Current_batch_loss: 0.01115\n",
      "Current_batch_loss: 0.00759\n",
      "Current_batch_loss: 0.00922\n",
      "Current_batch_loss: 14.86769\n",
      "Current_batch_loss: 0.00657\n",
      "Current_batch_loss: 10.03543\n",
      "Current_batch_loss: 0.0107\n",
      "Current_batch_loss: 0.00894\n",
      "Current_batch_loss: 11.29378\n",
      "Current_batch_loss: 8.3937\n",
      "Current_batch_loss: 9.00291\n",
      "Current_batch_loss: 16.81547\n",
      "Current_batch_loss: 0.00939\n",
      "Current_batch_loss: 14.40302\n",
      "Current_batch_loss: 20.06625\n",
      "Current_batch_loss: 9.16876\n",
      "Current_batch_loss: 11.49685\n",
      "Current_batch_loss: 0.01063\n",
      "Current_batch_loss: 8.87683\n",
      "Current_batch_loss: 0.00951\n",
      "Current_batch_loss: 0.00854\n",
      "Current_batch_loss: 9.95782\n",
      "Current_batch_loss: 21.42652\n",
      "Current_batch_loss: 0.00998\n",
      "Current_batch_loss: 10.46408\n",
      "Current_batch_loss: 0.01124\n",
      "Current_batch_loss: 0.0072\n",
      "Current_batch_loss: 0.00731\n",
      "Current_batch_loss: 0.0073\n",
      "Current_batch_loss: 0.00796\n",
      "Current_batch_loss: 0.00736\n",
      "Current_batch_loss: 0.0076\n",
      "Current_batch_loss: 47.58016\n",
      "Current_batch_loss: 0.00784\n",
      "Current_batch_loss: 0.00868\n",
      "Current_batch_loss: 0.01054\n",
      "Current_batch_loss: 0.00878\n",
      "Current_batch_loss: 7.73531\n",
      "Current_batch_loss: 18.6024\n",
      "Current_batch_loss: 8.15616\n",
      "Current_batch_loss: 0.00792\n",
      "Current_batch_loss: 24.61502\n",
      "Current_batch_loss: 0.00651\n",
      "Current_batch_loss: 12.37005\n",
      "Current_batch_loss: 16.50559\n",
      "Current_batch_loss: 11.02244\n",
      "Current_batch_loss: 0.00864\n",
      "Current_batch_loss: 0.00476\n",
      "Current_batch_loss: 0.00611\n",
      "Current_batch_loss: 0.00825\n",
      "Current_batch_loss: 0.00844\n",
      "Current_batch_loss: 16.99657\n",
      "Current_batch_loss: 0.01053\n",
      "Current_batch_loss: 31.06687\n",
      "Current_batch_loss: 0.00854\n",
      "Current_batch_loss: 11.0421\n",
      "Current_batch_loss: 6.59237\n",
      "Current_batch_loss: 0.00742\n",
      "Current_batch_loss: 0.00787\n",
      "Current_batch_loss: 11.7306\n",
      "Current_batch_loss: 8.24015\n",
      "Current_batch_loss: 0.00558\n",
      "Current_batch_loss: 7.98102\n",
      "Current_batch_loss: 12.02357\n",
      "Current_batch_loss: 0.00463\n",
      "Current_batch_loss: 8.69354\n",
      "Current_batch_loss: 0.00727\n",
      "Current_batch_loss: 0.00805\n",
      "Current_batch_loss: 0.00754\n",
      "Current_batch_loss: 8.60867\n",
      "Current_batch_loss: 6.88878\n",
      "Current_batch_loss: 0.0094\n",
      "Current_batch_loss: 0.00768\n",
      "Current_batch_loss: 15.60481\n",
      "Current_batch_loss: 0.00936\n",
      "Current_batch_loss: 12.17804\n",
      "Current_batch_loss: 0.00895\n",
      "Current_batch_loss: 0.00623\n",
      "Current_batch_loss: 0.0079\n",
      "Current_batch_loss: 12.91391\n",
      "Current_batch_loss: 9.01974\n",
      "Current_batch_loss: 0.00615\n",
      "Current_batch_loss: 0.00905\n",
      "Current_batch_loss: 0.01182\n",
      "Current_batch_loss: 0.00777\n",
      "Current_batch_loss: 10.39835\n",
      "Current_batch_loss: 9.88389\n",
      "Current_batch_loss: 0.00798\n",
      "Current_batch_loss: 0.00805\n",
      "Current_batch_loss: 0.00857\n",
      "Current_batch_loss: 0.0075\n",
      "Current_batch_loss: 0.00747\n",
      "Current_batch_loss: 21.89893\n",
      "Current_batch_loss: 10.83721\n",
      "Current_batch_loss: 11.06133\n",
      "Current_batch_loss: 0.0053\n",
      "Current_batch_loss: 0.00616\n",
      "Current_batch_loss: 0.00945\n",
      "Current_batch_loss: 8.98839\n",
      "Current_batch_loss: 0.00567\n",
      "Current_batch_loss: 0.007\n",
      "Current_batch_loss: 11.93188\n",
      "Current_batch_loss: 0.00605\n",
      "Current_batch_loss: 10.66214\n",
      "Current_batch_loss: 0.01181\n",
      "Current_batch_loss: 0.00972\n",
      "Current_batch_loss: 4.50171\n",
      "Current_batch_loss: 0.00665\n",
      "Current_batch_loss: 0.01022\n",
      "Current_batch_loss: 0.00879\n",
      "Current_batch_loss: 6.02692\n",
      "Current_batch_loss: 0.00885\n",
      "Current_batch_loss: 0.01017\n",
      "Current_batch_loss: 0.0044\n",
      "Current_batch_loss: 0.00505\n",
      "Current_batch_loss: 0.00452\n",
      "Current_batch_loss: 9.96518\n",
      "Current_batch_loss: 0.01586\n",
      "Current_batch_loss: 14.999\n",
      "Current_batch_loss: 0.0088\n",
      "Current_batch_loss: 24.76887\n",
      "Current_batch_loss: 0.00436\n",
      "Current_batch_loss: 0.00818\n",
      "Current_batch_loss: 12.96418\n",
      "Current_batch_loss: 0.01179\n",
      "Current_batch_loss: 0.013\n",
      "Current_batch_loss: 14.68301\n",
      "Current_batch_loss: 0.00794\n",
      "Current_batch_loss: 15.15686\n",
      "Current_batch_loss: 12.23717\n",
      "Current_batch_loss: 0.00647\n",
      "Current_batch_loss: 0.00589\n",
      "Current_batch_loss: 0.00752\n",
      "Current_batch_loss: 6.80776\n",
      "Current_batch_loss: 10.85176\n",
      "Current_batch_loss: 0.00612\n",
      "Current_batch_loss: 0.00858\n",
      "Current_batch_loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [8:02:15<52:14:37, 7233.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_batch_loss: 0.00724\n",
      "Epoch #4 | Loss: 297.08622\n",
      "Current_batch_loss: 1038.198\n",
      "Current_batch_loss: 978.97961\n",
      "Current_batch_loss: 900.82178\n",
      "Current_batch_loss: 995.25458\n",
      "Current_batch_loss: 1044.84583\n",
      "Current_batch_loss: 939.13409\n",
      "Current_batch_loss: 1001.48804\n",
      "Current_batch_loss: 945.78723\n",
      "Current_batch_loss: 894.20941\n",
      "Current_batch_loss: 804.46832\n",
      "Current_batch_loss: 1011.46716\n",
      "Current_batch_loss: 938.51355\n",
      "Current_batch_loss: 906.55328\n",
      "Current_batch_loss: 890.73206\n",
      "Current_batch_loss: 956.48212\n",
      "Current_batch_loss: 945.9314\n",
      "Current_batch_loss: 1028.70215\n",
      "Current_batch_loss: 971.64465\n",
      "Current_batch_loss: 905.69269\n",
      "Current_batch_loss: 939.59607\n",
      "Current_batch_loss: 991.73816\n",
      "Current_batch_loss: 1033.88184\n",
      "Current_batch_loss: 1037.82092\n",
      "Current_batch_loss: 983.34039\n",
      "Current_batch_loss: 1000.99237\n",
      "Current_batch_loss: 959.10187\n",
      "Current_batch_loss: 843.77997\n",
      "Current_batch_loss: 878.09888\n",
      "Current_batch_loss: 927.27026\n",
      "Current_batch_loss: 879.05389\n",
      "Current_batch_loss: 972.16937\n",
      "Current_batch_loss: 908.01044\n",
      "Current_batch_loss: 982.03809\n",
      "Current_batch_loss: 1036.00989\n",
      "Current_batch_loss: 976.0188\n",
      "Current_batch_loss: 896.63605\n",
      "Current_batch_loss: 950.83398\n",
      "Current_batch_loss: 941.27307\n",
      "Current_batch_loss: 797.46423\n",
      "Current_batch_loss: 900.98181\n",
      "Current_batch_loss: 1004.57874\n",
      "Current_batch_loss: 862.17786\n",
      "Current_batch_loss: 876.23981\n",
      "Current_batch_loss: 897.66876\n",
      "Current_batch_loss: 833.1676\n",
      "Current_batch_loss: 791.13263\n",
      "Current_batch_loss: 731.04047\n",
      "Current_batch_loss: 809.41302\n",
      "Current_batch_loss: 978.10059\n",
      "Current_batch_loss: 918.41846\n",
      "Current_batch_loss: 880.56128\n",
      "Current_batch_loss: 818.2666\n",
      "Current_batch_loss: 978.07959\n",
      "Current_batch_loss: 902.03638\n",
      "Current_batch_loss: 896.49854\n",
      "Current_batch_loss: 1014.93719\n",
      "Current_batch_loss: 978.12042\n",
      "Current_batch_loss: 971.37494\n",
      "Current_batch_loss: 879.51093\n",
      "Current_batch_loss: 870.07562\n",
      "Current_batch_loss: 905.40704\n",
      "Current_batch_loss: 846.4939\n",
      "Current_batch_loss: 879.91162\n",
      "Current_batch_loss: 793.06677\n",
      "Current_batch_loss: 907.48248\n",
      "Current_batch_loss: 878.82513\n",
      "Current_batch_loss: 865.44836\n",
      "Current_batch_loss: 949.72266\n",
      "Current_batch_loss: 903.05719\n",
      "Current_batch_loss: 776.90723\n",
      "Current_batch_loss: 920.53589\n",
      "Current_batch_loss: 847.55615\n",
      "Current_batch_loss: 893.48297\n",
      "Current_batch_loss: 851.90869\n",
      "Current_batch_loss: 764.69287\n",
      "Current_batch_loss: 832.20764\n",
      "Current_batch_loss: 796.66968\n",
      "Current_batch_loss: 910.54779\n",
      "Current_batch_loss: 707.45782\n",
      "Current_batch_loss: 963.07288\n",
      "Current_batch_loss: 896.20435\n",
      "Current_batch_loss: 907.78473\n",
      "Current_batch_loss: 953.27539\n",
      "Current_batch_loss: 918.18665\n",
      "Current_batch_loss: 854.43933\n",
      "Current_batch_loss: 896.32825\n",
      "Current_batch_loss: 870.8197\n",
      "Current_batch_loss: 769.11041\n",
      "Current_batch_loss: 855.39062\n",
      "Current_batch_loss: 722.58545\n",
      "Current_batch_loss: 933.22687\n",
      "Current_batch_loss: 662.59167\n",
      "Current_batch_loss: 960.80792\n",
      "Current_batch_loss: 920.198\n",
      "Current_batch_loss: 844.6665\n",
      "Current_batch_loss: 919.88373\n",
      "Current_batch_loss: 804.72003\n",
      "Current_batch_loss: 845.28912\n",
      "Current_batch_loss: 833.73309\n",
      "Current_batch_loss: 789.65729\n",
      "Current_batch_loss: 902.42596\n",
      "Current_batch_loss: 882.99493\n",
      "Current_batch_loss: 838.97693\n",
      "Current_batch_loss: 866.91583\n",
      "Current_batch_loss: 893.81934\n",
      "Current_batch_loss: 804.34558\n",
      "Current_batch_loss: 818.05322\n",
      "Current_batch_loss: 740.3869\n",
      "Current_batch_loss: 914.93341\n",
      "Current_batch_loss: 877.83875\n",
      "Current_batch_loss: 969.19214\n",
      "Current_batch_loss: 892.08142\n",
      "Current_batch_loss: 907.42023\n",
      "Current_batch_loss: 765.71912\n",
      "Current_batch_loss: 927.77319\n",
      "Current_batch_loss: 702.66388\n",
      "Current_batch_loss: 712.83179\n",
      "Current_batch_loss: 796.76587\n",
      "Current_batch_loss: 772.40613\n",
      "Current_batch_loss: 941.185\n",
      "Current_batch_loss: 785.29083\n",
      "Current_batch_loss: 658.78369\n",
      "Current_batch_loss: 832.32947\n",
      "Current_batch_loss: 785.48779\n",
      "Current_batch_loss: 934.72015\n",
      "Current_batch_loss: 721.70856\n",
      "Current_batch_loss: 715.00006\n",
      "Current_batch_loss: 915.68933\n",
      "Current_batch_loss: 780.17627\n",
      "Current_batch_loss: 818.84955\n",
      "Current_batch_loss: 908.19427\n",
      "Current_batch_loss: 710.55353\n",
      "Current_batch_loss: 894.04462\n",
      "Current_batch_loss: 946.88989\n",
      "Current_batch_loss: 827.24451\n",
      "Current_batch_loss: 829.38373\n",
      "Current_batch_loss: 750.95447\n",
      "Current_batch_loss: 841.8822\n",
      "Current_batch_loss: 769.12732\n",
      "Current_batch_loss: 882.34613\n",
      "Current_batch_loss: 830.86243\n",
      "Current_batch_loss: 763.92896\n",
      "Current_batch_loss: 881.25714\n",
      "Current_batch_loss: 832.30042\n",
      "Current_batch_loss: 934.27307\n",
      "Current_batch_loss: 873.07117\n",
      "Current_batch_loss: 428.96863\n",
      "Current_batch_loss: 816.41791\n",
      "Current_batch_loss: 818.78302\n",
      "Current_batch_loss: 813.59595\n",
      "Current_batch_loss: 879.84375\n",
      "Current_batch_loss: 887.68262\n",
      "Current_batch_loss: 788.89233\n",
      "Current_batch_loss: 891.92175\n",
      "Current_batch_loss: 825.05389\n",
      "Current_batch_loss: 831.30371\n",
      "Current_batch_loss: 698.12537\n",
      "Current_batch_loss: 715.72186\n",
      "Current_batch_loss: 858.61261\n",
      "Current_batch_loss: 901.93945\n",
      "Current_batch_loss: 690.97247\n",
      "Current_batch_loss: 819.99506\n",
      "Current_batch_loss: 887.75342\n",
      "Current_batch_loss: 915.26788\n",
      "Current_batch_loss: 870.15076\n",
      "Current_batch_loss: 810.69421\n",
      "Current_batch_loss: 860.35754\n",
      "Current_batch_loss: 814.26471\n",
      "Current_batch_loss: 792.22314\n",
      "Current_batch_loss: 677.96814\n",
      "Current_batch_loss: 703.4433\n",
      "Current_batch_loss: 534.88599\n",
      "Current_batch_loss: 913.30292\n",
      "Current_batch_loss: 815.47455\n",
      "Current_batch_loss: 668.03107\n",
      "Current_batch_loss: 780.61353\n",
      "Current_batch_loss: 810.95557\n",
      "Current_batch_loss: 871.73724\n",
      "Current_batch_loss: 816.87494\n",
      "Current_batch_loss: 915.50543\n",
      "Current_batch_loss: 795.94067\n",
      "Current_batch_loss: 829.5708\n",
      "Current_batch_loss: 827.61267\n",
      "Current_batch_loss: 817.85468\n",
      "Current_batch_loss: 811.0271\n",
      "Current_batch_loss: 773.30902\n",
      "Current_batch_loss: 901.61731\n",
      "Current_batch_loss: 783.73792\n",
      "Current_batch_loss: 788.91504\n",
      "Current_batch_loss: 806.84637\n",
      "Current_batch_loss: 742.06482\n",
      "Current_batch_loss: 709.97192\n",
      "Current_batch_loss: 688.49152\n",
      "Current_batch_loss: 760.3833\n",
      "Current_batch_loss: 818.95135\n",
      "Current_batch_loss: 777.83789\n",
      "Current_batch_loss: 789.62653\n",
      "Current_batch_loss: 570.00269\n",
      "Current_batch_loss: 781.33551\n",
      "Current_batch_loss: 930.05542\n",
      "Current_batch_loss: 827.98029\n",
      "Current_batch_loss: 838.59998\n",
      "Current_batch_loss: 738.36035\n",
      "Current_batch_loss: 769.18634\n",
      "Current_batch_loss: 631.05298\n",
      "Current_batch_loss: 870.46277\n",
      "Current_batch_loss: 738.79376\n",
      "Current_batch_loss: 805.76434\n",
      "Current_batch_loss: 872.98047\n",
      "Current_batch_loss: 691.72992\n",
      "Current_batch_loss: 797.18518\n",
      "Current_batch_loss: 685.50037\n",
      "Current_batch_loss: 583.80493\n",
      "Current_batch_loss: 698.54578\n",
      "Current_batch_loss: 790.56079\n",
      "Current_batch_loss: 744.04773\n",
      "Current_batch_loss: 699.4574\n",
      "Current_batch_loss: 690.56427\n",
      "Current_batch_loss: 733.46558\n",
      "Current_batch_loss: 801.07336\n",
      "Current_batch_loss: 618.1059\n",
      "Current_batch_loss: 658.07819\n",
      "Current_batch_loss: 847.3418\n",
      "Current_batch_loss: 780.7934\n",
      "Current_batch_loss: 742.01459\n",
      "Current_batch_loss: 557.78632\n",
      "Current_batch_loss: 750.94403\n",
      "Current_batch_loss: 784.76111\n",
      "Current_batch_loss: 614.26672\n",
      "Current_batch_loss: 775.75726\n",
      "Current_batch_loss: 711.45398\n",
      "Current_batch_loss: 652.43927\n",
      "Current_batch_loss: 687.20807\n",
      "Current_batch_loss: 786.94745\n",
      "Current_batch_loss: 606.63861\n",
      "Current_batch_loss: 873.95062\n",
      "Current_batch_loss: 710.40118\n",
      "Current_batch_loss: 818.92853\n",
      "Current_batch_loss: 770.98163\n",
      "Current_batch_loss: 643.68036\n",
      "Current_batch_loss: 691.69836\n",
      "Current_batch_loss: 677.3869\n",
      "Current_batch_loss: 887.26794\n",
      "Current_batch_loss: 811.07971\n",
      "Current_batch_loss: 725.29297\n",
      "Current_batch_loss: 693.73828\n",
      "Current_batch_loss: 665.25977\n",
      "Current_batch_loss: 736.39594\n",
      "Current_batch_loss: 776.74005\n",
      "Current_batch_loss: 818.03314\n",
      "Current_batch_loss: 827.81171\n",
      "Current_batch_loss: 566.30249\n",
      "Current_batch_loss: 742.34039\n",
      "Current_batch_loss: 797.08893\n",
      "Current_batch_loss: 782.78302\n",
      "Current_batch_loss: 754.22205\n",
      "Current_batch_loss: 574.69519\n",
      "Current_batch_loss: 750.19025\n",
      "Current_batch_loss: 803.51318\n",
      "Current_batch_loss: 741.57452\n",
      "Current_batch_loss: 744.75391\n",
      "Current_batch_loss: 656.97186\n",
      "Current_batch_loss: 795.91797\n",
      "Current_batch_loss: 675.36298\n",
      "Current_batch_loss: 756.81012\n",
      "Current_batch_loss: 650.61096\n",
      "Current_batch_loss: 732.7193\n",
      "Current_batch_loss: 624.24078\n",
      "Current_batch_loss: 744.16174\n",
      "Current_batch_loss: 674.72388\n",
      "Current_batch_loss: 539.54993\n",
      "Current_batch_loss: 779.47028\n",
      "Current_batch_loss: 518.21375\n",
      "Current_batch_loss: 601.94055\n",
      "Current_batch_loss: 693.98407\n",
      "Current_batch_loss: 920.01978\n",
      "Current_batch_loss: 763.13055\n",
      "Current_batch_loss: 635.68237\n",
      "Current_batch_loss: 783.18219\n",
      "Current_batch_loss: 746.33948\n",
      "Current_batch_loss: 787.12866\n",
      "Current_batch_loss: 852.92114\n",
      "Current_batch_loss: 649.71375\n",
      "Current_batch_loss: 737.93018\n",
      "Current_batch_loss: 771.41498\n",
      "Current_batch_loss: 695.94879\n",
      "Current_batch_loss: 669.60516\n",
      "Current_batch_loss: 959.32178\n",
      "Current_batch_loss: 701.7926\n",
      "Current_batch_loss: 752.28882\n",
      "Current_batch_loss: 627.1358\n",
      "Current_batch_loss: 751.078\n",
      "Current_batch_loss: 603.026\n",
      "Current_batch_loss: 738.99506\n",
      "Current_batch_loss: 653.90387\n",
      "Current_batch_loss: 700.56262\n",
      "Current_batch_loss: 687.85547\n",
      "Current_batch_loss: 703.8446\n",
      "Current_batch_loss: 720.28278\n",
      "Current_batch_loss: 713.85791\n",
      "Current_batch_loss: 753.92786\n",
      "Current_batch_loss: 743.35059\n",
      "Current_batch_loss: 711.38446\n",
      "Current_batch_loss: 701.27582\n",
      "Current_batch_loss: 712.38428\n",
      "Current_batch_loss: 812.3299\n",
      "Current_batch_loss: 799.31671\n",
      "Current_batch_loss: 742.78149\n",
      "Current_batch_loss: 635.00403\n",
      "Current_batch_loss: 656.03119\n",
      "Current_batch_loss: 719.31506\n",
      "Current_batch_loss: 566.45221\n",
      "Current_batch_loss: 758.56995\n",
      "Current_batch_loss: 573.16687\n",
      "Current_batch_loss: 686.88507\n",
      "Current_batch_loss: 726.86823\n",
      "Current_batch_loss: 644.31757\n",
      "Current_batch_loss: 744.71313\n",
      "Current_batch_loss: 802.51636\n",
      "Current_batch_loss: 634.71814\n",
      "Current_batch_loss: 795.92078\n",
      "Current_batch_loss: 510.18726\n",
      "Current_batch_loss: 676.6535\n",
      "Current_batch_loss: 665.95807\n",
      "Current_batch_loss: 634.55524\n",
      "Current_batch_loss: 568.9693\n",
      "Current_batch_loss: 570.08301\n",
      "Current_batch_loss: 671.56226\n",
      "Current_batch_loss: 687.19702\n",
      "Current_batch_loss: 700.10645\n",
      "Current_batch_loss: 559.80804\n",
      "Current_batch_loss: 793.29993\n",
      "Current_batch_loss: 563.26587\n",
      "Current_batch_loss: 642.42896\n",
      "Current_batch_loss: 581.72565\n",
      "Current_batch_loss: 671.06378\n",
      "Current_batch_loss: 652.84363\n",
      "Current_batch_loss: 819.07269\n",
      "Current_batch_loss: 762.53943\n",
      "Current_batch_loss: 688.77466\n",
      "Current_batch_loss: 593.05688\n",
      "Current_batch_loss: 762.21039\n",
      "Current_batch_loss: 668.16437\n",
      "Current_batch_loss: 698.53345\n",
      "Current_batch_loss: 506.07922\n",
      "Current_batch_loss: 650.02924\n",
      "Current_batch_loss: 605.21088\n",
      "Current_batch_loss: 690.24054\n",
      "Current_batch_loss: 692.83728\n",
      "Current_batch_loss: 632.99854\n",
      "Current_batch_loss: 669.23004\n",
      "Current_batch_loss: 628.38489\n",
      "Current_batch_loss: 589.53857\n",
      "Current_batch_loss: 533.87085\n",
      "Current_batch_loss: 699.68262\n",
      "Current_batch_loss: 590.89709\n",
      "Current_batch_loss: 545.32367\n",
      "Current_batch_loss: 610.36603\n",
      "Current_batch_loss: 726.39545\n",
      "Current_batch_loss: 711.60028\n",
      "Current_batch_loss: 586.12598\n",
      "Current_batch_loss: 569.79608\n",
      "Current_batch_loss: 623.45258\n",
      "Current_batch_loss: 727.4848\n",
      "Current_batch_loss: 708.40271\n",
      "Current_batch_loss: 642.73083\n",
      "Current_batch_loss: 581.06018\n",
      "Current_batch_loss: 643.61584\n",
      "Current_batch_loss: 577.99408\n",
      "Current_batch_loss: 659.75616\n",
      "Current_batch_loss: 648.63605\n",
      "Current_batch_loss: 604.53851\n",
      "Current_batch_loss: 576.66895\n",
      "Current_batch_loss: 675.18243\n",
      "Current_batch_loss: 629.2464\n",
      "Current_batch_loss: 594.62152\n",
      "Current_batch_loss: 715.1001\n",
      "Current_batch_loss: 610.961\n",
      "Current_batch_loss: 631.71454\n",
      "Current_batch_loss: 581.26398\n",
      "Current_batch_loss: 694.28168\n",
      "Current_batch_loss: 688.646\n",
      "Current_batch_loss: 572.1366\n",
      "Current_batch_loss: 525.47705\n",
      "Current_batch_loss: 690.16187\n",
      "Current_batch_loss: 600.30402\n",
      "Current_batch_loss: 668.62317\n",
      "Current_batch_loss: 623.85846\n",
      "Current_batch_loss: 679.25842\n",
      "Current_batch_loss: 729.95148\n",
      "Current_batch_loss: 596.18304\n",
      "Current_batch_loss: 633.61511\n",
      "Current_batch_loss: 468.7196\n",
      "Current_batch_loss: 682.57031\n",
      "Current_batch_loss: 619.00629\n",
      "Current_batch_loss: 557.23926\n",
      "Current_batch_loss: 647.45996\n",
      "Current_batch_loss: 528.95618\n",
      "Current_batch_loss: 559.91418\n",
      "Current_batch_loss: 558.41382\n",
      "Current_batch_loss: 570.16846\n",
      "Current_batch_loss: 525.6333\n",
      "Current_batch_loss: 600.73071\n",
      "Current_batch_loss: 677.54846\n",
      "Current_batch_loss: 712.24225\n",
      "Current_batch_loss: 700.4469\n",
      "Current_batch_loss: 539.72797\n",
      "Current_batch_loss: 522.62067\n",
      "Current_batch_loss: 567.52332\n",
      "Current_batch_loss: 670.10223\n",
      "Current_batch_loss: 576.06763\n",
      "Current_batch_loss: 588.48602\n",
      "Current_batch_loss: 556.91058\n",
      "Current_batch_loss: 732.21216\n",
      "Current_batch_loss: 486.26852\n",
      "Current_batch_loss: 579.26874\n",
      "Current_batch_loss: 742.39655\n",
      "Current_batch_loss: 624.46729\n",
      "Current_batch_loss: 648.71741\n",
      "Current_batch_loss: 735.03363\n",
      "Current_batch_loss: 576.90692\n",
      "Current_batch_loss: 657.01532\n",
      "Current_batch_loss: 464.18607\n",
      "Current_batch_loss: 533.64471\n",
      "Current_batch_loss: 450.47852\n",
      "Current_batch_loss: 590.2821\n",
      "Current_batch_loss: 733.05426\n",
      "Current_batch_loss: 716.802\n",
      "Current_batch_loss: 592.89105\n",
      "Current_batch_loss: 569.75476\n",
      "Current_batch_loss: 697.73358\n",
      "Current_batch_loss: 546.29126\n",
      "Current_batch_loss: 725.4325\n",
      "Current_batch_loss: 701.83966\n",
      "Current_batch_loss: 629.71863\n",
      "Current_batch_loss: 632.11298\n",
      "Current_batch_loss: 515.79138\n",
      "Current_batch_loss: 555.12555\n",
      "Current_batch_loss: 679.17114\n",
      "Current_batch_loss: 632.55719\n",
      "Current_batch_loss: 657.72449\n",
      "Current_batch_loss: 640.50714\n",
      "Current_batch_loss: 518.62451\n",
      "Current_batch_loss: 789.3111\n",
      "Current_batch_loss: 504.33832\n",
      "Current_batch_loss: 571.20123\n",
      "Current_batch_loss: 568.94263\n",
      "Current_batch_loss: 586.56653\n",
      "Current_batch_loss: 651.32153\n",
      "Current_batch_loss: 673.05872\n",
      "Current_batch_loss: 604.46979\n",
      "Current_batch_loss: 685.23114\n",
      "Current_batch_loss: 579.88629\n",
      "Current_batch_loss: 572.97559\n",
      "Current_batch_loss: 594.42175\n",
      "Current_batch_loss: 470.93427\n",
      "Current_batch_loss: 565.37842\n",
      "Current_batch_loss: 692.20135\n",
      "Current_batch_loss: 467.78833\n",
      "Current_batch_loss: 600.56891\n",
      "Current_batch_loss: 419.42722\n",
      "Current_batch_loss: 728.05273\n",
      "Current_batch_loss: 566.34509\n",
      "Current_batch_loss: 595.26117\n",
      "Current_batch_loss: 578.75153\n",
      "Current_batch_loss: 524.71893\n",
      "Current_batch_loss: 556.83527\n",
      "Current_batch_loss: 730.48822\n",
      "Current_batch_loss: 535.02094\n",
      "Current_batch_loss: 415.50815\n",
      "Current_batch_loss: 509.49472\n",
      "Current_batch_loss: 599.80847\n",
      "Current_batch_loss: 543.50128\n",
      "Current_batch_loss: 559.32599\n",
      "Current_batch_loss: 519.66669\n",
      "Current_batch_loss: 587.70111\n",
      "Current_batch_loss: 573.76367\n",
      "Current_batch_loss: 478.92526\n",
      "Current_batch_loss: 523.03595\n",
      "Current_batch_loss: 526.99622\n",
      "Current_batch_loss: 399.77072\n",
      "Current_batch_loss: 602.44592\n",
      "Current_batch_loss: 535.62354\n",
      "Current_batch_loss: 567.15039\n",
      "Current_batch_loss: 612.67639\n",
      "Current_batch_loss: 438.92978\n",
      "Current_batch_loss: 591.28033\n",
      "Current_batch_loss: 599.78967\n",
      "Current_batch_loss: 519.29083\n",
      "Current_batch_loss: 598.57874\n",
      "Current_batch_loss: 501.65048\n",
      "Current_batch_loss: 753.36029\n",
      "Current_batch_loss: 585.72156\n",
      "Current_batch_loss: 594.66187\n",
      "Current_batch_loss: 567.10541\n",
      "Current_batch_loss: 587.5993\n",
      "Current_batch_loss: 612.51538\n",
      "Current_batch_loss: 543.58081\n",
      "Current_batch_loss: 566.09076\n",
      "Current_batch_loss: 462.45374\n",
      "Current_batch_loss: 626.44733\n",
      "Current_batch_loss: 458.24997\n",
      "Current_batch_loss: 535.82977\n",
      "Current_batch_loss: 545.51831\n",
      "Current_batch_loss: 485.11127\n",
      "Current_batch_loss: 576.93665\n",
      "Current_batch_loss: 533.30017\n",
      "Current_batch_loss: 573.8028\n",
      "Current_batch_loss: 365.19595\n",
      "Current_batch_loss: 469.80551\n",
      "Current_batch_loss: 426.52298\n",
      "Current_batch_loss: 509.10941\n",
      "Current_batch_loss: 485.93625\n",
      "Current_batch_loss: 575.83459\n",
      "Current_batch_loss: 502.12958\n",
      "Current_batch_loss: 479.29889\n",
      "Current_batch_loss: 532.01294\n",
      "Current_batch_loss: 614.7948\n",
      "Current_batch_loss: 322.29895\n",
      "Current_batch_loss: 406.68515\n",
      "Current_batch_loss: 590.66974\n",
      "Current_batch_loss: 459.26556\n",
      "Current_batch_loss: 470.86679\n",
      "Current_batch_loss: 444.29477\n",
      "Current_batch_loss: 553.43292\n",
      "Current_batch_loss: 616.38934\n",
      "Current_batch_loss: 486.00977\n",
      "Current_batch_loss: 496.56122\n",
      "Current_batch_loss: 517.22394\n",
      "Current_batch_loss: 437.4408\n",
      "Current_batch_loss: 603.617\n",
      "Current_batch_loss: 524.89587\n",
      "Current_batch_loss: 388.2804\n",
      "Current_batch_loss: 444.02203\n",
      "Current_batch_loss: 532.05762\n",
      "Current_batch_loss: 559.61591\n",
      "Current_batch_loss: 411.41803\n",
      "Current_batch_loss: 342.13101\n",
      "Current_batch_loss: 399.95859\n",
      "Current_batch_loss: 424.20511\n",
      "Current_batch_loss: 656.38123\n",
      "Current_batch_loss: 500.73834\n",
      "Current_batch_loss: 522.90668\n",
      "Current_batch_loss: 351.1781\n",
      "Current_batch_loss: 574.08423\n",
      "Current_batch_loss: 597.74835\n",
      "Current_batch_loss: 401.53958\n",
      "Current_batch_loss: 481.556\n",
      "Current_batch_loss: 468.52426\n",
      "Current_batch_loss: 433.59781\n",
      "Current_batch_loss: 555.06696\n",
      "Current_batch_loss: 459.48782\n",
      "Current_batch_loss: 452.67075\n",
      "Current_batch_loss: 427.86292\n",
      "Current_batch_loss: 460.98615\n",
      "Current_batch_loss: 432.00256\n",
      "Current_batch_loss: 442.66281\n",
      "Current_batch_loss: 478.34467\n",
      "Current_batch_loss: 486.18781\n",
      "Current_batch_loss: 537.57007\n",
      "Current_batch_loss: 485.99985\n",
      "Current_batch_loss: 543.83044\n",
      "Current_batch_loss: 363.68488\n",
      "Current_batch_loss: 535.87762\n",
      "Current_batch_loss: 438.61557\n",
      "Current_batch_loss: 491.59396\n",
      "Current_batch_loss: 453.46832\n",
      "Current_batch_loss: 381.57736\n",
      "Current_batch_loss: 524.45941\n",
      "Current_batch_loss: 433.16092\n",
      "Current_batch_loss: 362.14182\n",
      "Current_batch_loss: 517.30811\n",
      "Current_batch_loss: 522.79926\n",
      "Current_batch_loss: 356.22369\n",
      "Current_batch_loss: 429.31903\n",
      "Current_batch_loss: 450.1525\n",
      "Current_batch_loss: 623.09094\n",
      "Current_batch_loss: 494.58224\n",
      "Current_batch_loss: 534.01056\n",
      "Current_batch_loss: 586.12665\n",
      "Current_batch_loss: 606.20142\n",
      "Current_batch_loss: 547.59088\n",
      "Current_batch_loss: 549.89032\n",
      "Current_batch_loss: 382.78467\n",
      "Current_batch_loss: 443.93344\n",
      "Current_batch_loss: 438.92264\n",
      "Current_batch_loss: 334.54932\n",
      "Current_batch_loss: 413.10303\n",
      "Current_batch_loss: 481.66995\n",
      "Current_batch_loss: 490.78076\n",
      "Current_batch_loss: 459.06775\n",
      "Current_batch_loss: 476.27008\n",
      "Current_batch_loss: 406.83148\n",
      "Current_batch_loss: 396.31607\n",
      "Current_batch_loss: 508.02466\n",
      "Current_batch_loss: 400.1423\n",
      "Current_batch_loss: 493.46527\n",
      "Current_batch_loss: 438.33453\n",
      "Current_batch_loss: 507.68851\n",
      "Current_batch_loss: 555.80133\n",
      "Current_batch_loss: 371.6088\n",
      "Current_batch_loss: 421.96014\n",
      "Current_batch_loss: 514.88379\n",
      "Current_batch_loss: 381.76154\n",
      "Current_batch_loss: 313.11179\n",
      "Current_batch_loss: 391.07187\n",
      "Current_batch_loss: 465.75812\n",
      "Current_batch_loss: 489.54956\n",
      "Current_batch_loss: 498.23187\n",
      "Current_batch_loss: 315.16272\n",
      "Current_batch_loss: 462.97443\n",
      "Current_batch_loss: 611.87726\n",
      "Current_batch_loss: 325.74619\n",
      "Current_batch_loss: 563.19885\n",
      "Current_batch_loss: 481.8075\n",
      "Current_batch_loss: 436.5397\n",
      "Current_batch_loss: 505.70517\n",
      "Current_batch_loss: 457.82379\n",
      "Current_batch_loss: 354.19443\n",
      "Current_batch_loss: 470.72308\n",
      "Current_batch_loss: 425.96243\n",
      "Current_batch_loss: 432.88968\n",
      "Current_batch_loss: 490.17599\n",
      "Current_batch_loss: 352.19327\n",
      "Current_batch_loss: 352.49173\n",
      "Current_batch_loss: 455.6828\n",
      "Current_batch_loss: 468.75491\n",
      "Current_batch_loss: 444.86606\n",
      "Current_batch_loss: 407.41275\n",
      "Current_batch_loss: 424.45978\n",
      "Current_batch_loss: 418.39816\n",
      "Current_batch_loss: 455.83676\n",
      "Current_batch_loss: 587.76074\n",
      "Current_batch_loss: 395.37698\n",
      "Current_batch_loss: 359.30869\n",
      "Current_batch_loss: 488.88153\n",
      "Current_batch_loss: 335.65762\n",
      "Current_batch_loss: 490.38342\n",
      "Current_batch_loss: 292.46878\n",
      "Current_batch_loss: 440.50153\n",
      "Current_batch_loss: 483.69357\n",
      "Current_batch_loss: 479.15756\n",
      "Current_batch_loss: 324.23532\n",
      "Current_batch_loss: 327.9574\n",
      "Current_batch_loss: 290.23053\n",
      "Current_batch_loss: 405.45468\n",
      "Current_batch_loss: 505.91101\n",
      "Current_batch_loss: 427.52332\n",
      "Current_batch_loss: 383.99805\n",
      "Current_batch_loss: 532.26758\n",
      "Current_batch_loss: 441.7623\n",
      "Current_batch_loss: 345.95337\n",
      "Current_batch_loss: 435.86569\n",
      "Current_batch_loss: 387.78\n",
      "Current_batch_loss: 430.97418\n",
      "Current_batch_loss: 336.33221\n",
      "Current_batch_loss: 356.10092\n",
      "Current_batch_loss: 487.48706\n",
      "Current_batch_loss: 312.24646\n",
      "Current_batch_loss: 533.76001\n",
      "Current_batch_loss: 460.8252\n",
      "Current_batch_loss: 335.84845\n",
      "Current_batch_loss: 520.81024\n",
      "Current_batch_loss: 423.55966\n",
      "Current_batch_loss: 295.09656\n",
      "Current_batch_loss: 489.72797\n",
      "Current_batch_loss: 506.62103\n",
      "Current_batch_loss: 436.91537\n",
      "Current_batch_loss: 233.54358\n",
      "Current_batch_loss: 443.71106\n",
      "Current_batch_loss: 322.29373\n",
      "Current_batch_loss: 381.29977\n",
      "Current_batch_loss: 354.16852\n",
      "Current_batch_loss: 372.06204\n",
      "Current_batch_loss: 367.68451\n",
      "Current_batch_loss: 398.64484\n",
      "Current_batch_loss: 370.09344\n",
      "Current_batch_loss: 384.05493\n",
      "Current_batch_loss: 251.25197\n",
      "Current_batch_loss: 465.7453\n",
      "Current_batch_loss: 163.54735\n",
      "Current_batch_loss: 311.68982\n",
      "Current_batch_loss: 543.29779\n",
      "Current_batch_loss: 391.10355\n",
      "Current_batch_loss: 364.71695\n",
      "Current_batch_loss: 310.39117\n",
      "Current_batch_loss: 292.31436\n",
      "Current_batch_loss: 190.67249\n",
      "Current_batch_loss: 440.27188\n",
      "Current_batch_loss: 415.76205\n",
      "Current_batch_loss: 426.06821\n",
      "Current_batch_loss: 486.93311\n",
      "Current_batch_loss: 305.29517\n",
      "Current_batch_loss: 305.85202\n",
      "Current_batch_loss: 339.53235\n",
      "Current_batch_loss: 359.849\n",
      "Current_batch_loss: 253.26079\n",
      "Current_batch_loss: 410.02557\n",
      "Current_batch_loss: 371.27942\n",
      "Current_batch_loss: 390.36414\n",
      "Current_batch_loss: 333.53918\n",
      "Current_batch_loss: 534.01465\n",
      "Current_batch_loss: 378.77466\n",
      "Current_batch_loss: 344.35349\n",
      "Current_batch_loss: 466.64862\n",
      "Current_batch_loss: 425.10693\n",
      "Current_batch_loss: 494.75516\n",
      "Current_batch_loss: 323.69107\n",
      "Current_batch_loss: 426.09027\n",
      "Current_batch_loss: 447.73959\n",
      "Current_batch_loss: 284.74969\n",
      "Current_batch_loss: 405.20386\n",
      "Current_batch_loss: 361.43323\n",
      "Current_batch_loss: 343.77771\n",
      "Current_batch_loss: 346.80136\n",
      "Current_batch_loss: 436.42767\n",
      "Current_batch_loss: 438.67761\n",
      "Current_batch_loss: 374.21945\n",
      "Current_batch_loss: 311.87582\n",
      "Current_batch_loss: 321.31699\n",
      "Current_batch_loss: 336.12921\n",
      "Current_batch_loss: 315.60666\n",
      "Current_batch_loss: 285.77341\n",
      "Current_batch_loss: 301.40121\n",
      "Current_batch_loss: 413.31161\n",
      "Current_batch_loss: 255.75444\n",
      "Current_batch_loss: 361.86609\n",
      "Current_batch_loss: 378.71573\n",
      "Current_batch_loss: 211.99792\n",
      "Current_batch_loss: 320.62146\n",
      "Current_batch_loss: 391.80362\n",
      "Current_batch_loss: 341.51166\n",
      "Current_batch_loss: 285.94055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [8:45:49<56:57:49, 7887.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses_of_glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_glove_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 19\u001b[0m, in \u001b[0;36mtrain_glove_model\u001b[0;34m(the_model, epoch, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m the_loss \u001b[38;5;241m=\u001b[39m the_loss\u001b[38;5;241m/\u001b[39mcount\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mthe_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m(the_loss\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prog/venv-MyPython/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses_of_glove_model = train_glove_model(glove_mod,epoch,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(glove_mod.state_dict(),'/home/luchian/all_data/uni_data/glove_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove_mod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/glove_model_weights.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6e5af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedCNNMod(\n",
       "  (embed): Embedding(57568, 150, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 150), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(4, 150), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(5, 150), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=942, stride=942, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train cnn model with glove vectors\n",
    "glove_150_cnn_mod = EmbedCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=150,ker_sizes=[2,4,5])\n",
    "glove_mod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/glove_model_weights.pth',weights_only=True))\n",
    "glove_150_cnn_mod.embed.weight = glove_mod.embed.weight\n",
    "glove_150_cnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0372b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21028 2336\n"
     ]
    }
   ],
   "source": [
    "MainTrain,MainVal = random_split(TrainDataset,lengths=[0.9,0.1])\n",
    "print(len(MainTrain),len(MainVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38fc2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader_val_glove_cnn = DataLoader(dataset = MainTrain,shuffle=True,batch_size=35)\n",
    "epoch = 30\n",
    "lr = 0.00005\n",
    "optimizer_glove_cnn = torch.optim.Adam(lr = lr,params = glove_150_cnn_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa7917",
   "metadata": {},
   "source": [
    "обучаем сверточную модель на векторных прествавлениях glove    \n",
    "также для модели cbow поллное обкчение представлено в google_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.915160243007387 | Val acc: 0.42937 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.813440762819744 | Val acc: 0.52483 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.7371219162536342 | Val acc: 0.58604 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.6462238685858626 | Val acc: 0.61002 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.581516339755098 | Val acc: 0.625 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.52174074231289 | Val acc: 0.62928 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.4862695725110922 | Val acc: 0.65068 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.4556876242458325 | Val acc: 0.65668 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.4268412114478983 | Val acc: 0.66182 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.4107145033342865 | Val acc: 0.66524 | Best acc on epoch #10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.3771178477417412 | Val acc: 0.67038 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.3563187094575753 | Val acc: 0.67295 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.3637293149250715 | Val acc: 0.6768 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.33865911190006 | Val acc: 0.67551 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.319054201170926 | Val acc: 0.67851 | Best acc on epoch #15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.307553712272406 | Val acc: 0.68151 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.308659553131129 | Val acc: 0.6845 | Best acc on epoch #17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.2837031373069212 | Val acc: 0.68878 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.285470471505119 | Val acc: 0.6905 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.303606267776743 | Val acc: 0.68836 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.2533936843201643 | Val acc: 0.69178 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.2383903763357693 | Val acc: 0.69349 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.2108402847747834 | Val acc: 0.69692 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.1983564999853316 | Val acc: 0.69563 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.1837558171614235 | Val acc: 0.69949 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.1849734649285302 | Val acc: 0.69777 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.1684637786495111 | Val acc: 0.69906 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.1693391589178221 | Val acc: 0.70163 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.1767322582234558 | Val acc: 0.69949 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.1591952266094094 | Val acc: 0.70077 | Best acc on epoch #30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time: 24 minutes\n",
    "cnn_glove_res = train_classification_model(glove_150_cnn_mod,\n",
    "                                           epoch,\n",
    "                                           train_loader_val_glove_cnn,\n",
    "                                           optimizer_glove_cnn,\n",
    "                                           loss_func,\n",
    "                                           MainVal,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a78f65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.1740581328777624 | Val acc: 0.70462 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.1621838998477194 | Val acc: 0.70462 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.1626287326836546 | Val acc: 0.70548 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.1662661880998564 | Val acc: 0.70676 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.154011960979706 | Val acc: 0.70634 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.1827252082737432 | Val acc: 0.70719 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.1711817768568207 | Val acc: 0.70548 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.1570361185192863 | Val acc: 0.70548 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.1527714481617568 | Val acc: 0.70719 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.167231215018401 | Val acc: 0.70676 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.1402608799359168 | Val acc: 0.7089 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.1545584705129837 | Val acc: 0.70848 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.1667246403987712 | Val acc: 0.7089 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.1566156175192104 | Val acc: 0.70848 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.1418452424534942 | Val acc: 0.70762 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.1467164258492766 | Val acc: 0.70933 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.1252355910775664 | Val acc: 0.70848 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.1485538464318497 | Val acc: 0.70762 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.1625529649650397 | Val acc: 0.71062 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.1682656868523646 | Val acc: 0.70976 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.148141472242438 | Val acc: 0.70805 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.140019454471085 | Val acc: 0.70762 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.136092548932688 | Val acc: 0.70848 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.1400147694567475 | Val acc: 0.70719 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.1682586594052402 | Val acc: 0.70848 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.1661106866727058 | Val acc: 0.70933 | Best acc on epoch #26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.1478132625883708 | Val acc: 0.71062 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.1379214220753129 | Val acc: 0.70805 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.134593547010382 | Val acc: 0.70591 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.1608419249736133 | Val acc: 0.70976 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.1347977331345172 | Val acc: 0.70933 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.137776103447162 | Val acc: 0.71019 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.1500039826613695 | Val acc: 0.70762 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.1329890771237467 | Val acc: 0.70976 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.149942349425568 | Val acc: 0.70933 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.1504393489705942 | Val acc: 0.71019 | Best acc on epoch #36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.1749202703775066 | Val acc: 0.71019 | Best acc on epoch #36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.145834966973338 | Val acc: 0.71019 | Best acc on epoch #36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.1162315550044848 | Val acc: 0.70848 | Best acc on epoch #36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.1412841657334676 | Val acc: 0.71062 | Best acc on epoch #40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.1581291930994853 | Val acc: 0.7089 | Best acc on epoch #40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.1295291454145397 | Val acc: 0.70933 | Best acc on epoch #42\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.1342075301486125 | Val acc: 0.70976 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 44 | Loss: 1.1548653641923294 | Val acc: 0.71233 | Best acc on epoch #44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 45 | Loss: 1.145633527844797 | Val acc: 0.70933 | Best acc on epoch #44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 46 | Loss: 1.1445551107508967 | Val acc: 0.70976 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 47 | Loss: 1.155686765909195 | Val acc: 0.71147 | Best acc on epoch #47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 48 | Loss: 1.1412440168232767 | Val acc: 0.7119 | Best acc on epoch #48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 49 | Loss: 1.167789365408226 | Val acc: 0.71233 | Best acc on epoch #49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50 | Loss: 1.152214875038769 | Val acc: 0.71233 | Best acc on epoch #49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "cnn_glove_res_2 = train_classification_model(glove_150_cnn_mod,\n",
    "                                           50,\n",
    "                                           train_loader_val_glove_cnn,\n",
    "                                           optimizer_glove_cnn,\n",
    "                                           loss_func,\n",
    "                                           MainVal,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f37c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.1425201842769013 | Val acc: 0.71147 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.1477197774386843 | Val acc: 0.7119 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.1651244843611106 | Val acc: 0.71019 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.146826641358274 | Val acc: 0.71062 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.1221625814074883 | Val acc: 0.7089 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.114942629156811 | Val acc: 0.70933 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.1670866341836836 | Val acc: 0.71019 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.1384142698741793 | Val acc: 0.70933 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.1354924456747915 | Val acc: 0.71019 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.151969924048457 | Val acc: 0.71104 | Best acc on epoch #10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.1299228632013731 | Val acc: 0.70976 | Best acc on epoch #10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.1660559616995334 | Val acc: 0.70976 | Best acc on epoch #10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.1782147098152886 | Val acc: 0.71019 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.16123799773897 | Val acc: 0.70976 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.1288289581975604 | Val acc: 0.70933 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.1421322398842273 | Val acc: 0.71104 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.158676013648014 | Val acc: 0.71062 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.150984557491174 | Val acc: 0.71062 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.129455595911045 | Val acc: 0.71104 | Best acc on epoch #19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.1272759924514124 | Val acc: 0.71147 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.1259242980531765 | Val acc: 0.71019 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.152816421180319 | Val acc: 0.71147 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.1377905901775582 | Val acc: 0.70933 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.1412465659135986 | Val acc: 0.71147 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.1303096049389307 | Val acc: 0.71062 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.1581416812395295 | Val acc: 0.70976 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.1175365698416895 | Val acc: 0.71062 | Best acc on epoch #27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.1395186011783296 | Val acc: 0.71104 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.1592739290881673 | Val acc: 0.71104 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.1394722895098606 | Val acc: 0.7119 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.1457927704055773 | Val acc: 0.71276 | Best acc on epoch #31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.157571295731873 | Val acc: 0.7119 | Best acc on epoch #31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.1580989789000764 | Val acc: 0.70933 | Best acc on epoch #31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.0943221747875214 | Val acc: 0.71147 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:44<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.161373312530819 | Val acc: 0.70976 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.1261008433910853 | Val acc: 0.70976 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.1396366802606726 | Val acc: 0.71062 | Best acc on epoch #37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.1565560441148064 | Val acc: 0.70805 | Best acc on epoch #37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.13711251558956 | Val acc: 0.71019 | Best acc on epoch #39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.1525934195756515 | Val acc: 0.71233 | Best acc on epoch #40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.144738693428714 | Val acc: 0.71062 | Best acc on epoch #40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.138608761342512 | Val acc: 0.70933 | Best acc on epoch #40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 601/601 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.168450863855254 | Val acc: 0.71019 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  83%|████████▎ | 496/601 [00:36<00:07, 13.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "cnn_glove_res_3 = train_classification_model(glove_150_cnn_mod,\n",
    "                                           50,\n",
    "                                           train_loader_val_glove_cnn,\n",
    "                                           optimizer_glove_cnn,\n",
    "                                           loss_func,\n",
    "                                           MainVal,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd968915",
   "metadata": {},
   "source": [
    "выыводи некотрые кривые обучения    \n",
    "именно: точность на валидационном множестве и значения функции ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "75d84971",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_values = cnn_glove_res[-1] + cnn_glove_res_2[-1] + cnn_glove_res_3[-1] \n",
    "loss_values = cnn_glove_res[1] + cnn_glove_res_2[1] + cnn_glove_res_3[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b57bca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_val(the_losses,the_vals,val_name = 'Validation'):\n",
    "    some_figure = plt.figure(figsize = (10,5),facecolor = 'skyblue')\n",
    "    some_ax = some_figure.add_subplot()\n",
    "    some_ax.grid(linestyle = '--',c = 'gray',alpha = 0.4)\n",
    "    some_ax.plot(the_vals,c = 'green',label = 'Accuracy on ' + val_name)\n",
    "    some_ax.plot(the_losses,color = 'purple',label  = 'Average error rate')\n",
    "    some_ax.set_ylabel('Accuracy - Error')\n",
    "    some_ax.set_xlabel('Epoch')\n",
    "    some_ax.legend()\n",
    "    return some_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e5601b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epoch', ylabel='Accuracy - Error'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "the_ax = print_loss_val(loss_values,val_values)\n",
    "the_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e136c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedCNNMod(\n",
       "  (embed): Embedding(57568, 150, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 111, kernel_size=(2, 150), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 111, kernel_size=(4, 150), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 111, kernel_size=(5, 150), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=333, out_features=7, bias=True)\n",
       "  (mp1): MaxPool1d(kernel_size=944, stride=944, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp2): MaxPool1d(kernel_size=942, stride=942, padding=0, dilation=1, ceil_mode=False)\n",
       "  (mp3): MaxPool1d(kernel_size=941, stride=941, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout1d(p=0.5, inplace=False)\n",
       "  (rel): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train cnn model with glove vectors on Train\n",
    "glove_150_cnn_mod = EmbedCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=150,ker_sizes=[2,4,5])\n",
    "glove_mod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/glove_model_weights.pth',weights_only=True))\n",
    "glove_150_cnn_mod.embed.weight = glove_mod.embed.weight\n",
    "glove_150_cnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0e5b5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "main_loader = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=35)\n",
    "epoch = 70\n",
    "lr = 9*10**-6\n",
    "optimizer_glove_cnn_main = torch.optim.Adam(lr = lr,params = glove_150_cnn_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bbcd85c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.9629125270300996 | Val acc: 0.32291 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.9061011134864327 | Val acc: 0.37692 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.885976202473669 | Val acc: 0.41785 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.8677164648107425 | Val acc: 0.4531 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.848646854033727 | Val acc: 0.47072 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.8345244905191982 | Val acc: 0.50028 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.8097859872672373 | Val acc: 0.53383 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.7887302317662153 | Val acc: 0.53667 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.7720297121716118 | Val acc: 0.54747 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.7630984003315429 | Val acc: 0.56964 | Best acc on epoch #10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.7384424512971661 | Val acc: 0.57874 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.7251105474497743 | Val acc: 0.59125 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.7015206531850164 | Val acc: 0.59864 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.6903168772330541 | Val acc: 0.60489 | Best acc on epoch #14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.6720179478208461 | Val acc: 0.60659 | Best acc on epoch #15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.6529589886972291 | Val acc: 0.61626 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.635664129863956 | Val acc: 0.62138 | Best acc on epoch #17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.6152353192160944 | Val acc: 0.62251 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.594463115234575 | Val acc: 0.62251 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.593178178914293 | Val acc: 0.63047 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.5728089928091644 | Val acc: 0.63388 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.5558053018625624 | Val acc: 0.63673 | Best acc on epoch #22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.5445193716151986 | Val acc: 0.63957 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.5364286631881119 | Val acc: 0.63957 | Best acc on epoch #23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.5159320063933641 | Val acc: 0.64412 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.500823245612447 | Val acc: 0.64696 | Best acc on epoch #26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.4975297714243392 | Val acc: 0.64525 | Best acc on epoch #26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.4865935360064764 | Val acc: 0.65037 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.4859688352503462 | Val acc: 0.65037 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.458439425883179 | Val acc: 0.65208 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.44565589618897 | Val acc: 0.65208 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.4308992899998934 | Val acc: 0.65719 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.4382908952807238 | Val acc: 0.65719 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.447386040123637 | Val acc: 0.66117 | Best acc on epoch #34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.414953782351431 | Val acc: 0.66345 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.3892994523940685 | Val acc: 0.66117 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.4072783352015261 | Val acc: 0.66117 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.408107476587781 | Val acc: 0.66458 | Best acc on epoch #38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.3759403304603999 | Val acc: 0.66401 | Best acc on epoch #38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.3868371516823055 | Val acc: 0.66401 | Best acc on epoch #38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.3760898272969766 | Val acc: 0.66515 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.3374016181437556 | Val acc: 0.66515 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.3550695305812859 | Val acc: 0.66742 | Best acc on epoch #43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 44 | Loss: 1.3493410267694268 | Val acc: 0.66856 | Best acc on epoch #44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 45 | Loss: 1.337897551006186 | Val acc: 0.67084 | Best acc on epoch #45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 46 | Loss: 1.3554940841155139 | Val acc: 0.6714 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 47 | Loss: 1.3221228519778052 | Val acc: 0.66856 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 48 | Loss: 1.327524880806129 | Val acc: 0.67254 | Best acc on epoch #48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 49 | Loss: 1.316478126375618 | Val acc: 0.67482 | Best acc on epoch #49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50 | Loss: 1.329017115121116 | Val acc: 0.67311 | Best acc on epoch #49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 51 | Loss: 1.3268767770179017 | Val acc: 0.67311 | Best acc on epoch #49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 52 | Loss: 1.2865940393088107 | Val acc: 0.67482 | Best acc on epoch #52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 53 | Loss: 1.306372726525732 | Val acc: 0.67482 | Best acc on epoch #52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 54 | Loss: 1.315537434361295 | Val acc: 0.67595 | Best acc on epoch #54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 55 | Loss: 1.2947923900213785 | Val acc: 0.67709 | Best acc on epoch #55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 56 | Loss: 1.3010512422658727 | Val acc: 0.67652 | Best acc on epoch #55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 57 | Loss: 1.3044868713010571 | Val acc: 0.67823 | Best acc on epoch #57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 58 | Loss: 1.3068976360791458 | Val acc: 0.67936 | Best acc on epoch #58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 59 | Loss: 1.267942953787878 | Val acc: 0.67993 | Best acc on epoch #59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 60 | Loss: 1.3066554095841454 | Val acc: 0.6805 | Best acc on epoch #60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 61 | Loss: 1.2554983298578661 | Val acc: 0.67823 | Best acc on epoch #60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 62 | Loss: 1.2543520628513691 | Val acc: 0.67936 | Best acc on epoch #62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 63 | Loss: 1.2815329738541277 | Val acc: 0.68107 | Best acc on epoch #63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 64 | Loss: 1.2613464199051172 | Val acc: 0.6805 | Best acc on epoch #63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 65 | Loss: 1.2599532306194305 | Val acc: 0.68221 | Best acc on epoch #65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 66 | Loss: 1.2534045613186802 | Val acc: 0.68107 | Best acc on epoch #65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 67 | Loss: 1.27390185404502 | Val acc: 0.68562 | Best acc on epoch #67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 68 | Loss: 1.2235013443285119 | Val acc: 0.68789 | Best acc on epoch #68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 69 | Loss: 1.2561635850610848 | Val acc: 0.68675 | Best acc on epoch #68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 70 | Loss: 1.25434267284449 | Val acc: 0.68732 | Best acc on epoch #70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "cnn_glove_res_main = train_classification_model(glove_150_cnn_mod,\n",
    "                                           epoch,\n",
    "                                           main_loader,\n",
    "                                           optimizer_glove_cnn_main,\n",
    "                                           loss_func,\n",
    "                                           TestDataset,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d626aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.2588801578044178 | Val acc: 0.68391 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.249368454524857 | Val acc: 0.68789 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.2457135422322565 | Val acc: 0.68789 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.2347650381173203 | Val acc: 0.68619 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.2327142642911322 | Val acc: 0.68619 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.2380397944393273 | Val acc: 0.68619 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.2396470341764525 | Val acc: 0.68789 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.2254343475410325 | Val acc: 0.68619 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.2176632911948386 | Val acc: 0.68789 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.2326188756677205 | Val acc: 0.68675 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.2232389310876766 | Val acc: 0.68675 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.2132957929711856 | Val acc: 0.68732 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.2384744098265015 | Val acc: 0.69073 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.2098283983187048 | Val acc: 0.68789 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.216461805035611 | Val acc: 0.68675 | Best acc on epoch #13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.2201939810535865 | Val acc: 0.68846 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.1845644274336136 | Val acc: 0.68789 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.2041085448539899 | Val acc: 0.68903 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.1943668784227914 | Val acc: 0.68903 | Best acc on epoch #18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.2162562035871838 | Val acc: 0.69073 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.2121554952627884 | Val acc: 0.69358 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.2083985039038572 | Val acc: 0.69187 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.179767832159996 | Val acc: 0.69187 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.2231914524189726 | Val acc: 0.69016 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.203483405450504 | Val acc: 0.69187 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.1980343139367904 | Val acc: 0.69187 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.213014100349235 | Val acc: 0.69016 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.1905856024094685 | Val acc: 0.69471 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.204571437737542 | Val acc: 0.69016 | Best acc on epoch #28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.1884337242491945 | Val acc: 0.69471 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.1829435331468097 | Val acc: 0.69073 | Best acc on epoch #30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.2110230058966995 | Val acc: 0.69414 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.1833550715428627 | Val acc: 0.69471 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.195085971833703 | Val acc: 0.69358 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.206859963293561 | Val acc: 0.69358 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.1895230832749497 | Val acc: 0.69301 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.1682418160988186 | Val acc: 0.69358 | Best acc on epoch #37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.1769295586440378 | Val acc: 0.69414 | Best acc on epoch #38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.1744089541945628 | Val acc: 0.69528 | Best acc on epoch #39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.1646809990623754 | Val acc: 0.69528 | Best acc on epoch #39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.1814461788329893 | Val acc: 0.69983 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.1913505461133882 | Val acc: 0.69869 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.1853996457721658 | Val acc: 0.69585 | Best acc on epoch #41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 44 | Loss: 1.160688601121931 | Val acc: 0.69812 | Best acc on epoch #44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 45 | Loss: 1.192573127186227 | Val acc: 0.70097 | Best acc on epoch #45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 46 | Loss: 1.176568318983752 | Val acc: 0.69756 | Best acc on epoch #45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 47 | Loss: 1.1563469937550808 | Val acc: 0.69869 | Best acc on epoch #47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 48 | Loss: 1.172053291813699 | Val acc: 0.69812 | Best acc on epoch #47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 49 | Loss: 1.1572342969164877 | Val acc: 0.69699 | Best acc on epoch #47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50 | Loss: 1.1677473699618242 | Val acc: 0.69642 | Best acc on epoch #47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "cnn_glove_res_main_2 = train_classification_model(glove_150_cnn_mod,\n",
    "                                           50,\n",
    "                                           main_loader,\n",
    "                                           optimizer_glove_cnn_main,\n",
    "                                           loss_func,\n",
    "                                           TestDataset,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32140787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.166490462413448 | Val acc: 0.69983 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.1573472253665953 | Val acc: 0.69869 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 1.1606074536587307 | Val acc: 0.69926 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 1.1881685810620912 | Val acc: 0.69983 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 1.1327829982438486 | Val acc: 0.69983 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 1.1654452680203016 | Val acc: 0.69983 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 1.1476502269952598 | Val acc: 0.7004 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 1.1732341215728286 | Val acc: 0.70097 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 1.1766986891538083 | Val acc: 0.70097 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 1.1607672138485365 | Val acc: 0.7004 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 1.1693954997790788 | Val acc: 0.70097 | Best acc on epoch #11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 1.173831311840854 | Val acc: 0.70153 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 1.165417379917142 | Val acc: 0.70153 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 1.1657740413071866 | Val acc: 0.70267 | Best acc on epoch #14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 1.1570613143775992 | Val acc: 0.70097 | Best acc on epoch #14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 16 | Loss: 1.148186636408289 | Val acc: 0.70153 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 17 | Loss: 1.1518951371133686 | Val acc: 0.70153 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 18 | Loss: 1.136475727943603 | Val acc: 0.70153 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 19 | Loss: 1.1600191145689187 | Val acc: 0.70153 | Best acc on epoch #16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 20 | Loss: 1.1396285618493658 | Val acc: 0.7021 | Best acc on epoch #20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 21 | Loss: 1.1555687230147287 | Val acc: 0.70381 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 22 | Loss: 1.1570059848492018 | Val acc: 0.70324 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 23 | Loss: 1.1995171007817378 | Val acc: 0.70153 | Best acc on epoch #21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 24 | Loss: 1.128475986376494 | Val acc: 0.7021 | Best acc on epoch #24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 25 | Loss: 1.1624158943841558 | Val acc: 0.70438 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 26 | Loss: 1.15991472359189 | Val acc: 0.70438 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 27 | Loss: 1.171804122225253 | Val acc: 0.70381 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 28 | Loss: 1.156338286123233 | Val acc: 0.70267 | Best acc on epoch #25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 29 | Loss: 1.1612544274794128 | Val acc: 0.70495 | Best acc on epoch #29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 30 | Loss: 1.152840083676898 | Val acc: 0.70324 | Best acc on epoch #29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 31 | Loss: 1.172852591661636 | Val acc: 0.7021 | Best acc on epoch #29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 32 | Loss: 1.1550320161138465 | Val acc: 0.70267 | Best acc on epoch #32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 33 | Loss: 1.1554996732644691 | Val acc: 0.70381 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:48<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 34 | Loss: 1.1494225910056137 | Val acc: 0.70153 | Best acc on epoch #33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 35 | Loss: 1.1377094430837802 | Val acc: 0.70324 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 36 | Loss: 1.1502978802173438 | Val acc: 0.70267 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 37 | Loss: 1.1490517613327431 | Val acc: 0.70267 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 38 | Loss: 1.1711481176985952 | Val acc: 0.70267 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 39 | Loss: 1.1681925080671995 | Val acc: 0.70267 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 40 | Loss: 1.1644140954859956 | Val acc: 0.7021 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 41 | Loss: 1.1284233788976412 | Val acc: 0.7021 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 42 | Loss: 1.1460771522300686 | Val acc: 0.7021 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 43 | Loss: 1.160070232779323 | Val acc: 0.7021 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 44 | Loss: 1.158350227567964 | Val acc: 0.7021 | Best acc on epoch #35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 45 | Loss: 1.157921298847584 | Val acc: 0.70267 | Best acc on epoch #45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 46 | Loss: 1.1654831641270016 | Val acc: 0.70324 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 47 | Loss: 1.1648750769164034 | Val acc: 0.70267 | Best acc on epoch #46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 48 | Loss: 1.15379521342184 | Val acc: 0.70381 | Best acc on epoch #48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 49 | Loss: 1.1267136877436124 | Val acc: 0.70438 | Best acc on epoch #49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 668/668 [00:47<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50 | Loss: 1.1322895876260217 | Val acc: 0.70495 | Best acc on epoch #50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "cnn_glove_res_main_3 = train_classification_model(glove_150_cnn_mod,\n",
    "                                           50,\n",
    "                                           main_loader,\n",
    "                                           optimizer_glove_cnn_main,\n",
    "                                           loss_func,\n",
    "                                           TestDataset,\n",
    "                                           dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(glove_150_cnn_mod.state_dict(),'/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_cnn_mod.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an_mode.load_state_dict(torch.load('/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_cnn_mod.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6c6b4",
   "metadata": {},
   "source": [
    "результат похож на остальные но заметим что тут размерность 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8556a742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70495"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(an_mode,TestDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be721cf",
   "metadata": {},
   "source": [
    "тестируем некоторые архитектуры рекуррентных нейронных сетей    \n",
    "последняя из них была выбрана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafb5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,in_size = 150,hid_size = 150,drop_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=in_size,hidden_size=hid_size,batch_first = True)\n",
    "        self.drop = nn.Dropout1d(p = drop_rate)\n",
    "    def forward(self,x):\n",
    "        #res 1\n",
    "        y = x.transpose(0,1)\n",
    "        _,(y,_) = self.lstm1(y)\n",
    "        y += x\n",
    "        y = self.drop(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1540ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN's\n",
    "class RNNMod(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hid_size = 350):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = embed_dim,hidden_size = hid_size,batch_first = True)\n",
    "        self.block1 = ResBlock(in_size = hid_size,hid_size=hid_size,drop_rate=0.35)\n",
    "        self.block2 = ResBlock(in_size = hid_size,hid_size=hid_size,drop_rate = 0.15)\n",
    "        self.lin = nn.Linear(hid_size,7)\n",
    "\n",
    "    def forward(self,inds_list):\n",
    "        y = self.embed(inds_list)\n",
    "        _,(y,_) = self.lstm1(y)\n",
    "        y = self.block1(y)\n",
    "        y = self.block2(y)\n",
    "\n",
    "        y = y.squeeze(dim = 0)\n",
    "        y = self.lin(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf10612",
   "metadata": {},
   "source": [
    "тут используем двунаправленную архитеткуры рекуррентных нейронных сетей LSTM (подробнее смотреть в аналитической записке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cfe3e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN's\n",
    "class RNNMod(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hid_size = 150,bi = False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = embed_dim,hidden_size = hid_size,batch_first = True,bidirectional = bi)\n",
    "        self.lin = nn.Linear(2*hid_size if bi else hid_size,7)\n",
    "\n",
    "    def forward(self,inds_list):\n",
    "        y = self.embed(inds_list)\n",
    "        _,(y,_) = self.lstm1(y)\n",
    "        y = y.transpose(0,1)\n",
    "        y = y.reshape(y.shape[0],-1)\n",
    "        y = self.lin(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d99c358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 945])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_value = next(iter(train_loader_val_glove_rnn))\n",
    "one_value[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b9f9c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 945])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_value[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22085cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_mod = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,bi = True)\n",
    "the_mod(one_value[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62b3aad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21027\n",
      "2337\n"
     ]
    }
   ],
   "source": [
    "Train_val_np, Val_val_np = train_test_split(Train_news,stratify=Train_news['topic'],train_size = 0.9)\n",
    "print(len(Train_val_np))\n",
    "print(len(Val_val_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4d4c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21027\n",
      "2337\n"
     ]
    }
   ],
   "source": [
    "Train_val_tensor = NewsDataset(Train_val_np,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))\n",
    "Val_val_tensor = NewsDataset(Val_val_np,le,Train_tokenizer,transforms=Padding(the_max_len_of_sents))\n",
    "print(len(Train_val_tensor))\n",
    "print(len(Val_val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dbc6ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNMod(\n",
       "  (embed): Embedding(57568, 300)\n",
       "  (lstm1): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
       "  (lin): Linear(in_features=300, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_300_rnn_mod = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,hid_size = 150,bi = True)\n",
    "ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_300_embeddings_epoch#9.pth',weights_only=True))\n",
    "ngram_300_rnn_mod.embed.weight = ngram_model.embed.weight\n",
    "ngram_300_rnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f12f4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader_val_ngram_rnn = DataLoader(dataset = Train_val_tensor,shuffle=True,batch_size=10)\n",
    "epoch = 50\n",
    "lr = 5*10**-5\n",
    "optimizer_val_ngram_rnn = torch.optim.Adam(params = ngram_300_rnn_mod.parameters(),\n",
    "                                           lr = lr,\n",
    "                                           betas = (0.9,0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5effdf",
   "metadata": {},
   "source": [
    "как обычно проводим обучения для каждого типа векторных представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75479665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(the_model,epoch,loader,optimizer,loss_func,val_dataset,dev = 'cuda'):\n",
    "    average_ep_losses = []\n",
    "    best_epoch = None\n",
    "    val_accs = []\n",
    "    try:\n",
    "        for one_ep in range(epoch):\n",
    "            the_model.train()\n",
    "            the_losses = []\n",
    "            for X,y in tqdm(loader,desc = 'Going through the loader'):\n",
    "                X,y = X.to(device = dev),y.to(device = dev)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = the_model(X)\n",
    "                the_loss = loss_func(y_pred,y)\n",
    "                the_loss.backward()\n",
    "                the_losses.append(the_loss.item())\n",
    "                optimizer.step()\n",
    "            if one_ep%1== 0:\n",
    "                avg_loss = np.array(the_losses).mean()\n",
    "                average_ep_losses.append(avg_loss)\n",
    "                val_score = get_accuracy(the_model,val_dataset,dev =dev)\n",
    "                val_accs.append(val_score)\n",
    "\n",
    "                if best_epoch == None:\n",
    "                    best_epoch = one_ep + 1\n",
    "                else:\n",
    "                    best_epoch = one_ep + 1 if val_accs[-1] > val_accs[-2] else best_epoch\n",
    "\n",
    "                print(f'Epoch # {one_ep+1} | Loss: {avg_loss} | Val acc: {val_score} | Best acc on epoch #{best_epoch}',end = '\\n\\n')\n",
    "        return best_epoch,average_ep_losses,val_accs\n",
    "    except KeyboardInterrupt:\n",
    "        return best_epoch,average_ep_losses,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b992a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [02:25<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.767588238006425 | Val acc: 0.46341 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [02:26<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.3902384818195446 | Val acc: 0.69833 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [03:20<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 0.9297968710582709 | Val acc: 0.77493 | Best acc on epoch #3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_val_res = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        30,\n",
    "                                        train_loader_val_ngram_rnn,\n",
    "                                        optimizer_val_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        Val_val_tensor,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c07763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [03:23<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 0.6842917821844362 | Val acc: 0.81686 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [35:52<00:00,  1.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 0.5552977318129282 | Val acc: 0.84938 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  62%|██████▏   | 1308/2103 [16:17<09:54,  1.34it/s]  \n"
     ]
    }
   ],
   "source": [
    "rnn_val_res_2 = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        15,\n",
    "                                        train_loader_val_ngram_rnn,\n",
    "                                        optimizer_val_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        Val_val_tensor,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a1ffc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [02:27<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 0.43275927229466904 | Val acc: 0.86521 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [10:54<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 0.36861497011740707 | Val acc: 0.87634 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [19:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 0.32034512737759707 | Val acc: 0.87035 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [20:33<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 0.26698040067941425 | Val acc: 0.89217 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [19:36<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 0.22376537658670528 | Val acc: 0.88019 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [19:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 0.1950612410259067 | Val acc: 0.89944 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:  49%|████▊     | 1021/2103 [09:03<09:36,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "rnn_val_res_3 = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        15,\n",
    "                                        train_loader_val_ngram_rnn,\n",
    "                                        optimizer_val_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        Val_val_tensor,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e09404ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [02:33<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 0.14369287143107787 | Val acc: 0.88789 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:36<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 0.12128129310872054 | Val acc: 0.89388 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [18:28<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 0.09492635262070097 | Val acc: 0.88318 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [18:21<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 0.0748735611351128 | Val acc: 0.89388 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:46<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 0.06860120689864631 | Val acc: 0.90201 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:44<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 0.048861171441382203 | Val acc: 0.90244 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [16:56<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 0.04173304410891575 | Val acc: 0.90757 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:16<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 0.034620223434407134 | Val acc: 0.90586 | Best acc on epoch #7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:29<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 0.032671835582754945 | Val acc: 0.90929 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:44<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 0.028977935428780577 | Val acc: 0.90843 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [17:14<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 11 | Loss: 0.019116370863038672 | Val acc: 0.908 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [24:33<00:00,  1.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 12 | Loss: 0.01736699607822731 | Val acc: 0.91228 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [27:14<00:00,  1.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 13 | Loss: 0.01606704661814676 | Val acc: 0.91185 | Best acc on epoch #12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [32:02<00:00,  1.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 14 | Loss: 0.012826096089435583 | Val acc: 0.91356 | Best acc on epoch #14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2103/2103 [32:11<00:00,  1.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 15 | Loss: 0.010948793096748512 | Val acc: 0.91271 | Best acc on epoch #14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_val_res_4 = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        15,\n",
    "                                        train_loader_val_ngram_rnn,\n",
    "                                        optimizer_val_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        Val_val_tensor,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a84394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_rnn_losses = rnn_val_res[1] + rnn_val_res_2[1] + rnn_val_res_3[1] + rnn_val_res_4[1]\n",
    "the_rnn_scores = rnn_val_res[2] + rnn_val_res_2[2] + rnn_val_res_3[2] + rnn_val_res_4[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65d308c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "the_ax = print_loss_val(the_rnn_losses,the_rnn_scores,val_name = 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e06eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_ax.savefig('lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13fddf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luchian/prog/self_prog/Projects/TextClassification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "874b2b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNMod(\n",
       "  (embed): Embedding(57568, 300)\n",
       "  (lstm1): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
       "  (lin): Linear(in_features=300, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train lstm on training set with ngram_300 embeddings\n",
    "ngram_300_rnn_mod = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,hid_size = 150,bi = True)\n",
    "ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_300_embeddings_epoch#9.pth',weights_only=True))\n",
    "ngram_300_rnn_mod.embed.weight = ngram_model.embed.weight\n",
    "ngram_300_rnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3dd534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_loader_ngram_rnn = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 50\n",
    "lr = 5*10**-5\n",
    "optimizer_loader_ngram_rnn = torch.optim.Adam(params = ngram_300_rnn_mod.parameters(),\n",
    "                                           lr = lr,\n",
    "                                           betas = (0.9,0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f10f4d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [02:58<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 1.7515779513296634 | Val acc: 0.47641 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [03:14<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 1.3017746847207392 | Val acc: 0.76066 | Best acc on epoch #2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [13:40<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 0.8958805681715983 | Val acc: 0.81069 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [20:26<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 0.7070336320003512 | Val acc: 0.83002 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [15:52<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 0.5707917145947947 | Val acc: 0.85219 | Best acc on epoch #5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader:   6%|▌         | 133/2337 [00:11<03:04, 11.92it/s]\n"
     ]
    }
   ],
   "source": [
    "rnn_main_res1 = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        15,\n",
    "                                        train_loader_ngram_rnn,\n",
    "                                        optimizer_loader_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        TestDataset,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "10a682dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [03:08<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 | Loss: 0.48088390606498943 | Val acc: 0.87152 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [12:14<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2 | Loss: 0.41658262180496813 | Val acc: 0.86981 | Best acc on epoch #1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [14:43<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3 | Loss: 0.3451706784875776 | Val acc: 0.88346 | Best acc on epoch #3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [15:27<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4 | Loss: 0.29855263344827954 | Val acc: 0.89937 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [16:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5 | Loss: 0.25581648396599077 | Val acc: 0.8772 | Best acc on epoch #4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [16:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6 | Loss: 0.21011633216712272 | Val acc: 0.90165 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [17:16<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7 | Loss: 0.17293005166228606 | Val acc: 0.89255 | Best acc on epoch #6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [17:19<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8 | Loss: 0.13741625835758403 | Val acc: 0.90279 | Best acc on epoch #8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [17:24<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9 | Loss: 0.11124065246823163 | Val acc: 0.90733 | Best acc on epoch #9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through the loader: 100%|██████████| 2337/2337 [18:18<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 10 | Loss: 0.08602573863156124 | Val acc: 0.91245 | Best acc on epoch #10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_main_res2 = train_classification_model(ngram_300_rnn_mod,\n",
    "                                        25,\n",
    "                                        train_loader_ngram_rnn,\n",
    "                                        optimizer_loader_ngram_rnn,\n",
    "                                        loss_func,\n",
    "                                        TestDataset,\n",
    "                                        dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(ngram_300_rnn_mod.state_dict(),'/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_rnn_mod.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo_gram = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,hid_size = 150,bi = True)\n",
    "# lo_gram.load_state_dict(torch.load('/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_rnn_mod.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb86381",
   "metadata": {},
   "source": [
    "полученный результат для ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44640037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91245"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_accuracy(lo_gram,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37030f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNMod(\n",
       "  (embed): Embedding(57568, 150)\n",
       "  (lstm1): LSTM(150, 150, batch_first=True, bidirectional=True)\n",
       "  (lin): Linear(in_features=300, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model for glove emebeddings\n",
    "glove_150_rnn_mod = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=150,bi = True)\n",
    "glove_mod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/glove_model_weights.pth',weights_only = True))\n",
    "glove_150_rnn_mod.embed.weight = glove_mod.embed.weight\n",
    "glove_150_rnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c536ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_main_glove_rnn = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 16\n",
    "lr = 5*10**-5\n",
    "optimizer_main_glove_rnn = torch.optim.Adam(lr = lr,params = glove_150_rnn_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model was trained in google colab (see google_train.ipynb)\n",
    "glove_rnn_res = train_classification_model(glove_150_rnn_mod,\n",
    "                           epoch,\n",
    "                           train_main_glove_rnn,\n",
    "                           optimizer_main_glove_rnn,\n",
    "                           loss_func,\n",
    "                           TestDataset,\n",
    "                           dev = 'cuda',\n",
    "                           model_name = 'glove_150_RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d713f",
   "metadata": {},
   "source": [
    "теперь аналогично обучаем ту же рекуррентную нейронку на cbow and glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d452f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self,vocab_size,dim_embed,window_size = 2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,dim_embed)\n",
    "        self.linear1 = nn.Linear(dim_embed,128)\n",
    "        self.linear2 = nn.Linear(128,vocab_size)\n",
    "        self.rel = nn.ReLU()\n",
    "\n",
    "    def forward(self,ind_list):\n",
    "        y = self.embed(ind_list).sum(dim = 1)\n",
    "        y = self.rel(self.linear1(y))\n",
    "        y = self.rel(self.linear2(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad43ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = CBOWModel(Train_tokenizer.get_vocab_size(),dim_embed = 300,window_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a433f7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNMod(\n",
       "  (embed): Embedding(57568, 300)\n",
       "  (lstm1): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
       "  (lin): Linear(in_features=300, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model for cbow embeddings \n",
    "cbow_300_rnn_mod = RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,bi = True)\n",
    "cbow_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/1_epoch#15.pth',weights_only=True))\n",
    "cbow_300_rnn_mod.embed.weight = cbow_model.embed.weight\n",
    "cbow_300_rnn_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9342bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_main_cbow_rnn = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 16\n",
    "lr = 5*10**-5\n",
    "optimizer_main_cbow_rnn = torch.optim.Adam(lr = lr,params = cbow_300_rnn_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model was trained in google colab (see google_train.ipynb)\n",
    "cbow_rnn_res = train_classification_model(cbow_300_rnn_mod,\n",
    "                           epoch,\n",
    "                           train_main_cbow_rnn,\n",
    "                           optimizer_main_cbow_rnn,\n",
    "                           loss_func,\n",
    "                           TestDataset,\n",
    "                           dev = 'cuda',\n",
    "                           model_name = 'cbow_300_RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ab0bc",
   "metadata": {},
   "source": [
    "модели были обучены в google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f0d009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_150_rnn_mod.load_state_dict(torch.load('/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_RNN_weights.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96afd278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_300_rnn_mod.load_state_dict(torch.load('/home/luchian/prog/self_prog/Projects/TextClassification/cbow_300_RNN_weights.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4455a66",
   "metadata": {},
   "source": [
    "получены следующие результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d148ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89937"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(glove_150_rnn_mod,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9e8603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91074"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(cbow_300_rnn_mod,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainsformer model with positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca5fe5",
   "metadata": {},
   "source": [
    "функция реализующая позициональное кодирование, котороне требуется для моделей transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0632f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding():\n",
    "    @torch.no_grad()\n",
    "    def __init__(self,vocab_size,embed_dim,calc = True,dev = 'cuda'):\n",
    "        '''Calculates the positional ecnodings'''\n",
    "        self.d_model = embed_dim\n",
    "        self.pos_embed = nn.Embedding(vocab_size,embed_dim).to(device = dev)\n",
    "        if calc == True:\n",
    "            for ind in range(vocab_size):\n",
    "                pos_tensor = torch.tensor([\n",
    "                    math.sin((ind+1)/10_000**(2*i/self.d_model)) \n",
    "                    if i%2 == 0 \n",
    "                    else \n",
    "                    math.cos((ind+1)/10_000**(2*i/self.d_model)) \n",
    "                    for i in range(self.d_model)\n",
    "                ])\n",
    "                self.pos_embed.weight[ind] = pos_tensor\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __call__(self,the_inds):\n",
    "        return self.pos_embed(the_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71acd8",
   "metadata": {},
   "source": [
    "моель Transformer Encoder (подробнее смотреть в аналитической записке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf883ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,d_mod = 300,nhead = 6,encoder_layers = 6,batch_first = True,dev = 'cuda',vocab_size = Train_tokenizer.get_vocab_size()):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,d_mod)\n",
    "        self.single_encoder = nn.TransformerEncoderLayer(d_model = d_mod,nhead=nhead,batch_first = batch_first)\n",
    "        self.full_encoder = nn.TransformerEncoder(encoder_layer = self.single_encoder,num_layers = encoder_layers)\n",
    "        self.pos_encoder = PositionalEncoding(vocab_size = vocab_size,embed_dim = d_mod,calc = True,dev = dev)\n",
    "        self.rnn = nn.RNN(input_size=d_mod,hidden_size=100,batch_first=batch_first)\n",
    "        self.linear = nn.Linear(100,7)\n",
    "    def forward(self,the_inds):\n",
    "        embeds = self.embed(the_inds)\n",
    "        pos_encodings = self.pos_encoder(the_inds)\n",
    "        final_embed = embeds + pos_encodings\n",
    "        y = self.full_encoder(final_embed)\n",
    "        _,y = self.rnn(y)\n",
    "        y = y.squeeze(dim = 0)\n",
    "        y = self.linear(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf4cca",
   "metadata": {},
   "source": [
    "далее для всех векторных представлений модели были обычным образом обучены (некоторые из них в google colab) смотреть аналитичсекую записку и google_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf803e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embed): Embedding(57568, 300)\n",
       "  (single_encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "    (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (full_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rnn): RNN(300, 100, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Transformer model with skip_gram embeddings\n",
    "ngram_300_transformer_mod = TransformerModel(d_mod = 300,\n",
    "                               nhead = 6,\n",
    "                               encoder_layers = 3,\n",
    "                               batch_first = True,\n",
    "                               dev = 'cuda',\n",
    "                               vocab_size = Train_tokenizer.get_vocab_size()).to(device = 'cuda')\n",
    "ngram_model.load_state_dict(torch.load('/home/luchian/all_data/uni_data/ngram_300_embeddings_epoch#9.pth',weights_only=True))\n",
    "ngram_300_transformer_mod.embed.weight = ngram_model.embed.weight\n",
    "ngram_300_transformer_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beed1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_main_ngram_transformers = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 10\n",
    "lr = 5*10**-5\n",
    "optimizer_main_ngram_transformers = torch.optim.Adam(lr = lr,params = ngram_300_transformer_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f81de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the moel was trained in google.colab\n",
    "ngram_transformer_res = train_classification_model(ngram_300_transformer_mod,\n",
    "                           epoch,\n",
    "                           train_main_ngram_transformers,\n",
    "                           optimizer_main_ngram_transformers,\n",
    "                           loss_func,\n",
    "                           TestDataset,\n",
    "                           dev = 'cuda',\n",
    "                           model_name = 'ngram_300_Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "750d12b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_300_transformer_mod.load_state_dict(torch.load(f = '/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_Transformer_weights.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21b559",
   "metadata": {},
   "source": [
    "получен хороший результат для векторных представлений ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bed840ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91188"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(ngram_300_transformer_mod,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ab720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Transformer model with cbow embeddings\n",
    "cbow_300_transformer_mod = TransformerModel(d_mod = 300,\n",
    "                               nhead = 6,\n",
    "                               encoder_layers = 3,\n",
    "                               batch_first = True,\n",
    "                               dev = 'cuda',\n",
    "                               vocab_size = Train_tokenizer.get_vocab_size()).to(device = 'cuda')\n",
    "cbow_model.load_state_dict(torch.load('1_epoch#15.pth',weights_only=True))\n",
    "cbow_300_transformer_mod.embed.weight = cbow_model.embed.weight\n",
    "cbow_300_transformer_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_main_cbow_transformers = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 10\n",
    "lr = 5*10**-5\n",
    "optimizer_main_cbow_transformers = torch.optim.Adam(lr = lr,params = cbow_300_transformer_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c291a",
   "metadata": {},
   "source": [
    "далее представлены процессы для cbow и glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8190840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_300_transformer_mod.load_state_dict(torch.load(f = '/home/luchian/prog/self_prog/Projects/TextClassification/cbow_300_Transformer_weights.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5644f306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9187"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(cbow_300_transformer_mod,TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model was trained in google colab\n",
    "cbow_transformer_res = train_classification_model(cbow_300_transformer_mod,\n",
    "                           epoch,\n",
    "                           train_main_cbow_transformers,\n",
    "                           optimizer_main_cbow_transformers,\n",
    "                           loss_func,\n",
    "                           TestDataset,\n",
    "                           dev = 'cuda',\n",
    "                           model_name = 'cbow_300_Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb8f5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embed): Embedding(57568, 150)\n",
       "  (single_encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=150, out_features=150, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=150, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=150, bias=True)\n",
       "    (norm1): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (full_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=150, out_features=150, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=150, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=150, bias=True)\n",
       "        (norm1): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rnn): RNN(150, 100, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Transformer model with glove embeddings\n",
    "glove_150_transformer_mod = TransformerModel(d_mod = 150,\n",
    "                               nhead = 6,\n",
    "                               encoder_layers = 3,\n",
    "                               batch_first = True,\n",
    "                               dev = 'cuda',\n",
    "                               vocab_size = Train_tokenizer.get_vocab_size()).to(device = 'cuda')\n",
    "glove_mod.load_state_dict(torch.load('/home/luchian/all_data/uni_data/glove_model_weights.pth',weights_only = True))\n",
    "glove_150_transformer_mod.embed.weight = glove_mod.embed.weight\n",
    "glove_150_transformer_mod.to(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95665ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=torch_train_weights.to(device = 'cuda'),reduction='mean')\n",
    "train_main_glove_transformers = DataLoader(dataset = TrainDataset,shuffle=True,batch_size=10)\n",
    "epoch = 10\n",
    "lr = 5*10**-5\n",
    "optimizer_main_glove_transformers = torch.optim.Adam(lr = lr,params = glove_150_transformer_mod.parameters(),betas = (0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323cab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_transformer_res = train_classification_model(glove_150_transformer_mod,\n",
    "                           epoch,\n",
    "                           train_main_glove_transformers,\n",
    "                           optimizer_main_glove_transformers,\n",
    "                           loss_func,\n",
    "                           TestDataset,\n",
    "                           dev = 'cuda',\n",
    "                           model_name = 'glove_150_Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60039a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_150_transformer_mod.load_state_dict(torch.load(f = '/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_Transformer_weights.pth',weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2bf8fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91529"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(glove_150_transformer_mod,TestDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eaed7b",
   "metadata": {},
   "source": [
    "тут происходит процесс создания таблицы для всех моделей содержащая в секбе все метрики "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57496ec",
   "metadata": {},
   "source": [
    "также реализована функция инференса для наилучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34d06462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,dataset,dev = 'cuda'):\n",
    "    \"\"\"returns all preditions of the model\"\"\"\n",
    "    model.eval()\n",
    "    y_values = []\n",
    "    y_pred_values = []\n",
    "    the_loader = DataLoader(dataset = dataset,shuffle=False,batch_size=1)\n",
    "    for X,y in the_loader:\n",
    "        X,y = X.to(device = dev),y.to(device = dev)\n",
    "        y_pred = model(X).softmax(dim = 1).argmax(dim = 1).detach().item()\n",
    "        y = y.item()\n",
    "        y_values.append(y)\n",
    "        y_pred_values.append(y_pred)\n",
    "    return y_values,y_pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "319983f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_metris(model,dataset,dev = 'cuda'):\n",
    "    \"\"\"returns accuracy, precision, and recall\"\"\"\n",
    "    y_true,y_pred = get_predictions(model,dataset,dev)\n",
    "    the_dict = {\n",
    "        'accuracy': accuracy_score(y_true = y_true,y_pred = y_pred),\n",
    "        'precision': precision_score(y_true = y_true,y_pred = y_pred,average='macro'),\n",
    "        'recall': recall_score(y_true = y_true,y_pred = y_pred,average = 'macro')  \n",
    "    }\n",
    "    return the_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9985a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dict = get_my_metris(model=glove_150_transformer_mod,dataset=TestDataset,dev = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccba1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_mod_weights = [\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/cbow_300_cnn_mod_epoch#13.pth', #cnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/cbow_300_RNN_weights.pth', #rnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/cbow_300_Transformer_weights.pth' # transformer\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abdb6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_mod_weights = [\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_cnn_mod_epoch#20.pth', #cnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_rnn_mod.pth', #rnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/ngram_300_Transformer_weights.pth' # transformer\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "972e544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mod_weights = [\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_cnn_mod.pth', #cnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_RNN_weights.pth', #rnn\n",
    "    '/home/luchian/prog/self_prog/Projects/TextClassification/glove_150_Transformer_weights.pth' #transformer\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7972f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = [\n",
    "    ngram_mod_weights,\n",
    "    cbow_mod_weights,\n",
    "    glove_mod_weights\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98216aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    EmbedCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=300,ker_sizes=[2,4,5]).to(device='cuda'), #cnn 300\n",
    "    RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=300,bi = True).to(device='cuda'), #rnn 300\n",
    "    TransformerModel(d_mod = 300,nhead = 6,encoder_layers = 3,batch_first = True,dev = 'cuda',vocab_size = Train_tokenizer.get_vocab_size()).to(device = 'cuda'), #transformer 300\n",
    "    EmbedCNNMod(Train_tokenizer.get_vocab_size(),dim_embed=150,ker_sizes=[2,4,5]).to(device='cuda'), #cnn 150\n",
    "    RNNMod(Train_tokenizer.get_vocab_size(),embed_dim=150,bi = True).to(device='cuda'), #rnn 150\n",
    "    TransformerModel(d_mod = 150,nhead = 6,encoder_layers = 3,batch_first = True,dev = 'cuda',vocab_size = Train_tokenizer.get_vocab_size()).to(device = 'cuda'), #transformer 150\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039e8d3",
   "metadata": {},
   "source": [
    "создаем нужную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "226c6ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN/ngram</th>\n",
       "      <th>RNN/ngram</th>\n",
       "      <th>Transformer/ngram</th>\n",
       "      <th>CNN/cbow</th>\n",
       "      <th>RNN/cbow</th>\n",
       "      <th>Transformer/cbow</th>\n",
       "      <th>CNN/glove</th>\n",
       "      <th>RNN/glove</th>\n",
       "      <th>Transformer/glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNN/ngram RNN/ngram  ... RNN/glove Transformer/glove\n",
       "accuracy        NaN       NaN  ...       NaN               NaN\n",
       "precision       NaN       NaN  ...       NaN               NaN\n",
       "recall          NaN       NaN  ...       NaN               NaN\n",
       "\n",
       "[3 rows x 9 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_table = pd.DataFrame(columns=['CNN/ngram','RNN/ngram','Transformer/ngram','CNN/cbow','RNN/cbow','Transformer/cbow','CNN/glove','RNN/glove','Transformer/glove'],index = ['accuracy','precision','recall'])\n",
    "res_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46a343",
   "metadata": {},
   "source": [
    "создаем функцию заполняющуя данную талицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c040dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchian/prog/venv-MyPython/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luchian/prog/venv-MyPython/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luchian/prog/venv-MyPython/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for embed_ind in range(3):\n",
    "    weights = all_weights[embed_ind]\n",
    "    for mod_ind in range(3):\n",
    "        if embed_ind != 2:\n",
    "            models[mod_ind].load_state_dict(torch.load(f = weights[mod_ind],weights_only = True))\n",
    "        else:\n",
    "            models[mod_ind + 3].load_state_dict(torch.load(f = weights[mod_ind],weights_only = True))\n",
    "    ###\n",
    "    if embed_ind != 2:\n",
    "        for mod_ind in range(3):\n",
    "            the_mod = models[mod_ind]\n",
    "            results = get_my_metris(the_mod,TestDataset)\n",
    "            for k in range(3):\n",
    "                res_table.iloc[k,mod_ind + 3*embed_ind] = results[list(results.keys())[k]]\n",
    "    else:\n",
    "        for mod_ind in range(3):\n",
    "            the_mod = models[mod_ind + 3]\n",
    "            results = get_my_metris(the_mod,TestDataset)\n",
    "            for k in range(3):\n",
    "                res_table.iloc[k,mod_ind + 3*embed_ind] = results[list(results.keys())[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16d17420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN/ngram</th>\n",
       "      <th>RNN/ngram</th>\n",
       "      <th>Transformer/ngram</th>\n",
       "      <th>CNN/cbow</th>\n",
       "      <th>RNN/cbow</th>\n",
       "      <th>Transformer/cbow</th>\n",
       "      <th>CNN/glove</th>\n",
       "      <th>RNN/glove</th>\n",
       "      <th>Transformer/glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.712905</td>\n",
       "      <td>0.91245</td>\n",
       "      <td>0.911882</td>\n",
       "      <td>0.702103</td>\n",
       "      <td>0.910745</td>\n",
       "      <td>0.918704</td>\n",
       "      <td>0.704946</td>\n",
       "      <td>0.899375</td>\n",
       "      <td>0.915293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.399556</td>\n",
       "      <td>0.725892</td>\n",
       "      <td>0.762562</td>\n",
       "      <td>0.395023</td>\n",
       "      <td>0.697529</td>\n",
       "      <td>0.729894</td>\n",
       "      <td>0.451661</td>\n",
       "      <td>0.700634</td>\n",
       "      <td>0.736322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.558678</td>\n",
       "      <td>0.794635</td>\n",
       "      <td>0.759552</td>\n",
       "      <td>0.543359</td>\n",
       "      <td>0.731711</td>\n",
       "      <td>0.744777</td>\n",
       "      <td>0.616459</td>\n",
       "      <td>0.752406</td>\n",
       "      <td>0.744632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNN/ngram RNN/ngram  ... RNN/glove Transformer/glove\n",
       "accuracy   0.712905   0.91245  ...  0.899375          0.915293\n",
       "precision  0.399556  0.725892  ...  0.700634          0.736322\n",
       "recall     0.558678  0.794635  ...  0.752406          0.744632\n",
       "\n",
       "[3 rows x 9 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93efb7",
   "metadata": {},
   "source": [
    "тут мы создаем функцию, которая будем по введенному тексту выдавать названия темы новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d3766606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(the_text,dev = 'cuda'):\n",
    "    \"\"\"returns a label using the text as an input\"\"\"\n",
    "    tokens = normalize_text(the_text)\n",
    "    inds = [Train_tokenizer.token_to_id(token) if Train_tokenizer.token_to_id(token) != None else Train_tokenizer.token_to_id('[UNK]') for token in tokens]\n",
    "    inds = torch.tensor(inds,dtype = torch.long)\n",
    "    inds = Padding()(inds)\n",
    "    inds = inds.reshape(1,-1)\n",
    "    y_pred = ngram_300_transformer_mod(inds.to(device = 'cuda')).softmax(dim = 1).argmax(dim = 1).detach().item()\n",
    "    the_clses = le.classes_.tolist()\n",
    "    return the_clses[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23471f28",
   "metadata": {},
   "source": [
    "выбираем некоторые реальные новости "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4abfad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_texts = [\n",
    "    ('Российский прыгун с шестом Александр Соловьев выиграл чемпионат американской Национальной ассоциации студенческого спорта (NCAA) в Юджине (штат Орегон).Соловьев победил с результатом 5,78 метра — он был единственным из участников, кто взял эту высоту.В прошлом году стало известно, что Соловьев, победитель Кубка России, поступил в Университет Техаса (Texas A&M University) и уехал учиться в США.','Спорт'),\n",
    "    ('Президент США Дональд Трамп назвал «очень милыми» извинения предпринимателя Илона Маска, с которым у него ранее произошла ссора. Такое мнение глава государства высказал в беседе с изданием The New York Post. «Я считаю, что это очень мило с его стороны», — заявил Трамп.При этом президент США не ответил, готов ли он оставить в прошлом произошедшее между ним и предпринимателем, отмечает газета.Маск и Трамп публично поссорились после того, как бизнесмен раскритиковал «большой прекрасный законопроект» республиканцев о налоговых льготах, который увеличивает дефицит бюджета США. Маск, до начала июня руководивший сокращением госрасходов в администрации Трампа, назвал законопроект «отвратительной мерзостью». Президент заявил, что разочарован в Маске.','Политика'),\n",
    "    ('Банк России анонсировал выпуск памятных серебряных монет номиналом 3 рубля — они войдут в обращение и будут приурочены к «100-летнему юбилею научно-технической разведки». Об этом пишет Центральный банк (ЦБ).Тираж монеты — 3 тыс. штук. На оборотной стороне серебряной монеты изобразят модель атома и надпись по окружности «научно-техническая разведка СВР России», двухголового орла — на лицевой. Все художественные элементы выполнят в рельефе, а центральную часть — эмблему Службы внешней разведки — сделают в цвете.','Финансы'),\n",
    "    ('В 2027 году на территории России может не появиться ни одного нового дома либо объемы ввода жилья могут сократиться до 30%. Такой прогноз сделал вице-премьер Марат Хуснуллин в эфире телеканала «Россия 24».Хуснуллин подчеркнул, что сохраняется «серьезная нехорошая тенденция», которая связана с остановкой новых проектов в строительной сфере. «Это означает, что в 2027 году не будет ввода жилья или будет значительно меньше. Это первый тревожный фактор», — сказал он.Второй, заявил вице-премьер, касается ситуации, когда застройщики из-за нехватки средств на эскроу-счетах начинают замедлять темпы строительства уже начатых объектов. В результате запланированные сроки сдачи домов могут быть перенесены. «Если тенденция будет сохраняться, до 30% может быть падение», — заявил Хуснуллин.','экономика')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27aab8",
   "metadata": {},
   "source": [
    "Для этих новостей попробуем провести инференс модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6858a8",
   "metadata": {},
   "source": [
    "представлены некоторые результаты инференса "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d378ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the text: Российский прыгун с шестом Александр Соловьев выиграл чемпионат американской Нац\n",
      "Model prediction: Спорт\n",
      "True label: Спорт\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Some of the text: Президент США Дональд Трамп назвал «очень милыми» извинения предпринимателя Илон\n",
      "Model prediction: Экономика\n",
      "True label: Политика\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Some of the text: Банк России анонсировал выпуск памятных серебряных монет номиналом 3 рубля — они\n",
      "Model prediction: Финансы\n",
      "True label: Финансы\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Some of the text: В 2027 году на территории России может не появиться ни одного нового дома либо о\n",
      "Model prediction: Экономика\n",
      "True label: экономика\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for one_text in some_texts:\n",
    "    print(f'Some of the text: {one_text[0][:80]}')\n",
    "    print(f'Model prediction: {model_inference(one_text[0])}')\n",
    "    print(f'True label: {one_text[1]}')\n",
    "    print(''.join(['-' for _ in range(100)]))\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-MyPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
